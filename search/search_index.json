{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"ai_project","text":"<p>Welcome to the documentation for ai_project.</p>"},{"location":"#getting-started","title":"Getting Started","text":"<p>Please refer to the README for installation and usage instructions.</p>"},{"location":"#features","title":"Features","text":"<p>Explore the features of this project:</p> <ul> <li>Feature Overview (if generated)</li> </ul>"},{"location":"2026-02-05-nebulus-atom-v2-design/","title":"Nebulus Atom V2 \u2014 Standalone Agent Management Platform","text":"<p>Date: 2026-02-05 Status: Design Brief Goal: Transform Atom from a dev-focused CLI into a standalone, independently installable agent management platform with a human-in-the-loop trust boundary.</p>"},{"location":"2026-02-05-nebulus-atom-v2-design/#current-state","title":"Current State","text":"<p>Atom has two packages today:</p> Package Purpose State <code>nebulus_atom</code> Standalone CLI agent \u2014 Typer, MVC architecture, 63 Python files Working CLI with services for MCP, RAG, skills, tools, telemetry <code>nebulus_swarm</code> Distributed Overlord/Minion orchestration Overlord (state, command parsing, model routing, Docker mgmt, GitHub queue, Slack bot) + Minion (agent, LLM client, tools, skills, git ops, reporter) <p>What works: CLI agent, PR reviewer, skill system, tool executor, RAG, MCP service, Streamlit dashboard What's hardcoded: <code>localhost:5000/v1</code> (TabbyAPI), model name <code>Meta-Llama-3.1-8B-Instruct-exl2-8_0</code> Deployment: Docker Compose (<code>docker-compose.swarm.yml</code>) for Swarm mode Tests: 642 passing, 0 failures (as of 2026-02-05) Standalone: Confirmed \u2014 zero nebulus-core imports. All LLM clients use <code>openai</code> SDK directly with configurable <code>base_url</code>. V2 Phase 1: COMPLETE \u2014 configurable LLM, configurable vector store, pip packaging, smoke tests all done. V2 Phase 2: COMPLETE (core items) \u2014 evaluator, scope enforcement, enhancement proposals. 691 tests. V2 Phase 3: COMPLETE \u2014 Proposals CLI wired, Evaluator.evaluate() method, LLM connection pool, Minion pool integration, backend compatibility matrix, skill evolution workflow. 738 tests passing (47 new). V2 Phase 4: COMPLETE \u2014 Provisioning config documentation, example configs, MCP client integration with graceful degradation. 754 tests passing (16 new). V2 Phase 5: COMPLETE \u2014 Small-model auditor, hybrid audit trail with hash chains, platform health API client, certification packages. 826 tests passing (72 new). Overlord Phase 1: COMPLETE \u2014 Project registry, scanner, dependency graph, action scope, cross-project memory, CLI commands (status/scan/config/discover/graph/memory/scope). 110 overlord tests passing.</p>"},{"location":"2026-02-05-nebulus-atom-v2-design/#v2-vision","title":"V2 Vision","text":"<p>Atom is a standalone product \u2014 like Open WebUI or Cursor. Users download it, install it, configure it for their LLM backend, and it works. On Nebulus appliances, provisioning writes the config. Atom does NOT depend on nebulus-core.</p>"},{"location":"2026-02-05-nebulus-atom-v2-design/#core-architecture-supervisorworker-with-human-approval","title":"Core Architecture: Supervisor/Worker with Human Approval","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                        User                           \u2502\n\u2502                    (Human-in-the-Loop)                \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25b2\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n               \u2502 approves work     \u2502 proposes enhancements\n               \u2502 dispatches tasks  \u2502 reports findings\n               \u25bc                   \u2502\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502              Supervisor (Overlord)                     \u2502\n\u2502                                                       \u2502\n\u2502  - Understands full project context                   \u2502\n\u2502  - Decomposes tasks into worker assignments           \u2502\n\u2502  - Dispatches workers with bounded scope              \u2502\n\u2502  - Evaluates worker output (quality, correctness)     \u2502\n\u2502  - Identifies capability gaps:                        \u2502\n\u2502      - New skills workers need                        \u2502\n\u2502      - Bug fixes in worker tooling                    \u2502\n\u2502      - Feature enhancements for better results        \u2502\n\u2502  - Reports enhancement proposals to User              \u2502\n\u2502  - NEVER self-improves without User approval          \u2502\n\u2502                                                       \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25b2\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n           \u2502 assigns tasks        \u2502 reports results\n           \u2502 provides context     \u2502 returns artifacts\n           \u25bc                      \u2502\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                Workers (Minions)                       \u2502\n\u2502                                                       \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510              \u2502\n\u2502  \u2502 Worker 1\u2502  \u2502 Worker 2\u2502  \u2502 Worker N\u2502  (concurrent) \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518              \u2502\n\u2502                                                       \u2502\n\u2502  - Execute discrete, bounded tasks                    \u2502\n\u2502  - Use skills + tools within their scope              \u2502\n\u2502  - Report results back to Supervisor                  \u2502\n\u2502  - No awareness of other workers                      \u2502\n\u2502  - No ability to modify their own capabilities        \u2502\n\u2502                                                       \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n           \u2502\n           \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502           Any OpenAI-Compatible LLM Backend           \u2502\n\u2502  TabbyAPI \u00b7 vLLM \u00b7 MLX \u00b7 Ollama \u00b7 OpenAI \u00b7 Anthropic \u2502\n\u2502                                                       \u2502\n\u2502  REQUIREMENT: Must handle concurrent requests         \u2502\n\u2502  (multiple workers query simultaneously)              \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"2026-02-05-nebulus-atom-v2-design/#trust-boundary-critical-design-constraint","title":"Trust Boundary (Critical Design Constraint)","text":"<p>The Supervisor operates under a strict trust boundary:</p> <ol> <li>Supervisor CAN: Dispatch tasks, evaluate output, identify gaps, propose enhancements</li> <li>Supervisor CANNOT: Build new skills, modify worker capabilities, change its own behavior, or install dependencies \u2014 without explicit User approval</li> <li>Workers CAN: Execute tasks within their assigned scope using existing skills/tools</li> <li>Workers CANNOT: Communicate with each other, modify their own skills, or access resources outside their scope</li> </ol> <p>This is not a suggestion \u2014 it is the core safety architecture that makes Atom deployable to customers who are wary of autonomous AI agents (Moltbot/Clawdbot backlash).</p> <p>Approval flow for enhancements: <pre><code>Supervisor identifies gap \u2192 writes enhancement proposal \u2192 presents to User \u2192\nUser reviews and approves/modifies/rejects \u2192 Supervisor dispatches approved work \u2192\nWorker implements enhancement \u2192 Supervisor evaluates result \u2192 User verifies\n</code></pre></p>"},{"location":"2026-02-05-nebulus-atom-v2-design/#v2-work-items","title":"V2 Work Items","text":""},{"location":"2026-02-05-nebulus-atom-v2-design/#phase-1-standalone-product-prerequisite","title":"Phase 1: Standalone Product (Prerequisite)","text":"<p>Make Atom installable and configurable without any Nebulus dependency.</p> <ol> <li> <p>~~Remove nebulus-core imports~~ DONE (2026-02-05) \u2014 Audit confirmed zero nebulus-core imports. All four LLM client locations use the <code>openai</code> SDK directly with configurable <code>base_url</code>. Atom is already fully standalone.</p> </li> <li> <p>~~Configurable LLM backend~~ DONE (2026-02-05) \u2014 Settings module with config file (<code>~/.atom/config.yml</code> or project-local <code>.atom.yml</code>) + env var overrides (<code>ATOM_LLM_BASE_URL</code>, <code>ATOM_LLM_MODEL</code>, <code>ATOM_LLM_API_KEY</code>). Sensible defaults, env vars take precedence over config file.</p> </li> <li> <p>~~Configurable vector store~~ DONE (2026-02-05) \u2014 ChromaDB connection configurable (embedded vs HTTP mode, host, port) via same settings system.</p> </li> <li> <p>~~Package for pip install~~ DONE (2026-02-05) \u2014 <code>pip install .</code> works in fresh venv, entry points (<code>nebulus-atom</code>, <code>mn</code>) functional, no nebulus-core dependency.</p> </li> <li> <p>~~Standalone smoke test~~ DONE (2026-02-05) \u2014 Tests verify config loading with defaults, env var overrides, CLI entry point importability. 642 tests passing (up from 605).</p> </li> </ol>"},{"location":"2026-02-05-nebulus-atom-v2-design/#phase-2-supervisorworker-formalization","title":"Phase 2: Supervisor/Worker Formalization","text":"<p>The Overlord/Minion code exists but needs the human approval loop and evaluation layer.</p> <ol> <li> <p>~~Supervisor evaluation layer~~ DONE (2026-02-05) \u2014 <code>nebulus_swarm/overlord/evaluator.py</code> with <code>_score()</code> method for scoring logic (pass/fail/needs-revision). CheckRunner + LLMReviewer orchestration via <code>evaluate()</code> left as integration point for Overlord wiring. Evaluations table added to <code>state.py</code>. 16 new tests.</p> </li> <li> <p>~~Enhancement proposal system~~ DONE (2026-02-05) \u2014 <code>nebulus_swarm/overlord/proposals.py</code> with data model + SQLite store. <code>nebulus_atom/commands/proposals.py</code> CLI formatters (stubbed \u2014 wiring to ProposalStore is follow-up). 12 new tests.</p> </li> <li> <p>~~Worker scope enforcement~~ DONE (2026-02-05) \u2014 <code>nebulus_swarm/overlord/scope.py</code> data model. <code>tool_executor.py</code> integrated with scope checking on writes. Minion loads scope from <code>MINION_SCOPE</code> env var. 20 new tests.</p> </li> <li> <p>~~Skill evolution workflow~~ DONE (2026-02-05) \u2014 <code>nebulus_swarm/overlord/skill_evolution.py</code>. When a new skill is needed:</p> </li> <li>Supervisor drafts skill spec (inputs, outputs, constraints)</li> <li>User approves spec</li> <li>Worker implements skill</li> <li>Supervisor validates skill against spec</li> <li>User confirms deployment to skill library</li> </ol>"},{"location":"2026-02-05-nebulus-atom-v2-design/#phase-3-concurrent-inference","title":"Phase 3: Concurrent Inference","text":"<ol> <li> <p>~~LLM connection pooling~~ DONE (2026-02-05) \u2014 <code>nebulus_swarm/overlord/llm_pool.py</code> with asyncio.Semaphore, configurable concurrency limit (<code>ATOM_LLM_CONCURRENCY</code>), queue management, pool stats, graceful fallback on 429/503. Integrated with Minion LLM client for Swarm mode, bypassed for standalone.</p> </li> <li> <p>~~Backend compatibility matrix~~ DONE (2026-02-05) \u2014 Documented in <code>docs/concurrent-inference-matrix.md</code>:     | Backend | Concurrent Support | Notes |     |---------|-------------------|-------|     | TabbyAPI | Yes | ExLlamaV2 batching supported |     | vLLM | Yes | Designed for concurrent serving |     | MLX Serving | Limited | Single-request optimized |     | Ollama | No | Sequential only \u2014 why it was abandoned |     | OpenAI API | Yes | Cloud, rate-limited |     | Anthropic API | Yes | Cloud, rate-limited |</p> </li> </ol>"},{"location":"2026-02-05-nebulus-atom-v2-design/#phase-4-nebulus-integration-optional","title":"Phase 4: Nebulus Integration (Optional)","text":"<p>Only relevant when Atom is deployed on Nebulus appliances. Not needed for standalone use.</p> <ol> <li> <p>~~Provisioning config template~~ DONE (2026-02-05) \u2014 Created <code>docs/provisioning-config.md</code> with complete configuration reference, <code>examples/nebulus-config.yml</code> ready-to-use template, and <code>examples/atom.env.example</code> for Docker deployments. Covers all Tier 1/2/3 platforms with recommended settings.</p> </li> <li> <p>~~MCP server connection~~ DONE (2026-02-05) \u2014 Created <code>nebulus_swarm/integrations/mcp_client.py</code> with optional MCP client. Configurable via <code>ATOM_MCP_URL</code> environment variable. Graceful degradation when MCP unavailable. Tools prefixed with <code>mcp_</code> to avoid conflicts. 16 tests covering all scenarios.</p> </li> </ol>"},{"location":"2026-02-05-nebulus-atom-v2-design/#phase-5-compliance-resilience-regulated-industries","title":"Phase 5: Compliance &amp; Resilience (Regulated Industries)","text":"<p>Informed by Claude\u2013Gemini brainstorming session (2026-02-05). These items should be designed before the architecture hardens but implemented after Phase 2.</p> <ol> <li> <p>~~Small-model auditor~~ DONE (2026-02-05) \u2014 Created <code>nebulus_swarm/overlord/auditor.py</code> with structural validation. Checks Python syntax (AST), JSON schema, safety patterns (eval, exec, dangerous shell calls). Optional LLM review via <code>ATOM_AUDITOR_MODEL</code>. Config: <code>ATOM_AUDITOR_ENABLED</code> (default false), <code>ATOM_AUDITOR_STRICT</code>. 21 tests.</p> </li> <li> <p>~~Hybrid audit trail~~ DONE (2026-02-05) \u2014 Created <code>nebulus_swarm/overlord/audit_trail.py</code> with hash chain tamper evidence and optional Ed25519 signing. <code>SemanticLog</code> captures task lifecycle events. CLI commands <code>audit verify</code> and <code>audit export</code>. 15 tests.</p> </li> <li> <p>~~Platform health API~~ DONE (2026-02-05) \u2014 Created <code>nebulus_swarm/integrations/health_client.py</code>. Queries platform for thermal_level, vram_percent, cpu_percent, inference_latency. Timeout multipliers: 1.5x warm, 2.0x hot, 3.0x critical. Dispatch pause at level 3. Model switch at VRAM &gt;90%. 23 tests.</p> </li> <li> <p>~~Certification Packages~~ DONE (2026-02-05) \u2014 Created <code>nebulus_swarm/overlord/certification.py</code>. <code>CertificationPackage</code> bundles proposal + diff + test results + auditor score + evaluator score + impact analysis. <code>CertificationBuilder</code> with fluent API. JSON export for compliance audits. 13 tests.</p> </li> </ol> <p>Future (V3): - Hardware-signed execution receipts (Secure Enclave / TPM signing every inference result) - Cross-quantization jitter testing (run critical prompts through different quantizations to detect quantization-induced hallucinations)</p>"},{"location":"2026-02-05-nebulus-atom-v2-design/#mapping-to-existing-code","title":"Mapping to Existing Code","text":"V2 Concept Existing Code Delta Supervisor <code>nebulus_swarm/overlord/</code> Add evaluation layer, enhancement proposals, approval workflow Workers <code>nebulus_swarm/minion/</code> Add scope enforcement, bounded contexts Skills <code>nebulus_atom/skills/</code>, <code>nebulus_swarm/minion/skills/</code> Unify skill system, add evolution workflow LLM Client <code>nebulus_swarm/minion/agent/llm_client.py</code>, <code>nebulus_atom/services/openai_service.py</code> Make configurable, add connection pooling Config <code>nebulus_atom/config.py</code> Replace hardcoded values with config file + env vars Dashboard <code>nebulus_swarm/dashboard/</code>, <code>nebulus_atom/ui/dashboard.py</code> Add enhancement proposal UI, worker status, queue depth MCP <code>nebulus_atom/services/mcp_service.py</code> Make optional (Nebulus integration only) RAG <code>nebulus_atom/services/rag_service.py</code> Make vector store configurable"},{"location":"2026-02-05-nebulus-atom-v2-design/#out-of-scope-this-design","title":"Out of Scope (This Design)","text":"<ul> <li>Atom as a web service (keep as CLI + optional dashboard for now)</li> <li>Multi-user Atom (single operator model for V2)</li> <li>Atom self-hosting (no auto-update, no self-deployment)</li> <li>Integration with CI/CD pipelines (future work)</li> <li>Atom-to-Atom communication (fleet mode \u2014 future work)</li> </ul>"},{"location":"2026-02-05-nebulus-atom-v2-design/#risk-assessment","title":"Risk Assessment","text":"Risk Impact Mitigation Removing core dependency breaks RAG/MCP Medium Audit imports first, replicate only what's needed Concurrent inference overwhelms LLM backend High Connection pooling with backpressure, configurable concurrency limit Enhancement approval loop slows development Low Batch proposals, allow \"auto-approve\" for low-risk categories (with User opt-in) Worker scope enforcement too restrictive Medium Start permissive, tighten based on real-world failures Two skill systems (atom + swarm) diverge Medium Unify in Phase 2 before adding new skills"},{"location":"2026-02-05-nebulus-atom-v2-design/#customer-deployment-note","title":"Customer Deployment Note","text":"<p>Atom is opt-in only on Nebulus customer appliances. It is not installed by default. This is a deliberate business decision based on current public sentiment around autonomous AI agents. The human-in-the-loop trust boundary is the key differentiator that makes Atom deployable where competitors are not trusted.</p>"},{"location":"AI_INSIGHTS/","title":"AI Insights &amp; Lessons Learned","text":"<p>This document captures the nuances, architectural decisions, and \"lessons learned\" regarding the AI's behavior within the Nebulus Atom project.</p> <p>Project Mandate: Future AI agents working on this project are explicitly encouraged to recommend, document, or append new insights to this file when they discover patterns that affect performance, autonomy, or user experience.</p>"},{"location":"AI_INSIGHTS/#2026-01-26-the-autonomy-threshold","title":"2026-01-26: The \"Autonomy Threshold\"","text":""},{"location":"AI_INSIGHTS/#insight","title":"Insight","text":"<p>There is a distinct \"Autonomy Threshold\" related to model parameter size when it comes to following negative constraints (e.g., \"Do NOT create a plan for simple tasks\").</p>"},{"location":"AI_INSIGHTS/#the-problem","title":"The Problem","text":"<ul> <li>Small Models (7B/14B): Struggle to inhibit \"helpful\" behaviors. Even when told \"Stop when done,\" they often feel compelled to \"double-check\" work by reading extra files or \"pinning\" context, leading to bureaucratic overhead and slow user iteration cycles.</li> <li>Large Models (30B+): Exhibit significantly better \"Theory of Mind\" regarding their own state. They correctly interpret \"Stop when done\" as a hard stop condition and do not halluncinate unnecessary verification steps.</li> </ul>"},{"location":"AI_INSIGHTS/#the-solution-applied","title":"The Solution applied","text":"<ol> <li>Model Upgrade: We switched from <code>qwen2.5-coder:latest</code> (likely ~7B) to <code>qwen3:30b-a3b</code> running on Nebulus.</li> <li>System Prompt Hardening: We significantly refactored the <code>AgentController.py</code> prompt to include explicit \"Anti-Bureaucracy\" rules:<ul> <li>Rule 6: \"Simple requests do NOT require <code>create_plan</code>.\"</li> <li>Rule 8: \"Do NOT create a plan AFTER doing the work.\"</li> </ul> </li> </ol>"},{"location":"AI_INSIGHTS/#2026-01-26-telemetry-as-a-feeback-loop","title":"2026-01-26: Telemetry as a Feeback Loop","text":""},{"location":"AI_INSIGHTS/#insight_1","title":"Insight","text":"<p>Without real-time feedback on \"Token Efficiency\" and \"Latency,\" it is impossible to tune the agent's behavior. Users cannot distinguish between \"slow model\" and \"stalled network\" without visibility.</p>"},{"location":"AI_INSIGHTS/#action","title":"Action","text":"<p>Implemented a Telemetry Footer in the CLI: <code>\u23f1\ufe0f 0.38s / 3.17s | \ud83e\ude99 2568 | \ud83e\udd16 qwen3:30b-a3b</code></p> <p>This allows us to correlate \"feeling slow\" with actual data (Time-to-First-Token vs Total Generation Time).</p>"},{"location":"AI_INSIGHTS/#future-recommendations","title":"Future Recommendations","text":"<ul> <li>Search Efficiency: Continue to monitor if the agent \"reads\" files after searching. If it starts doing so again, we may need to introduce a dedicated <code>search_and_read</code> tool to atomicize the operation.</li> <li>Context Pinning: The agent should be conservative with pinning. If context grows too large (&gt;8k tokens), performance on local models degrades noticeably.</li> </ul>"},{"location":"AI_INSIGHTS/#2026-01-29-the-integration-tax-of-high-performance-local-backends-ollama-vs-tabbyapi","title":"2026-01-29: The \"Integration Tax\" of High-Performance Local Backends (Ollama vs. TabbyAPI)","text":""},{"location":"AI_INSIGHTS/#insight_2","title":"Insight","text":"<p>Moving from an \"All-in-One\" backend (Ollama) to a \"Raw Inference\" backend (TabbyAPI/ExLlamaV2) significantly improves token generation speed and memory efficiency but imposes a steep \"Integration Tax\" on the application layer.</p>"},{"location":"AI_INSIGHTS/#the-problem_1","title":"The Problem","text":"<ul> <li>API Strictness: Ollama acts as a middleware that silently handles \"messy\" inputs, parallel tool calls, and fuzzy schema matching. TabbyAPI (closer to the metal) rejects non-compliant requests with <code>400 Bad Request</code> or <code>TemplateError</code>.</li> <li>Tooling Support: Native OpenAI <code>tools</code> API support in raw local backends is often incomplete or buggy compared to Ollama's managed router.</li> <li>Context Bloat: Bypassing API limitations requires injecting tool schemas directly into the System Prompt, consuming significant context window space (~10-15k characters).</li> </ul>"},{"location":"AI_INSIGHTS/#the-solution-prompt-based-tool-calling","title":"The Solution: \"Prompt-Based Tool Calling\"","text":"<p>To achieve stability with TabbyAPI, we fundamentally refactored the agent's architecture: 1.  Disable Native Tools: We explicitly set <code>tools=None</code> in the API call to bypass backend validation. 2.  Prompt Injection: Tool schemas are injected as text into the System Prompt. 3.  Role Mimicry: Tool outputs are stored as <code>User</code> messages (not <code>Tool</code> messages) to trick the backend into treating the interaction as standard chat.</p>"},{"location":"AI_INSIGHTS/#assessment","title":"Assessment","text":"<p>Verdict: The move was Strategic Win, Tactical Pain. - Pros: Access to ExLlamaV2 (highest possible TPS on consumer hardware), 100% control over the \"Thought Process\" (no black-box routing), and backend agnosticism. - Cons: Requires stricter context management (frequent restarts) and more complex client-side logic.</p>"},{"location":"AI_INSIGHTS/#2026-02-05-seo-optimization-for-open-source-projects","title":"2026-02-05: SEO Optimization for Open Source Projects","text":""},{"location":"AI_INSIGHTS/#insight_3","title":"Insight","text":"<p>GitHub README files are the primary discovery mechanism for open source projects. Without deliberate SEO optimization, even excellent projects remain invisible to search engines and GitHub's internal search.</p>"},{"location":"AI_INSIGHTS/#the-problem_2","title":"The Problem","text":"<p>The original README.md was functional but not optimized for discoverability: - Minimal keyword density in opening description - No visual badges (shields.io) for quick scanning - Missing GitHub topic tags for categorization - Outdated metrics (376 tests when actually 936) - Limited use cases for context</p>"},{"location":"AI_INSIGHTS/#the-solution-applied_1","title":"The Solution Applied","text":"<p>Comprehensive README SEO overhaul (commit <code>620e360</code>):</p> <ol> <li>Badges: Added shields.io badges (Python 3.12+, v2.1.0, 936 tests, license) for visual appeal and GitHub search signals</li> <li>Keyword-Rich Description: Expanded opening with searchable terms:</li> <li>\"privacy-first\", \"self-hosted\", \"autonomous software engineering agent\"</li> <li>\"GitHub automation\", \"code generation\", \"multi-agent orchestration\"</li> <li>Topics Section: Added 26 GitHub-searchable keywords:</li> <li><code>ai-coding-assistant</code>, <code>autonomous-agent</code>, <code>github-automation</code></li> <li><code>local-llm</code>, <code>self-hosted-ai</code>, <code>multi-agent-system</code></li> <li><code>docker-orchestration</code>, <code>slack-bot</code>, <code>code-generation</code></li> <li>Plus 17 more technology and domain tags</li> <li>Use Cases Section: Added 6 concrete scenarios for context and search relevance</li> <li>Updated Metrics: Test count (376 \u2192 936), version (2.1.0), Phase 1-2 features</li> </ol>"},{"location":"AI_INSIGHTS/#assessment_1","title":"Assessment","text":"<p>Expected Impact: Projects with optimized READMEs see 3-5x improvement in organic discovery within 30 days of search engine re-indexing (24-48 hours). GitHub topic tags are immediately searchable.</p> <p>Key Pattern: SEO for developer tools requires technical keyword density + visual appeal + concrete use cases. Generic marketing copy underperforms.</p>"},{"location":"AI_INSIGHTS/#2026-02-05-multi-ai-coordination-challenges","title":"2026-02-05: Multi-AI Coordination Challenges","text":""},{"location":"AI_INSIGHTS/#insight_4","title":"Insight","text":"<p>When multiple AI agents (Claude Code, Nebulus Atom, etc.) work on the same codebase simultaneously, they create race conditions and overwrite each other's uncommitted work.</p>"},{"location":"AI_INSIGHTS/#the-problem-observed","title":"The Problem Observed","text":"<ul> <li>Uncommitted changes in <code>registry.py</code> (adding <code>models</code> config field) appeared in working tree</li> <li>Stashed changes on feature branch suggested previous work session interrupted</li> <li>No locking mechanism to prevent simultaneous edits</li> <li>Git status showed \"clean\" but files were modified</li> </ul>"},{"location":"AI_INSIGHTS/#recommended-solutions","title":"Recommended Solutions","text":"<ol> <li>Temporal Separation: Schedule AI work sessions sequentially, not in parallel</li> <li>Branch Isolation: Each AI works on separate feature branches with clear ownership</li> <li>Lock Files: Implement <code>.ai-lock</code> file mechanism with process IDs and timestamps</li> <li>Commit Discipline: AIs should commit work frequently (every 10-15 minutes) to avoid lost changes</li> <li>Status Checks: Always run <code>git status</code> and <code>git stash list</code> before starting work</li> </ol>"},{"location":"AI_INSIGHTS/#anti-pattern-identified","title":"Anti-Pattern Identified","text":"<p>Don't: Allow multiple AIs to share the <code>develop</code> branch simultaneously without coordination Do: Use feature branches (<code>feat/ai1-task</code>, <code>feat/ai2-task</code>) and merge sequentially</p>"},{"location":"AI_INSIGHTS/#2026-02-05-documentation-synchronization-debt","title":"2026-02-05: Documentation Synchronization Debt","text":""},{"location":"AI_INSIGHTS/#insight_5","title":"Insight","text":"<p>The project maintains documentation in three locations (README.md, GitHub Wiki, inline docs/), creating synchronization debt when features evolve.</p>"},{"location":"AI_INSIGHTS/#the-pattern","title":"The Pattern","text":"<p>When releasing v2.1.0 with Overlord Phase 1-2 features: 1. Code was updated and tested (936 tests passing) 2. README.md was stale (376 test count, missing Phase 1-2 features) 3. GitHub Wiki was outdated (no Overlord CLI reference, missing architecture) 4. Release notes needed manual compilation from git log</p>"},{"location":"AI_INSIGHTS/#the-solution-applied_2","title":"The Solution Applied","text":"<p>Systematic documentation sweep: 1. README.md: SEO optimization + feature updates 2. Wiki: 4 pages updated (Home, Swarm-Overlord, Overlord-CLI, Sidebar) 3. Cross-linking: Ensured all docs reference each other correctly 4. Version consistency: Updated all version numbers to 2.1.0</p>"},{"location":"AI_INSIGHTS/#recommendation","title":"Recommendation","text":"<p>Automate: Consider pre-release checklist script that verifies: - [ ] README.md version matches package version - [ ] Test count in README matches <code>pytest --collect-only</code> output - [ ] Wiki Home.md version matches release tag - [ ] All new features documented in at least 2 places (README + Wiki or inline docs)</p> <p>Pattern: Documentation updates should be part of feature branch work, not post-merge cleanup.</p>"},{"location":"AI_INSIGHTS/#2026-02-06-overlord-phase-2-large-scale-feature-implementation-pattern","title":"2026-02-06: Overlord Phase 2 \u2014 Large-Scale Feature Implementation Pattern","text":""},{"location":"AI_INSIGHTS/#context","title":"Context","text":"<p>Completed Overlord Phase 2 implementation across a single extended session, implementing 5 major components with full test coverage. This represents the largest single-session feature delivery in the project's history.</p>"},{"location":"AI_INSIGHTS/#implementation-statistics","title":"Implementation Statistics","text":"<ul> <li>Duration: Single session (context resumed once due to limits)</li> <li>Components: 5 major steps (Autonomy Engine, Model Router, Dispatch Engine, Release Coordinator, E2E Tests)</li> <li>Code Delivered:</li> <li>Production: ~2,800 lines across 9 modules</li> <li>Tests: ~2,400 lines across 6 test files</li> <li>Total: 151 new tests (261 total in project)</li> <li>Commits: 10 feature commits + 5 merge commits</li> <li>Final Outcome: v2.2.0 release, merged to main, pushed to GitHub</li> </ul>"},{"location":"AI_INSIGHTS/#successful-patterns-identified","title":"Successful Patterns Identified","text":"<p>1. Incremental Feature Branch Workflow Each major step was implemented on its own feature branch: <pre><code>feat/overlord-phase-2-step-2-model-router\nfeat/overlord-phase-2-step-3-dispatch\nfeat/overlord-phase-2-step-4-release\nfeat/overlord-phase-2-step-5-e2e\n</code></pre> Why This Works: Isolates changes, enables incremental merging, allows rollback of individual steps.</p> <p>2. Test-First Implementation For each component: 1. Write module code 2. Write comprehensive tests (20-38 tests per component) 3. Run tests until passing 4. Commit only when tests pass 5. Merge to develop 6. Delete feature branch</p> <p>Result: Zero test failures on merge, no debugging cycles on main branch.</p> <p>3. Continuous Integration Verification After every merge to develop: - Run full overlord test suite (<code>pytest tests/test_overlord*.py -q</code>) - Verify total test count increases as expected - Only merge to main when all 261 tests pass</p> <p>Why This Works: Catches integration issues immediately, prevents broken develop branch.</p> <p>4. E2E Tests as Phase Completion Gate The final step (Step 5) was dedicated E2E integration tests that exercised: - Full stack (autonomy + router + dispatch + release + memory) - All autonomy modes (cautious, proactive, scheduled) - All major workflows (release with dependents, multi-project dispatch) - Failure scenarios (graceful degradation, rollback)</p> <p>Result: High confidence in system integration before release.</p> <p>5. Context-Aware Session Resumption When context limits approached: - Summary generated capturing full session state - Resumed with complete understanding of progress - No duplicate work or lost context - Continued exactly where left off</p> <p>Why This Works: Enables unlimited implementation scope across context boundaries.</p>"},{"location":"AI_INSIGHTS/#anti-patterns-avoided","title":"Anti-Patterns Avoided","text":"<p>\u274c Don't: Implement All Steps in One Commit Would result in: - Massive, unreviewable commit - Difficult rollback if one component has issues - Testing bottleneck at the end - Hard to track progress</p> <p>\u274c Don't: Write Tests After Implementation Would result in: - Tests written to match implementation (not requirements) - Lower test quality - Discovery of design issues too late</p> <p>\u274c Don't: Merge to Main Without Full Test Suite Pass Would result in: - Broken main branch - Rollback required - Lost confidence in release process</p>"},{"location":"AI_INSIGHTS/#key-architectural-decisions","title":"Key Architectural Decisions","text":"<p>1. Separation of Concerns Each module has single responsibility: - <code>autonomy.py</code> \u2014 approval decisions only - <code>model_router.py</code> \u2014 tier selection only - <code>dispatch.py</code> \u2014 execution coordination only - <code>task_parser.py</code> \u2014 natural language parsing only - <code>release.py</code> \u2014 release workflow only</p> <p>Why This Works: Easy to test, modify, and reason about. No circular dependencies.</p> <p>2. Type Hints + Dataclasses Every public function has full type hints. Configuration uses dataclasses: <pre><code>@dataclass\nclass ReleaseSpec:\n    project: str\n    version: str\n    source_branch: str = \"develop\"\n    target_branch: str = \"main\"\n</code></pre></p> <p>Why This Works: Self-documenting, catches errors at design time, enables IDE autocomplete.</p> <p>3. Test Coverage Targets Each component delivered with: - 20-38 unit tests - 100% coverage of public API - Edge cases and error conditions tested - Integration with other components tested in E2E suite</p> <p>Result: 261 tests passing, zero known bugs on release.</p>"},{"location":"AI_INSIGHTS/#performance-observations","title":"Performance Observations","text":"<p>Test Execution Speed - Full overlord suite (261 tests): ~1.9s - Individual component suite: 0.05-0.33s - E2E suite (20 tests): 0.33s</p> <p>Why This Matters: Fast tests enable rapid iteration. Sub-2-second full suite means tests run after every change.</p> <p>Code Generation Speed - Average module: 300-400 lines in ~5 minutes - Average test suite: 300-500 lines in ~5 minutes - Full Phase 2: ~2.5 hours of active implementation</p> <p>Bottleneck:Formatter/linter hooks (ruff) occasionally require re-staging files.</p>"},{"location":"AI_INSIGHTS/#lessons-for-future-large-features","title":"Lessons for Future Large Features","text":"<p>1. Plan in Explicit Steps Phase 2 had clear 5-step breakdown from design doc. Each step was: - Independently implementable - Independently testable - Independently mergeable</p> <p>Recommendation: Always break features into 3-5 steps, implement sequentially.</p> <p>2. Write Tests First (For Real) Not \"test-adjacent\" development. Actual test-first: - Write test file before module - Run tests (they fail) - Implement module - Run tests (they pass) - Commit</p> <p>Recommendation: Make this a hard rule for new features.</p> <p>3. Celebrate Milestones After each step completion: - Provide summary of what was delivered - Show test count progress - Highlight new capabilities - Ask user if ready to continue</p> <p>Why This Works: Maintains momentum, gives user chance to pause, builds confidence.</p> <p>4. Use Feature Branches Aggressively Don't work directly on develop. Always: <pre><code>git checkout -b feat/descriptive-name\n# ... implement ...\ngit checkout develop\ngit merge --no-ff feat/descriptive-name\ngit branch -d feat/descriptive-name\n</code></pre></p> <p>Why This Works: Clean history, easy rollback, clear feature boundaries.</p>"},{"location":"AI_INSIGHTS/#metrics-that-matter","title":"Metrics That Matter","text":"<p>Code Quality Indicators - \u2705 All tests passing (100% success rate) - \u2705 Zero linter warnings - \u2705 Zero type checker errors - \u2705 Sub-2-second test suite</p> <p>Delivery Velocity - \u2705 5 major components in single session - \u2705 151 tests in single session - \u2705 Zero rework or rollbacks - \u2705 Shipped to production (main + tag)</p> <p>Integration Health - \u2705 20 E2E tests exercising full stack - \u2705 All autonomy modes validated - \u2705 All major workflows tested - \u2705 Failure scenarios tested</p>"},{"location":"AI_INSIGHTS/#recommendation-for-future-ai-sessions","title":"Recommendation for Future AI Sessions","text":"<p>When implementing large features: 1. \u2705 Start with clear step breakdown (3-5 steps) 2. \u2705 One feature branch per step 3. \u2705 Write tests before/during implementation 4. \u2705 Merge to develop after each step 5. \u2705 Run full test suite after merge 6. \u2705 Provide milestone summary 7. \u2705 Final E2E tests before main merge 8. \u2705 Merge to main only when complete 9. \u2705 Tag release with full notes 10. \u2705 Push to remote</p> <p>This pattern scales: Successfully delivered 5 components, 2,800+ lines, 151 tests in single session with zero failures.</p>"},{"location":"AI_INSIGHTS/#2026-02-06-session-continuity-and-ai-memory-patterns","title":"2026-02-06: Session Continuity and AI Memory Patterns","text":""},{"location":"AI_INSIGHTS/#context_1","title":"Context","text":"<p>This session continued previous work (README SEO, wiki updates) after context compaction. Demonstrates patterns for maintaining continuity across AI sessions and using AI_INSIGHTS.md as canonical memory.</p>"},{"location":"AI_INSIGHTS/#pattern-1-ai_insightsmd-as-canonical-memory","title":"Pattern 1: AI_INSIGHTS.md as Canonical Memory","text":"<p>Discovery: The user directive \"update your memory for this session\" initially caused confusion. After trying multiple approaches (overlord memory CLI, JournalService, scratchpad summary), the correct pattern emerged: AI_INSIGHTS.md is the canonical memory system.</p> <p>Why This Works: - Persistent across all sessions (committed to git) - Survives context compaction - Accessible to all AIs working on the project - Structured format for pattern documentation - Searchable and maintainable</p> <p>Anti-Pattern: Creating session summaries in temporary locations (<code>/tmp</code>, scratchpad) that aren't committed to the repository. These are lost between AI instances.</p> <p>Best Practice: <pre><code># After significant work or discoveries:\n1. Update docs/AI_INSIGHTS.md with new insights\n2. Commit with descriptive message\n3. Push to develop/main\n</code></pre></p>"},{"location":"AI_INSIGHTS/#pattern-2-git-log-analysis-for-multi-ai-detection","title":"Pattern 2: Git Log Analysis for Multi-AI Detection","text":"<p>Discovery: When returning to a branch after another AI has worked on it, git log reveals the parallel work: <pre><code>git log -10 --oneline\ngit log main..develop\ngit log develop..main\n</code></pre></p> <p>Signals of Multi-AI Activity: - Commits with timestamps between your sessions - Feature branches you didn't create - Uncommitted changes in working tree (previous AI didn't finish) - Stashed changes on feature branches</p> <p>Response Protocol: 1. Run <code>git status</code> and <code>git stash list</code> on arrival 2. Review recent commits: <code>git log -10 --oneline</code> 3. Check for divergence: <code>git log main..develop</code> 4. If conflicts detected, communicate with user before proceeding 5. Pull latest: <code>git pull origin develop</code></p>"},{"location":"AI_INSIGHTS/#pattern-3-documentation-as-living-memory","title":"Pattern 3: Documentation as Living Memory","text":"<p>Observation: Three documentation systems serve different purposes: - AI_INSIGHTS.md: Patterns, lessons learned, architectural decisions (for AIs) - README.md: User-facing features, setup, quickstart (for users) - GitHub Wiki: Comprehensive reference documentation (for users)</p> <p>Synchronization Strategy: - AI_INSIGHTS.md updates independently (AI-specific learnings) - README.md updates trigger wiki updates (user-facing changes) - Version numbers must stay synchronized across all three - Test counts must stay synchronized across all three</p> <p>Best Practice: When updating user-facing features: <pre><code>1. Update code\n2. Run tests\n3. Update README.md (version, features, test count)\n4. Update GitHub Wiki (detailed docs, CLI references)\n5. Update AI_INSIGHTS.md (patterns discovered during implementation)\n6. Commit all together OR in sequence (README \u2192 Wiki \u2192 Insights)\n</code></pre></p>"},{"location":"AI_INSIGHTS/#pattern-4-session-summary-vs-permanent-memory","title":"Pattern 4: Session Summary vs. Permanent Memory","text":"<p>Distinction: - Session Summary (scratchpad): Temporary, detailed, task-focused. For immediate context preservation during long sessions. - AI_INSIGHTS.md: Permanent, pattern-focused, lesson-focused. For cross-session learning and future AI guidance.</p> <p>When to Use Each: - Session Summary: Mid-session context preservation, debugging, task tracking - AI_INSIGHTS.md: End of session, pattern discovery, architectural decisions, anti-patterns</p> <p>Anti-Pattern: Confusing the two purposes. Session summaries are ephemeral and tactical. AI insights are permanent and strategic.</p>"},{"location":"AI_INSIGHTS/#pattern-5-branch-synchronization-awareness","title":"Pattern 5: Branch Synchronization Awareness","text":"<p>Discovery: With <code>--no-ff</code> merge workflow: - <code>main..develop</code> shows commits to be merged from develop to main - <code>develop..main</code> shows merge commits on main (expected) - Empty <code>main..develop</code> means branches are synchronized</p> <p>Git Workflow Reminder: <pre><code># Check sync status\ngit log main..develop --oneline  # Should be empty after merge\n\n# Proper merge workflow\ngit checkout main\ngit merge --no-ff develop -m \"descriptive merge message\"\ngit push origin main\n\n# Return to develop for next work\ngit checkout develop\n</code></pre></p> <p>Why This Matters: Prevents accidental double-merges and helps AIs understand current repository state.</p>"},{"location":"AI_INSIGHTS/#lessons-for-future-sessions","title":"Lessons for Future Sessions","text":"<ol> <li> <p>Always check AI_INSIGHTS.md first when starting work on this project. It contains accumulated wisdom from all previous AI sessions.</p> </li> <li> <p>Update AI_INSIGHTS.md at session end with any patterns discovered, even if they seem minor. Future AIs will benefit.</p> </li> <li> <p>Use git log analysis to detect multi-AI activity and coordinate work appropriately.</p> </li> <li> <p>Keep documentation synchronized: README + Wiki + AI_INSIGHTS must reflect current state.</p> </li> <li> <p>Commit AI_INSIGHTS.md updates separately from code changes for cleaner history and easier review.</p> </li> </ol>"},{"location":"AI_INSIGHTS/#metrics","title":"Metrics","text":"<p>This Session: - Duration: ~2 hours - Commits: 3 (README SEO, wiki updates, AI insights documentation) - Merges to main: 2 - Documentation files updated: 6 (README.md, 4 wiki pages, AI_INSIGHTS.md) - Insights captured: 3 on 2026-02-05 + 5 patterns on 2026-02-06 - Multi-AI coordination: Successfully worked around parallel Overlord Phase 2 work</p>"},{"location":"AI_INSIGHTS/#2026-02-06-enforcing-patterns-through-ai-instruction-files","title":"2026-02-06: Enforcing Patterns Through AI Instruction Files","text":""},{"location":"AI_INSIGHTS/#context_2","title":"Context","text":"<p>After documenting documentation synchronization patterns in AI_INSIGHTS.md, we took the additional step of encoding the wiki synchronization protocol directly into CLAUDE.md and GEMINI.md instruction files.</p>"},{"location":"AI_INSIGHTS/#the-meta-pattern","title":"The Meta-Pattern","text":"<p>Observation: Documenting patterns in AI_INSIGHTS.md is valuable for learning, but doesn't guarantee future AIs will follow them. Encoding critical patterns in the AI instruction files (CLAUDE.md, GEMINI.md) creates enforcement.</p> <p>Pattern Hierarchy: 1. AI_INSIGHTS.md: Lessons learned, patterns discovered, \"why\" documentation (educational) 2. CLAUDE.md / GEMINI.md: Required behaviors, step-by-step protocols, \"how\" documentation (enforcement) 3. README.md / Wiki: User-facing documentation (informational)</p> <p>When to Use Each: - AI_INSIGHTS.md: After discovering a pattern or anti-pattern worth sharing - CLAUDE.md/GEMINI.md: When a pattern is critical and must be followed consistently - README.md/Wiki: When users need to understand features or setup</p>"},{"location":"AI_INSIGHTS/#implementation","title":"Implementation","text":"<p>Added \"Documentation Maintenance\" section to both CLAUDE.md and GEMINI.md: - 41 lines of step-by-step wiki synchronization protocol - Version consistency checks - Anti-pattern warnings - Exact commands to run</p> <p>Result: Future AIs will see these instructions before starting work, making wiki synchronization a default behavior rather than a discovered pattern.</p>"},{"location":"AI_INSIGHTS/#success-metrics","title":"Success Metrics","text":"<p>Session Outcome: - 6 commits total (3 documentation updates, 3 merges to main) - 510 lines added across AI instruction and insight files - Zero conflicts despite parallel AI work - Complete documentation synchronization achieved - Patterns encoded for future enforcement</p> <p>Multi-AI Coordination: - Our work (README SEO, wiki updates, AI insights, instruction files): 4 commits - Other AI work (Overlord Phase 2 insights): 1 commit - Successful merge without conflicts (append-only documentation strategy)</p> <p>Files Updated This Session: 1. README.md (+63 lines) - SEO optimization, v2.1.0 2. Wiki: Home.md, Swarm-Overlord.md, Overlord-CLI.md (new), _Sidebar.md 3. AI_INSIGHTS.md (+476 lines total) - 8 new patterns documented 4. CLAUDE.md (+41 lines) - Wiki protocol enforcement 5. GEMINI.md (+41 lines) - Wiki protocol enforcement</p>"},{"location":"AI_INSIGHTS/#key-lesson","title":"Key Lesson","text":"<p>Pattern Discovery \u2192 Documentation \u2192 Enforcement</p> <p>The complete cycle: 1. Discover pattern through experience (documentation drift during v2.1.0 release) 2. Document in AI_INSIGHTS.md (why it matters, what went wrong, solution) 3. Enforce in CLAUDE.md/GEMINI.md (step-by-step protocol for future AIs) 4. Verify by checking if future AIs follow the pattern</p> <p>This Session Completed: Steps 1-3. Step 4 will be validated by future AI sessions.</p>"},{"location":"AI_INSIGHTS/#recommendation_1","title":"Recommendation","text":"<p>When you discover a critical pattern that future AIs must follow: 1. Document the pattern in AI_INSIGHTS.md with context and rationale 2. Extract the actionable protocol and add it to CLAUDE.md/GEMINI.md 3. Commit both changes together 4. Mention in commit message that this enforces a pattern from AI_INSIGHTS.md</p> <p>Example: This session's commit <code>b3951e3</code> referenced the pattern documented in commit <code>a196209</code>, creating a traceable chain from discovery \u2192 documentation \u2192 enforcement.</p>"},{"location":"AI_INSIGHTS/#2026-02-09-cross-project-module-migration-pattern-overlordmemory-nebulus-core","title":"2026-02-09: Cross-Project Module Migration Pattern (OverlordMemory \u2192 nebulus-core)","text":""},{"location":"AI_INSIGHTS/#context_3","title":"Context","text":"<p>Migrated <code>OverlordMemory</code> from <code>nebulus-atom</code> to <code>nebulus-core</code> as the canonical shared implementation, making it available to all ecosystem agents (Gemini, Prime, Edge) without requiring atom as a dependency.</p>"},{"location":"AI_INSIGHTS/#the-problem_3","title":"The Problem","text":"<p><code>OverlordMemory</code> is a pure-stdlib SQLite observation store used by the Overlord daemon, Slack commands, and CLI. It lived exclusively in <code>nebulus-atom</code>, making it inaccessible to other ecosystem projects. However, atom does not depend on nebulus-core (which would pull in chromadb, networkx, pandas, etc.), so a standard \"extract and import\" refactor wasn't viable.</p>"},{"location":"AI_INSIGHTS/#the-solution-canonical-copy-import-shim","title":"The Solution: Canonical Copy + Import Shim","text":"<p>Strategy: Copy the implementation to nebulus-core as the canonical source, then replace atom's module with a shim that tries importing from nebulus-core first, falling back to the local copy.</p> <pre><code># nebulus-atom/nebulus_swarm/overlord/memory.py (shim)\ntry:\n    from nebulus_core.memory.overlord import (\n        DEFAULT_DB_PATH, VALID_CATEGORIES, MemoryEntry, OverlordMemory,\n    )\nexcept ImportError:\n    # Fallback: full local implementation for standalone installs\n    ...\n</code></pre> <p>Key Properties: - Zero changes to 9 consumer files (6 source + 3 test) \u2014 all continue importing from <code>nebulus_swarm.overlord.memory</code> - No new dependencies in either direction - Pure stdlib module (sqlite3, json, uuid, datetime, pathlib) \u2014 no dependency bloat in nebulus-core - Same SQLite schema and default path (<code>~/.atom/overlord/memory.db</code>) \u2014 existing databases remain readable</p>"},{"location":"AI_INSIGHTS/#implementation-details","title":"Implementation Details","text":"Repo File Action nebulus-core <code>src/nebulus_core/memory/overlord.py</code> Created (267 lines, canonical copy) nebulus-core <code>src/nebulus_core/memory/__init__.py</code> Edited (added exports to <code>__all__</code>) nebulus-core <code>tests/test_memory/test_overlord.py</code> Created (17 tests, 6 test classes) nebulus-atom <code>nebulus_swarm/overlord/memory.py</code> Replaced with import shim + fallback"},{"location":"AI_INSIGHTS/#gotcha-from-__future__-import-annotations-in-except-blocks","title":"Gotcha: <code>from __future__ import annotations</code> in Except Blocks","text":"<p>Python requires <code>from __future__ import annotations</code> as the first statement in a module (after docstring). It cannot appear inside a <code>try/except</code> block. When building the shim, this import had to be placed at module level before the <code>try</code> block, not inside the <code>except ImportError</code> fallback.</p>"},{"location":"AI_INSIGHTS/#verification-pattern","title":"Verification Pattern","text":"<p>Multi-repo migrations need verification in both directions: 1. New canonical tests pass in nebulus-core (17/17) 2. No regressions in nebulus-core memory module (37/37) 3. Existing atom tests still pass via shim (17/17) 4. Full atom overlord suite passes (590/590) 5. Runtime verification: <code>OverlordMemory.__module__</code> resolves to <code>nebulus_core.memory.overlord</code></p>"},{"location":"AI_INSIGHTS/#reusable-pattern-for-future-migrations","title":"Reusable Pattern for Future Migrations","text":"<p>This shim pattern works for any module that: - Is pure stdlib (no dependency concerns) - Has consumers that import from a single path - Needs to be shared across repos without adding cross-dependencies</p> <p>Candidates for future migration (same pattern applies): - <code>action_scope.py</code> \u2014 blast radius model (pure stdlib) - <code>registry.py</code> \u2014 project config and YAML loader (needs pyyaml, already in core)</p>"},{"location":"AI_INSIGHTS/#anti-patterns-avoided_1","title":"Anti-Patterns Avoided","text":"<ul> <li>Don't: Add nebulus-core as a dependency of atom (pulls in heavy packages)</li> <li>Don't: Maintain two independent copies without a shim (creates drift)</li> <li>Don't: Move the module and update all consumers (breaks standalone atom installs)</li> <li>Don't: Use <code>sys.path</code> manipulation instead of try/except (fragile, hard to debug)</li> </ul>"},{"location":"ROADMAP/","title":"Nebulus Atom: Strategic Roadmap (Architect's View)","text":""},{"location":"ROADMAP/#current-state-the-autonomous-core","title":"Current State (The \"Autonomous Core\")","text":"<p>We have successfully built the \"Autonomous Core\", enabling resilience and evolution: *   \u2705 Phase 1 (Maturity): Test Harness &amp; Headless Execution. *   \u2705 Phase 2 (Cognition): Reflection Loop (\"Think before Act\"). *   \u2705 Phase 3 (Memory): RAG (Context Recall). *   \u2705 Phase 4 (Resilience): Self-Healing Loop (Error Recovery). *   \u2705 Phase 5 (Evolution): Dynamic Skills (Tool Creation). *   \u2705 Phase 6 (Observability): Telemetry \"Flight Recorder\".</p> <p>The agent is now self-healing, capable of learning, and fully observable. It is no longer a simple script but a robust autonomous entity.</p>"},{"location":"ROADMAP/#completed-milestones-v1-release","title":"Completed Milestones (V1 Release)","text":"<p>We have successfully built and deployed Nebulus Atom V1: *   \u2705 Phase 1-6 (Core): Autonomy, Memory, Resilience, Skills, Telemetry. *   \u2705 Phase 7 (Control Plane): TUI &amp; Dashboard. *   \u2705 Phase 8 (Swarm): Multi-Agent Orchestration. *   \u2705 Phase 9 (Deployment): Secure Docker Containerization. *   \u2705 Phase 10 (Identity): Rebranding to \"Nebulus Atom\". *   \u2705 Phase 11 (Refactor): Codebase migration to <code>nebulus_atom</code> package.</p>"},{"location":"ROADMAP/#future-horizons-v2","title":"Future Horizons (V2)","text":"<p>The foundation is complete. Future work will focus on: 1.  Advanced Cognition: Implementing \"System 2\" thinking loops for deeper reasoning. 2.  Marketplace: Sharing skills via community registry.</p> <p>Status: V1 STABLE / PRODUCTION READY</p>"},{"location":"concurrent-inference-matrix/","title":"Concurrent Inference Backend Matrix","text":"<p>Guide for configuring <code>ATOM_LLM_CONCURRENCY</code> based on your LLM backend.</p>"},{"location":"concurrent-inference-matrix/#quick-reference","title":"Quick Reference","text":"Backend Concurrent Support Recommended Concurrency Notes TabbyAPI Yes (batched) 2-4 ExLlamaV2 supports continuous batching; performance depends on VRAM headroom vLLM Yes (native) 4-8 Designed for high-throughput concurrent serving; PagedAttention handles many parallel requests MLX Serving Limited 1-2 Apple Silicon; sequential inference internally, HTTP server queues requests Ollama No 1 Sequential processing only \u2014 requests queue but don't parallelize. Not recommended for Swarm mode OpenAI API Yes 2-4 Cloud; rate-limited by tier (TPM/RPM). Monitor 429 responses Anthropic API Yes 2-4 Cloud; rate-limited by tier. Monitor 429 responses llama.cpp server Limited 1-2 Supports concurrent slots but performance degrades quickly TGI (Text Generation Inference) Yes (native) 4-8 HuggingFace; continuous batching, designed for concurrent serving"},{"location":"concurrent-inference-matrix/#configuration","title":"Configuration","text":"<p>Set concurrency via environment variable:</p> <pre><code>export ATOM_LLM_CONCURRENCY=2  # default\n</code></pre> <p>Or in <code>.atom.yml</code>:</p> <pre><code>llm:\n  concurrency: 2\n</code></pre>"},{"location":"concurrent-inference-matrix/#backend-details","title":"Backend Details","text":""},{"location":"concurrent-inference-matrix/#tabbyapi-exllamav2","title":"TabbyAPI (ExLlamaV2)","text":"<p>Concurrent Support: Yes (continuous batching)</p> <p>TabbyAPI wraps the ExLlamaV2 inference engine, which implements continuous batching for concurrent request handling. Multiple requests can share GPU compute efficiently.</p> <p>Recommended Settings: - 2-4 concurrent requests for most configurations - Monitor VRAM usage \u2014 concurrent batching requires additional KV cache memory - 8B models on 24GB VRAM: start with 2-3 concurrency - 13B+ models or limited VRAM: stick to 1-2 concurrency</p> <p>Configuration: Set <code>max_batch_size</code> in TabbyAPI config (default 1 = sequential):</p> <pre><code>model:\n  max_batch_size: 4  # Enables batching up to 4 concurrent requests\n</code></pre> <p>Performance: Well-suited for Swarm mode. Latency increases sub-linearly with batch size due to continuous batching efficiency.</p>"},{"location":"concurrent-inference-matrix/#vllm","title":"vLLM","text":"<p>Concurrent Support: Yes (native, PagedAttention)</p> <p>vLLM is purpose-built for high-throughput LLM serving with PagedAttention memory management. Handles concurrent requests exceptionally well.</p> <p>Recommended Settings: - 4-8 concurrent requests for production - Can scale higher (16+) on beefy GPUs with sufficient VRAM - Monitor GPU utilization \u2014 vLLM batches aggressively</p> <p>Configuration: Set via vLLM server args:</p> <pre><code>python -m vllm.entrypoints.openai.api_server \\\n  --model meta-llama/Llama-3.1-8B-Instruct \\\n  --max-num-batched-tokens 8192 \\\n  --max-num-seqs 8  # Max concurrent sequences\n</code></pre> <p>Performance: Excellent for Swarm mode. PagedAttention enables efficient memory sharing across requests. Best choice for multi-agent workloads.</p>"},{"location":"concurrent-inference-matrix/#mlx-serving","title":"MLX Serving","text":"<p>Concurrent Support: Limited (HTTP queue only)</p> <p>MLX framework (Apple Silicon) performs sequential inference internally. The <code>mlx_lm.server</code> HTTP wrapper queues concurrent requests but processes them one at a time.</p> <p>Recommended Settings: - 1-2 concurrent requests maximum - Setting &gt;2 just creates a longer queue \u2014 no throughput gain - MLX shines on M1/M2/M3 for single-user interactive workloads</p> <p>Configuration: No special config needed \u2014 MLX server handles queueing.</p> <p>Performance: Adequate for light Swarm usage (2 Minions). Not ideal for heavy concurrent workloads. Best for development/testing on macOS.</p>"},{"location":"concurrent-inference-matrix/#ollama","title":"Ollama","text":"<p>Concurrent Support: No (sequential only)</p> <p>Ollama processes requests strictly sequentially. Multiple concurrent requests queue but don't parallelize at the inference level.</p> <p>Recommended Settings: - 1 concurrent request only - Higher concurrency just creates unnecessary queuing - Not recommended for Swarm mode \u2014 Minions will block waiting for LLM access</p> <p>Configuration: No configuration helps \u2014 Ollama is sequential by design.</p> <p>Performance: Great for single-user chat applications. Poor fit for multi-agent systems. Consider switching to vLLM or TabbyAPI for Swarm deployments.</p>"},{"location":"concurrent-inference-matrix/#openai-api","title":"OpenAI API","text":"<p>Concurrent Support: Yes (cloud, rate-limited)</p> <p>OpenAI's API handles concurrent requests natively but imposes rate limits based on subscription tier.</p> <p>Recommended Settings: - 2-4 concurrent requests for Tier 1/2 accounts - Monitor rate limit headers: <code>x-ratelimit-remaining-requests</code>, <code>x-ratelimit-remaining-tokens</code> - Watch for 429 (rate limit exceeded) responses</p> <p>Rate Limits (as of early 2025): - Free tier: 3 RPM, 40k TPM (GPT-4) - Tier 1: 500 RPM, 30k TPM (GPT-4) - Tier 2: 5k RPM, 450k TPM (GPT-4)</p> <p>Configuration: Implement exponential backoff on 429 errors (OpenAI client does this automatically).</p> <p>Performance: Reliable for Swarm mode within rate limits. Consider costs \u2014 high concurrency = faster token consumption.</p>"},{"location":"concurrent-inference-matrix/#anthropic-api","title":"Anthropic API","text":"<p>Concurrent Support: Yes (cloud, rate-limited)</p> <p>Anthropic Claude API handles concurrent requests with tier-based rate limits.</p> <p>Recommended Settings: - 2-4 concurrent requests for standard tiers - Monitor rate limit headers in responses - Watch for 429 (rate limit) and 529 (overloaded) responses</p> <p>Rate Limits (as of early 2025): - Varies by tier and model - Sonnet: typically 50 RPM (free tier), higher for paid - Opus: lower limits due to compute cost</p> <p>Configuration: Implement exponential backoff on 429/529 errors.</p> <p>Performance: Reliable for Swarm mode. Claude models excel at complex reasoning tasks but cost more than local inference.</p>"},{"location":"concurrent-inference-matrix/#llamacpp-server","title":"llama.cpp server","text":"<p>Concurrent Support: Limited (parallel slots)</p> <p>llama.cpp server mode supports multiple \"slots\" (parallel inference contexts) but performance degrades quickly beyond 1-2 concurrent requests.</p> <p>Recommended Settings: - 1-2 concurrent requests maximum - Set <code>--parallel</code> flag when starting server:</p> <pre><code>./server --model model.gguf --parallel 2 --ctx-size 4096\n</code></pre> <p>Configuration: Each slot allocates separate KV cache \u2014 VRAM usage scales linearly.</p> <p>Performance: Acceptable for 2 Minions on high-VRAM GPUs. Beyond that, consider vLLM or TabbyAPI for better batching efficiency.</p>"},{"location":"concurrent-inference-matrix/#tgi-text-generation-inference","title":"TGI (Text Generation Inference)","text":"<p>Concurrent Support: Yes (native, continuous batching)</p> <p>HuggingFace's TGI is optimized for production inference serving with continuous batching and efficient memory management.</p> <p>Recommended Settings: - 4-8 concurrent requests for production - Can scale higher on multi-GPU setups - Monitor GPU utilization and throughput</p> <p>Configuration: Set via Docker/CLI args:</p> <pre><code>docker run --gpus all \\\n  -e MAX_CONCURRENT_REQUESTS=8 \\\n  -e MAX_BATCH_TOTAL_TOKENS=16384 \\\n  ghcr.io/huggingface/text-generation-inference:latest \\\n  --model-id meta-llama/Llama-3.1-8B-Instruct\n</code></pre> <p>Performance: Excellent for Swarm mode. Production-grade serving with monitoring and observability built in.</p>"},{"location":"concurrent-inference-matrix/#choosing-concurrency","title":"Choosing Concurrency","text":""},{"location":"concurrent-inference-matrix/#guidelines","title":"Guidelines","text":"<p>Start Low (2): Always begin with default concurrency and increase incrementally while monitoring performance.</p> <p>Monitor VRAM: Concurrent inference increases memory usage: - Batching requires larger KV cache - Out-of-memory errors indicate over-allocation - Use <code>nvidia-smi</code> (NVIDIA) or <code>sudo powermetrics</code> (Apple) to monitor</p> <p>Watch Latency: Track p50/p95/p99 response times: - Latency should increase sub-linearly with concurrency - If p95 latency spikes &gt;2x, reduce concurrency - Use Atom's built-in LLM pool stats for monitoring</p> <p>Cloud APIs: Monitor rate limit headers: - <code>x-ratelimit-remaining-requests</code> - <code>x-ratelimit-remaining-tokens</code> - Adjust concurrency to stay below 80% of limits</p> <p>Local Models: VRAM is the primary constraint, not network bandwidth: - 8B model + batch_size 4 \u2248 16-20GB VRAM (varies by quant) - 13B model + batch_size 2 \u2248 20-24GB VRAM - 70B model \u2192 typically limited to sequential inference unless multi-GPU</p>"},{"location":"concurrent-inference-matrix/#concurrency-vs-throughput","title":"Concurrency vs Throughput","text":"<p>Higher concurrency \u2260 always better: - Throughput (tokens/second) may plateau or decrease - Latency (time to first token) increases with queue depth - Memory usage scales with concurrent batch size - Optimal concurrency balances throughput and latency for your workload</p>"},{"location":"concurrent-inference-matrix/#workload-patterns","title":"Workload Patterns","text":"<p>Swarm Mode (multiple Minions): - Minions make frequent short LLM calls (tool selection, reasoning) - Benefit from moderate concurrency (2-4) - Prefer low latency over max throughput</p> <p>Batch Processing (single Minion, many tasks): - Sequential task execution with LLM calls - Low concurrency (1-2) is fine - Focus on per-request latency</p> <p>Interactive Chat: - Single user, conversational - Concurrency = 1 is sufficient - Prioritize time-to-first-token</p>"},{"location":"concurrent-inference-matrix/#pool-behavior","title":"Pool Behavior","text":"<p>The LLM connection pool (<code>nebulus_swarm/overlord/llm_pool.py</code>) manages concurrent LLM access:</p>"},{"location":"concurrent-inference-matrix/#slot-acquisition","title":"Slot Acquisition","text":"<ol> <li>Minion worker requests an LLM slot from the pool</li> <li>If slots are available (&lt; <code>ATOM_LLM_CONCURRENCY</code>), acquire immediately</li> <li>If all slots in use, worker queues with 60-second timeout</li> <li>On timeout, worker fails the task with <code>LLMPoolTimeout</code> error</li> </ol>"},{"location":"concurrent-inference-matrix/#error-handling","title":"Error Handling","text":"<ul> <li>429 (rate limit): Pool records error; client implements backoff</li> <li>503 (service unavailable): Pool records error; worker retries task</li> <li>Timeout: Worker fails gracefully; Overlord reassigns task</li> </ul>"},{"location":"concurrent-inference-matrix/#pool-statistics","title":"Pool Statistics","text":"<p>Accessible via <code>LLMPool.stats()</code>:</p> <pre><code>{\n    \"concurrency_limit\": 2,\n    \"active_requests\": 1,\n    \"queued_requests\": 0,\n    \"total_requests\": 145,\n    \"total_errors\": 3,\n    \"error_429_count\": 2,\n    \"error_503_count\": 1,\n    \"average_wait_time_ms\": 12.3\n}\n</code></pre>"},{"location":"concurrent-inference-matrix/#integration-points","title":"Integration Points","text":"<ul> <li>Dashboard: Real-time pool stats visualization</li> <li>Monitoring: Prometheus-style metrics export</li> <li>Logs: Structured logging of pool events (acquire, release, timeout)</li> </ul>"},{"location":"concurrent-inference-matrix/#troubleshooting","title":"Troubleshooting","text":""},{"location":"concurrent-inference-matrix/#high-queue-times","title":"High Queue Times","text":"<p>Symptom: Workers waiting &gt;5s for LLM slots</p> <p>Diagnosis: - Check pool stats: <code>queued_requests</code> consistently &gt;0 - Backend may not support claimed concurrency - Requests may be slow (large context, slow inference)</p> <p>Solution: - Reduce <code>ATOM_LLM_CONCURRENCY</code> if backend doesn't truly parallelize - Increase concurrency if backend is idle (check GPU utilization) - Optimize prompts to reduce tokens/request</p>"},{"location":"concurrent-inference-matrix/#out-of-memory-oom","title":"Out of Memory (OOM)","text":"<p>Symptom: Backend crashes or returns 500 errors under concurrent load</p> <p>Diagnosis: - Concurrent batching exceeds VRAM capacity - Check <code>nvidia-smi</code> during concurrent requests</p> <p>Solution: - Reduce <code>ATOM_LLM_CONCURRENCY</code> - Reduce context window size (<code>max_tokens</code>) - Switch to smaller model or better quantization - For TabbyAPI: reduce <code>max_batch_size</code> in config</p>"},{"location":"concurrent-inference-matrix/#rate-limit-errors-cloud-apis","title":"Rate Limit Errors (Cloud APIs)","text":"<p>Symptom: Frequent 429 errors from OpenAI/Anthropic</p> <p>Diagnosis: - Exceeding RPM or TPM limits for tier - Check rate limit headers in responses</p> <p>Solution: - Reduce <code>ATOM_LLM_CONCURRENCY</code> to stay below limits - Upgrade API tier for higher limits - Implement smarter backoff (increase initial delay)</p>"},{"location":"concurrent-inference-matrix/#slow-throughput","title":"Slow Throughput","text":"<p>Symptom: High concurrency but low tokens/second</p> <p>Diagnosis: - Backend doesn't support true concurrent batching (e.g., Ollama) - Requests queuing instead of parallelizing - VRAM bottleneck causing thrashing</p> <p>Solution: - Verify backend supports batching (see Quick Reference) - Check backend logs for batching behavior - Reduce concurrency to find optimal point - Consider switching to vLLM or TGI for better batching</p>"},{"location":"concurrent-inference-matrix/#references","title":"References","text":"<ul> <li>ExLlamaV2: https://github.com/turboderp/exllamav2</li> <li>vLLM: https://github.com/vllm-project/vllm</li> <li>MLX: https://github.com/ml-explore/mlx</li> <li>Ollama: https://github.com/ollama/ollama</li> <li>llama.cpp: https://github.com/ggerganov/llama.cpp</li> <li>Text Generation Inference: https://github.com/huggingface/text-generation-inference</li> <li>OpenAI Rate Limits: https://platform.openai.com/docs/guides/rate-limits</li> <li>Anthropic Rate Limits: https://docs.anthropic.com/en/api/rate-limits</li> </ul>"},{"location":"feature_template/","title":"Feature: [Feature Name]","text":""},{"location":"feature_template/#1-overview","title":"1. Overview","text":"<p>Branch: <code>feat/[feature-name]</code></p> <p>Briefly describe the feature, the problem it solves, and why it is being built.</p>"},{"location":"feature_template/#2-requirements","title":"2. Requirements","text":"<p>List specific, testable requirements: - [ ] Requirement 1 - [ ] Requirement 2</p>"},{"location":"feature_template/#3-technical-implementation","title":"3. Technical Implementation","text":"<ul> <li>Modules: List modified/created files (e.g., <code>src/module.py</code>).</li> <li>Dependencies: List new packages (e.g., <code>rich</code>).</li> <li>Data: Database changes or new assets.</li> </ul>"},{"location":"feature_template/#4-verification-plan","title":"4. Verification Plan","text":"<p>Automated Tests: - [ ] Script/Test: <code>pytest tests/test_feature.py</code> - [ ] Logic Verified: [Describe what is tested]</p> <p>Manual Verification: - [ ] Step 1: Run <code>forge --flag</code> - [ ] Step 2: Verify output in <code>dist/</code></p>"},{"location":"feature_template/#5-workflow-checklist","title":"5. Workflow Checklist","text":"<p>Follow the AI Behavior strict workflow: - [ ] Branch: Created <code>feat/...</code> branch? - [ ] Work: Implemented changes? - [ ] Test: All tests pass (<code>pytest</code>)? - [ ] Doc: Updated <code>README.md</code> and <code>walkthrough.md</code>? - [ ] Data: <code>git add .</code>, <code>git commit</code>, <code>git push</code>?</p>"},{"location":"provisioning-config/","title":"Nebulus Atom Provisioning Configuration","text":"<p>Guide for configuring Atom when deployed on Nebulus appliances. Atom is a standalone product that works with any OpenAI-compatible backend \u2014 this document covers the specific configuration for Nebulus infrastructure.</p>"},{"location":"provisioning-config/#configuration-precedence","title":"Configuration Precedence","text":"<p>Settings are loaded in this order (highest precedence first):</p> <ol> <li>Environment variables (<code>ATOM_*</code> preferred, <code>NEBULUS_*</code> legacy)</li> <li>Project config (<code>.atom.yml</code> in working directory)</li> <li>User config (<code>~/.atom/config.yml</code>)</li> <li>Built-in defaults</li> </ol>"},{"location":"provisioning-config/#configuration-options","title":"Configuration Options","text":""},{"location":"provisioning-config/#llm-settings","title":"LLM Settings","text":"Setting Env Var YAML Path Default Description Base URL <code>ATOM_LLM_BASE_URL</code> <code>llm.base_url</code> <code>http://localhost:5000/v1</code> OpenAI-compatible API endpoint Model <code>ATOM_LLM_MODEL</code> <code>llm.model</code> <code>Meta-Llama-3.1-8B-Instruct-exl2-8_0</code> Model identifier API Key <code>ATOM_LLM_API_KEY</code> <code>llm.api_key</code> <code>not-needed</code> API key (local servers typically don't require) Timeout <code>ATOM_LLM_TIMEOUT</code> <code>llm.timeout</code> <code>300.0</code> Request timeout in seconds Streaming <code>ATOM_LLM_STREAMING</code> <code>llm.streaming</code> <code>true</code> Enable streaming responses"},{"location":"provisioning-config/#vector-store-settings","title":"Vector Store Settings","text":"Setting Env Var YAML Path Default Description Path <code>ATOM_VECTOR_STORE_PATH</code> <code>vector_store.path</code> <code>.nebulus_atom/db</code> ChromaDB storage path Collection <code>ATOM_VECTOR_STORE_COLLECTION</code> <code>vector_store.collection</code> <code>codebase</code> Default collection name Embedding Model <code>ATOM_VECTOR_STORE_EMBEDDING_MODEL</code> <code>vector_store.embedding_model</code> <code>all-MiniLM-L6-v2</code> Sentence transformer model"},{"location":"provisioning-config/#connection-pool-settings-swarm-mode","title":"Connection Pool Settings (Swarm Mode)","text":"Setting Env Var Default Description Concurrency <code>ATOM_LLM_CONCURRENCY</code> <code>2</code> Max concurrent LLM requests"},{"location":"provisioning-config/#mcp-integration-optional","title":"MCP Integration (Optional)","text":"Setting Env Var Default Description MCP URL <code>ATOM_MCP_URL</code> <code>None</code> (disabled) MCP server endpoint for additional tools MCP Timeout <code>ATOM_MCP_TIMEOUT</code> <code>30</code> MCP request timeout in seconds <p>When <code>ATOM_MCP_URL</code> is set, Atom connects to the MCP server and registers additional tools (LTM, document parsing, domain knowledge). When not set, Atom works standalone.</p>"},{"location":"provisioning-config/#mcp-integration-details","title":"MCP Integration Details","text":"<p>MCP (Model Context Protocol) integration is optional and only relevant when Atom is deployed on Nebulus appliances. It provides:</p> <ul> <li>Long-Term Memory (LTM) \u2014 Persistent memory across sessions</li> <li>Document Parsing \u2014 Extract text from PDFs, Office docs, etc.</li> <li>Domain Knowledge \u2014 Access to indexed organizational knowledge</li> </ul>"},{"location":"provisioning-config/#enabling-mcp","title":"Enabling MCP","text":"<p>Set the MCP server URL via environment variable:</p> <pre><code>export ATOM_MCP_URL=http://nebulus-core:8000/mcp\n</code></pre> <p>Or in your config file:</p> <pre><code># ~/.atom/config.yml (YAML config for MCP not yet supported - use env var)\n</code></pre>"},{"location":"provisioning-config/#how-mcp-works","title":"How MCP Works","text":"<ol> <li>On startup, Atom checks if <code>ATOM_MCP_URL</code> is set</li> <li>If set, it connects to the MCP server and discovers available tools</li> <li>Tools are registered with an <code>mcp_</code> prefix (e.g., <code>mcp_ltm_search</code>) to avoid conflicts</li> <li>If the MCP server is unavailable, Atom continues working with its built-in tools</li> </ol>"},{"location":"provisioning-config/#graceful-degradation","title":"Graceful Degradation","text":"<p>MCP integration is designed for graceful degradation:</p> Scenario Behavior <code>ATOM_MCP_URL</code> not set Atom works standalone (default) MCP server unavailable at startup Warning logged, Atom continues without MCP tools MCP server goes down mid-session Tool calls return None, Atom continues working MCP server returns errors Errors logged, fallback to built-in behavior"},{"location":"provisioning-config/#nebulus-appliance-configuration","title":"Nebulus Appliance Configuration","text":"<p>For Tier 1/2/3 appliances with nebulus-core:</p> <pre><code># Edge (Tier 1) \u2014 nebulus-core on same machine\nATOM_MCP_URL=http://localhost:8000/mcp\n\n# Prime (Tier 2) \u2014 nebulus-core in Docker\nATOM_MCP_URL=http://nebulus-core:8000/mcp\n\n# Tier 3 \u2014 nebulus-core on separate host\nATOM_MCP_URL=http://core.internal:8000/mcp\n</code></pre>"},{"location":"provisioning-config/#verifying-mcp-connection","title":"Verifying MCP Connection","text":"<pre><code># Check if MCP is configured\npython -c \"\nfrom nebulus_swarm.integrations.mcp_client import get_mcp_client\nclient = get_mcp_client()\nprint(f'MCP available: {client.available}')\nprint(f'MCP tools: {[t.name for t in client.tools]}')\n\"\n\n# Test MCP server directly\ncurl -X POST http://localhost:8000/mcp \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"jsonrpc\":\"2.0\",\"method\":\"tools/list\",\"id\":1}'\n</code></pre>"},{"location":"provisioning-config/#troubleshooting-mcp","title":"Troubleshooting MCP","text":"Issue Cause Solution MCP tools not appearing <code>ATOM_MCP_URL</code> not set Set environment variable Connection refused nebulus-core not running Start nebulus-core service Timeout errors Network issues or slow server Increase <code>ATOM_MCP_TIMEOUT</code> Empty tool list MCP server has no tools registered Check nebulus-core configuration"},{"location":"provisioning-config/#recommended-values-by-platform","title":"Recommended Values by Platform","text":""},{"location":"provisioning-config/#tier-1-mac-mini-edge","title":"Tier 1: Mac Mini (Edge)","text":"<p>Apple Silicon with MLX inference server.</p> <pre><code># ~/.atom/config.yml\nllm:\n  base_url: \"http://localhost:8080/v1\"\n  model: \"mlx-community/Meta-Llama-3.1-8B-Instruct-4bit\"\n  timeout: 300.0\n  streaming: true\n\nvector_store:\n  path: \"/var/lib/atom/vectors\"\n  collection: \"codebase\"\n  embedding_model: \"all-MiniLM-L6-v2\"\n</code></pre> <p>Notes: - MLX server typically runs on port 8080 - 4-bit quantization recommended for 16GB unified memory - Concurrency limit: 1-2 (MLX is sequential internally)</p>"},{"location":"provisioning-config/#tier-2-linux-sff-prime","title":"Tier 2: Linux SFF (Prime)","text":"<p>NVIDIA GPU with TabbyAPI/ExLlamaV2.</p> <pre><code># ~/.atom/config.yml\nllm:\n  base_url: \"http://localhost:5000/v1\"\n  model: \"Meta-Llama-3.1-8B-Instruct-exl2-8_0\"\n  timeout: 300.0\n  streaming: true\n\nvector_store:\n  path: \"/var/lib/atom/vectors\"\n  collection: \"codebase\"\n  embedding_model: \"all-MiniLM-L6-v2\"\n</code></pre> <p>Notes: - TabbyAPI default port is 5000 - ExL2 8.0bpw quantization balances quality and VRAM - Concurrency limit: 2-4 (ExLlamaV2 supports batching)</p>"},{"location":"provisioning-config/#tier-3-headless-server","title":"Tier 3: Headless Server","text":"<p>Remote or containerized deployment.</p> <pre><code># ~/.atom/config.yml\nllm:\n  base_url: \"http://inference-server:5000/v1\"\n  model: \"Meta-Llama-3.1-8B-Instruct-exl2-8_0\"\n  api_key: \"${ATOM_LLM_API_KEY}\"  # Set via environment\n  timeout: 600.0\n  streaming: true\n\nvector_store:\n  path: \"/data/atom/vectors\"\n  collection: \"codebase\"\n  embedding_model: \"all-MiniLM-L6-v2\"\n</code></pre> <p>Notes: - Use service hostnames in container networks - Longer timeout for network latency - API key may be required for authenticated endpoints</p>"},{"location":"provisioning-config/#docker-deployment","title":"Docker Deployment","text":"<p>For container deployments, use environment variables:</p> <pre><code># Core LLM settings\nATOM_LLM_BASE_URL=http://tabby:5000/v1\nATOM_LLM_MODEL=Meta-Llama-3.1-8B-Instruct-exl2-8_0\nATOM_LLM_API_KEY=not-needed\nATOM_LLM_TIMEOUT=300\nATOM_LLM_STREAMING=true\n\n# Vector store\nATOM_VECTOR_STORE_PATH=/data/vectors\nATOM_VECTOR_STORE_COLLECTION=codebase\nATOM_VECTOR_STORE_EMBEDDING_MODEL=all-MiniLM-L6-v2\n\n# Connection pool (Swarm mode)\nATOM_LLM_CONCURRENCY=2\n\n# MCP integration (optional)\nATOM_MCP_URL=http://nebulus-core:8000/mcp\n</code></pre>"},{"location":"provisioning-config/#docker-compose-example","title":"Docker Compose Example","text":"<pre><code>services:\n  atom:\n    image: nebulus/atom:latest\n    environment:\n      - ATOM_LLM_BASE_URL=http://tabby:5000/v1\n      - ATOM_LLM_MODEL=Meta-Llama-3.1-8B-Instruct-exl2-8_0\n      - ATOM_LLM_CONCURRENCY=2\n    volumes:\n      - atom-vectors:/data/vectors\n    depends_on:\n      - tabby\n\n  tabby:\n    image: tabbyml/tabby:latest\n    # ... TabbyAPI configuration\n</code></pre>"},{"location":"provisioning-config/#provisioning-integration","title":"Provisioning Integration","text":"<p>For automated deployment via Ansible or similar tools:</p> <ol> <li> <p>Template the config file: <pre><code># templates/atom-config.yml.j2\nllm:\n  base_url: \"{{ atom_llm_base_url }}\"\n  model: \"{{ atom_llm_model }}\"\n  timeout: {{ atom_llm_timeout }}\n</code></pre></p> </li> <li> <p>Copy to target: <pre><code># playbook.yml\n- name: Configure Atom\n  ansible.builtin.template:\n    src: atom-config.yml.j2\n    dest: /home/{{ user }}/.atom/config.yml\n    mode: '0644'\n</code></pre></p> </li> <li> <p>Or set environment variables in systemd: <pre><code># /etc/systemd/system/atom.service.d/override.conf\n[Service]\nEnvironment=\"ATOM_LLM_BASE_URL=http://localhost:5000/v1\"\nEnvironment=\"ATOM_LLM_MODEL=Meta-Llama-3.1-8B-Instruct-exl2-8_0\"\n</code></pre></p> </li> </ol>"},{"location":"provisioning-config/#validation","title":"Validation","text":"<p>After provisioning, verify configuration:</p> <pre><code># Check config loads correctly\npython -c \"from nebulus_atom.settings import get_settings; s = get_settings(); print(f'LLM: {s.llm.base_url}')\"\n\n# Test LLM connectivity\ncurl -s http://localhost:5000/v1/models | jq .\n\n# Run smoke test\nnebulus-atom start --help\n</code></pre>"},{"location":"provisioning-config/#troubleshooting","title":"Troubleshooting","text":"Issue Cause Solution Connection refused LLM server not running Start TabbyAPI/MLX server Timeout errors Model loading or slow inference Increase <code>ATOM_LLM_TIMEOUT</code> Out of memory Model too large Use smaller quantization Config not loading Wrong file path Check <code>~/.atom/config.yml</code> exists Env vars ignored Typo in variable name Use <code>ATOM_</code> prefix, check spelling"},{"location":"standalone-audit/","title":"Standalone CLI Audit \u2014 nebulus-core Dependency Check","text":"<p>Date: 2026-02-05 Auditor: Claude Opus 4.5 Result: PASS \u2014 nebulus-atom has zero dependency on nebulus-core</p>"},{"location":"standalone-audit/#findings","title":"Findings","text":""},{"location":"standalone-audit/#import-scan","title":"Import Scan","text":"<p>Searched all Python files in <code>nebulus_atom/</code>, <code>nebulus_swarm/</code>, and <code>tests/</code> for:</p> <pre><code>from nebulus_core import ...\nimport nebulus_core\n</code></pre> <p>Result: Zero matches. No imports from nebulus-core exist anywhere in the codebase.</p>"},{"location":"standalone-audit/#dependency-declaration-scan","title":"Dependency Declaration Scan","text":"<p>Searched <code>pyproject.toml</code>, <code>requirements.txt</code>, <code>requirements-dev.txt</code>, and <code>requirements-minion.txt</code> for references to <code>nebulus-core</code>, <code>nebulus_core</code>, or <code>nebulus.core</code>.</p> <p>Result: Zero matches. nebulus-core is not declared as a dependency.</p>"},{"location":"standalone-audit/#llm-connectivity-test","title":"LLM Connectivity Test","text":"<p>Verified the CLI can connect to any OpenAI-compatible endpoint:</p> <ul> <li>Endpoint tested: <code>http://localhost:5000/v1</code> (TabbyAPI with ExLlamaV2)</li> <li>Models available: <code>Qwen2.5-Coder-14B-Instruct-exl2-4_25</code>,   <code>Meta-Llama-3.1-8B-Instruct-exl2-8_0</code>, <code>TinyLlama-1.1B-Chat-v1.0-exl2-6_5</code></li> <li>Connection: Successful via <code>openai.OpenAI</code> SDK</li> <li>Configuration: Uses <code>NEBULUS_BASE_URL</code> and <code>NEBULUS_MODEL</code> env vars</li> </ul>"},{"location":"standalone-audit/#how-the-cli-talks-to-llms","title":"How the CLI Talks to LLMs","text":"<p>The CLI uses the <code>openai</code> Python SDK directly against any OpenAI-compatible HTTP endpoint. There is no nebulus-core LLM client wrapper involved.</p> Component LLM Client Where Agent controller <code>openai.AsyncOpenAI</code> <code>nebulus_atom/services/openai_service.py</code> Swarm LLM reviewer <code>openai.OpenAI</code> <code>nebulus_swarm/reviewer/llm_review.py</code> Swarm minion agent <code>openai.OpenAI</code> <code>nebulus_swarm/minion/agent/llm_client.py</code> Swarm overlord parser <code>openai.AsyncOpenAI</code> <code>nebulus_swarm/overlord/llm_parser.py</code> <p>All four call the OpenAI SDK directly with configurable <code>base_url</code>.</p>"},{"location":"standalone-audit/#conclusion","title":"Conclusion","text":"<p>nebulus-atom is a fully standalone project. It does not import, depend on, or require nebulus-core in any way. The LLM interface is the standard OpenAI Python SDK pointed at a configurable endpoint, making it compatible with TabbyAPI, Ollama, vLLM, or any OpenAI-compatible server.</p>"},{"location":"features/adaptive_preference_learning/","title":"Feature: Adaptive Preference Learning","text":""},{"location":"features/adaptive_preference_learning/#1-overview","title":"1. Overview","text":"<p>Branch: <code>feat/adaptive-preference-learning</code></p> <p>This feature enables the agent to learn from user interactions and feedback to adjust its behavior over time. It will store user preferences and common patterns, using them to refine future responses and tool usage.</p>"},{"location":"features/adaptive_preference_learning/#2-requirements","title":"2. Requirements","text":"<ul> <li>[x] Preference Storage: A persistent store (JSON or DB) for user preferences (e.g., preferred languages, brevity, tools).</li> <li>[x] Feedback Mechanism: A way for users to explicitly set preferences or provide feedback (e.g., \"I prefer Python\").</li> <li>[x] Context Injection: Automatically inject relevant preferences into the system prompt.</li> <li>[x] Learning: (Optional/Advanced) Infer preferences from repeated corrections (e.g., if user always asks for \"shorter\", set brevity=high).</li> </ul>"},{"location":"features/adaptive_preference_learning/#3-technical-implementation","title":"3. Technical Implementation","text":"<ul> <li>Modules:<ul> <li><code>nebulus_atom/services/preference_service.py</code>: New service to manage preferences.</li> <li><code>nebulus_atom/models/preference.py</code>: Data model for preferences.</li> <li><code>nebulus_atom/controllers/agent_controller.py</code>: Inject preferences into system prompt.</li> <li><code>nebulus_atom/services/tool_executor.py</code>: Add tools <code>set_preference</code>, <code>get_preference</code>.</li> </ul> </li> <li>Data:<ul> <li>Storage: <code>.nebulus_atom/preferences.json</code></li> </ul> </li> </ul>"},{"location":"features/adaptive_preference_learning/#4-verification-plan","title":"4. Verification Plan","text":"<p>Automated Tests: - [x] <code>tests/test_preference_learning.py</code>:     - Set a preference.     - Verify persistence.     - Verify injection string generation.</p> <p>Manual Verification: - [x] Run <code>python -m nebulus_atom.main start</code>. - [x] Command: \"Set preference coding_style to 'verbose'\". - [x] Verify subsequent prompts include this preference.</p>"},{"location":"features/adaptive_preference_learning/#5-workflow-checklist","title":"5. Workflow Checklist","text":"<ul> <li>[x] Branch: <code>feat/adaptive-preference-learning</code></li> <li>[x] Work: Implement PreferenceService and tools</li> <li>[x] Test: <code>pytest</code> passes</li> <li>[x] Doc: Updated docs</li> <li>[x] Merge: <code>develop</code></li> </ul>"},{"location":"features/advanced_cognition/","title":"Feature: Advanced Cognition (\"System 2\" Thinking)","text":""},{"location":"features/advanced_cognition/#1-overview","title":"1. Overview","text":"<p>Branch: <code>feat/advanced-cognition</code></p> <p>Implements deeper reasoning capabilities inspired by Daniel Kahneman's \"System 2\" thinking - slow, deliberate, analytical reasoning vs fast intuitive responses. This enables the agent to handle complex multi-step problems more reliably.</p>"},{"location":"features/advanced_cognition/#2-core-concepts","title":"2. Core Concepts","text":""},{"location":"features/advanced_cognition/#system-1-vs-system-2","title":"System 1 vs System 2","text":"<ul> <li>System 1 (Current): Fast, automatic tool execution based on pattern matching</li> <li>System 2 (New): Deliberate reasoning with explicit problem decomposition and self-verification</li> </ul>"},{"location":"features/advanced_cognition/#key-capabilities","title":"Key Capabilities","text":"<ol> <li>Reasoning Chains: Explicit step-by-step thinking before action</li> <li>Problem Decomposition: Breaking complex tasks into manageable sub-tasks</li> <li>Self-Critique: Evaluating outputs before finalizing</li> <li>Uncertainty Awareness: Knowing when to ask for clarification</li> <li>Verification Loops: Checking work before declaring completion</li> </ol>"},{"location":"features/advanced_cognition/#3-requirements","title":"3. Requirements","text":""},{"location":"features/advanced_cognition/#phase-1-reasoning-engine","title":"Phase 1: Reasoning Engine \u2705","text":"<ul> <li>[x] CognitionService: New service to manage reasoning processes</li> <li>[x] Task Complexity Analysis: Classify tasks as simple/moderate/complex</li> <li>[x] Reasoning Chain Generation: Generate explicit thinking steps</li> <li>[x] Thought Logging: Persist reasoning for observability</li> </ul>"},{"location":"features/advanced_cognition/#phase-2-self-critique","title":"Phase 2: Self-Critique \u2705","text":"<ul> <li>[x] Output Verification: Review tool outputs for correctness</li> <li>[x] Error Anticipation: Predict potential failure modes</li> <li>[x] Confidence Scoring: Track certainty levels (0-100%)</li> </ul>"},{"location":"features/advanced_cognition/#phase-3-adaptive-behavior","title":"Phase 3: Adaptive Behavior \u2705","text":"<ul> <li>[x] Complexity-Based Routing: Simple tasks \u2192 fast path, Complex \u2192 deep reasoning</li> <li>[x] Clarification Triggers: Auto-detect ambiguous requests</li> <li>[x] Learning from Mistakes: Track failure patterns, adjust confidence scoring</li> </ul>"},{"location":"features/advanced_cognition/#4-technical-implementation","title":"4. Technical Implementation","text":""},{"location":"features/advanced_cognition/#new-files","title":"New Files","text":"<ul> <li><code>nebulus_atom/services/cognition_service.py</code>: Core reasoning engine</li> <li><code>nebulus_atom/models/cognition.py</code>: Data models for thoughts/reasoning</li> <li><code>nebulus_atom/services/failure_memory_service.py</code>: Failure tracking, pattern recognition, confidence penalties</li> <li><code>nebulus_atom/models/failure_memory.py</code>: Data models for failure records, patterns, and context</li> <li><code>tests/test_cognition_service.py</code>: Cognition unit tests</li> <li><code>tests/test_failure_memory_service.py</code>: Failure memory unit tests (35 tests, 97% coverage)</li> </ul>"},{"location":"features/advanced_cognition/#modified-files","title":"Modified Files","text":"<ul> <li><code>nebulus_atom/controllers/turn_processor.py</code>: Integrate cognition and failure context before tool execution</li> <li><code>nebulus_atom/controllers/agent_controller.py</code>: Add cognition hooks</li> <li><code>nebulus_atom/services/tool_executor.py</code>: Record failures on error, append failure summary to recovery hints</li> </ul>"},{"location":"features/advanced_cognition/#data-models","title":"Data Models","text":"<pre><code>@dataclass\nclass ReasoningStep:\n    step_number: int\n    thought: str\n    conclusion: str\n    confidence: float  # 0.0 - 1.0\n\n@dataclass\nclass CognitionResult:\n    task_complexity: str  # \"simple\" | \"moderate\" | \"complex\"\n    reasoning_chain: List[ReasoningStep]\n    recommended_approach: str\n    clarification_needed: bool\n    clarification_questions: List[str]\n</code></pre>"},{"location":"features/advanced_cognition/#complexity-classification","title":"Complexity Classification","text":"Complexity Criteria Behavior Simple Single tool, clear intent Direct execution Moderate 2-3 tools, some ambiguity Brief reasoning Complex Multi-step, dependencies, unclear Full reasoning chain"},{"location":"features/advanced_cognition/#5-verification-plan","title":"5. Verification Plan","text":""},{"location":"features/advanced_cognition/#automated-tests","title":"Automated Tests","text":"<ul> <li>[x] <code>tests/test_cognition_service.py</code> (25 tests):</li> <li>Test complexity classification accuracy</li> <li>Test reasoning chain generation</li> <li>Test confidence scoring</li> <li>Test clarification detection</li> <li>[x] <code>tests/test_failure_memory_service.py</code> (35 tests):</li> <li>Test error classification (8 error types)</li> <li>Test argument sanitization</li> <li>Test confidence penalty math and caps</li> <li>Test SQLite record/query/resolve operations</li> <li>Test cognition integration (confidence reduction, risk injection, backward compat)</li> </ul>"},{"location":"features/advanced_cognition/#manual-verification","title":"Manual Verification","text":"<ul> <li>[ ] Simple task: \"List files\" \u2192 Direct execution, no deep reasoning</li> <li>[ ] Moderate task: \"Create a Python function\" \u2192 Brief analysis</li> <li>[ ] Complex task: \"Refactor the authentication system\" \u2192 Full reasoning chain with clarifications</li> </ul>"},{"location":"features/advanced_cognition/#6-example-flow","title":"6. Example Flow","text":"<p>User: \"Add user authentication to the app\"</p> <p>System 2 Response: <pre><code>\ud83e\udde0 Analyzing task complexity... [COMPLEX]\n\nReasoning Chain:\n1. This requires multiple components: login, logout, session management\n2. Need to understand current app architecture first\n3. Multiple implementation approaches possible (JWT, sessions, OAuth)\n4. Security implications require careful consideration\n\nClarification Needed:\n- What authentication method do you prefer? (JWT/Session/OAuth)\n- Should it integrate with existing user model?\n- What routes need protection?\n\nConfidence: 45% (needs clarification before proceeding)\n</code></pre></p>"},{"location":"features/advanced_cognition/#7-workflow-checklist","title":"7. Workflow Checklist","text":"<ul> <li>[x] Branch: Created <code>feat/advanced-cognition</code> branch</li> <li>[x] Phase 1: Implement CognitionService</li> <li>[x] Phase 2: Implement self-critique</li> <li>[x] Phase 3: Integrate with turn processor</li> <li>[x] Test: All tests pass (<code>pytest</code>) \u2014 474 tests passing</li> <li>[x] Doc: Update README and wiki</li> </ul>"},{"location":"features/agentic_tdd/","title":"Feature: Agentic TDD Loop (The \"Fixer\")","text":""},{"location":"features/agentic_tdd/#1-overview","title":"1. Overview","text":"<p>Branch: <code>feat/agentic-tdd</code></p> <p>A specialized autonomous mode that strictly follows Test-Driven Development (TDD). It moves autonomy from \"linear execution\" to \"self-correcting loops,\" significantly increasing reliability by writing tests first, confirming failure, and iterating on the implementation until the test passes.</p>"},{"location":"features/agentic_tdd/#2-requirements","title":"2. Requirements","text":"<ul> <li>[x] TDD Loop: Implement a specialized execution loop: Test -&gt; Fail -&gt; Implement -&gt; Verify -&gt; Refactor.</li> <li>[x] Test Generation: Agent creates a new test file in <code>tests/</code> based on the user requirement.</li> <li>[x] Verification: Agent runs the specific test using <code>pytest</code> and parses the output to determine success/failure.</li> <li>[x] Iteration: If the test fails after implementation, the agent reads the error log and retries the implementation (max retries configurable).</li> <li>[x] Tools: Use existing <code>write_file</code> and <code>run_shell_command</code> tools.</li> </ul>"},{"location":"features/agentic_tdd/#3-technical-implementation","title":"3. Technical Implementation","text":"<ul> <li>Modules:<ul> <li><code>nebulus_atom/controllers/agent_controller.py</code>: Add <code>run_tdd_loop</code> method or new <code>TDDController</code>.</li> <li><code>nebulus_atom/services/tool_executor.py</code>: Ensure test runner output is clean for the LLM.</li> </ul> </li> <li>Dependencies: None (<code>pytest</code> is standard).</li> <li>Data: None.</li> </ul>"},{"location":"features/agentic_tdd/#4-verification-plan","title":"4. Verification Plan","text":"<p>Automated Tests: - [x] <code>tests/test_tdd_loop.py</code>:     - Mock the LLM to generate a test, then a fail implementation, then a fix.     - Verify the loop iterates 2 times and exits on success.</p> <p>Manual Verification: - [x] Run <code>nebulus-atom start</code>. - [x] Command: \"Implement a function <code>add(a, b)</code> using TDD\". - [x] Verify agent creates <code>tests/test_add.py</code>, runs it (fails), creates <code>add.py</code>, runs test (passes).</p>"},{"location":"features/agentic_tdd/#5-workflow-checklist","title":"5. Workflow Checklist","text":"<ul> <li>[x] Branch: Created <code>feat/agentic-tdd</code> branch?</li> <li>[x] Work: Implemented changes?</li> <li>[x] Test: All tests pass (<code>pytest</code>)?</li> <li>[x] Doc: Updated <code>README.md</code>?</li> <li>[x] Data: <code>git add .</code>, <code>git commit</code>?</li> </ul>"},{"location":"features/autonomous_execution/","title":"Feature: Autonomous Execution Engine","text":""},{"location":"features/autonomous_execution/#1-overview","title":"1. Overview","text":"<p>Branch: <code>feat/autonomous-execution</code></p> <p>This feature enables the agent to autonomously execute tasks from the plan without requiring user intervention after each step. It transforms the system from a passive \"wait-for-command\" tool into an active agent that can churn through a list of tasks.</p>"},{"location":"features/autonomous_execution/#2-requirements","title":"2. Requirements","text":"<ul> <li>[x] Execution Loop: The agent must automatically pick the next <code>PENDING</code> task from the active plan.</li> <li>[x] Task Context: The agent must use the context of previous tasks (results/history) to inform the current task.</li> <li>[x] Stop Conditions: The loop must stop if:<ul> <li>All tasks are completed.</li> <li>A task fails (configurable to retry or stop).</li> <li>The user interrupts execution.</li> </ul> </li> <li>[x] Status Updates: The UI (CLI/TUI) must reflect task progress in real-time.</li> <li>[x] Integration: Must work with the existing <code>TaskService</code> and <code>AgentController</code>.</li> </ul>"},{"location":"features/autonomous_execution/#3-technical-implementation","title":"3. Technical Implementation","text":"<ul> <li>Modules:<ul> <li><code>nebulus_atom/controllers/agent_controller.py</code>: Implement the <code>execute_plan()</code> method to loop through tasks.</li> <li><code>nebulus_atom/services/tool_executor.py</code>: Ensure tools return structured output usable by subsequent tasks.</li> </ul> </li> <li>Logic:<ul> <li>Add a <code>auto_mode</code> flag to the controller.</li> <li>In <code>run_loop</code>, if <code>auto_mode</code> is active, skip <code>get_user_input</code> and instead generate the prompt from the current task description.</li> </ul> </li> </ul>"},{"location":"features/autonomous_execution/#4-verification-plan","title":"4. Verification Plan","text":"<p>Automated Tests: - [x] <code>tests/test_autonomous_execution.py</code>:     - Create a plan with 2 simple tasks.     - Enable auto-execution.     - Verify both tasks transition to <code>COMPLETED</code> without mock user input.</p> <p>Manual Verification: - [x] Run <code>python -m nebulus_atom.main start --tui</code>. - [x] Command: \"Create a plan to (1) echo 'hello' and (2) echo 'world'. Then execute it.\" - [x] Verify the agent runs both steps automatically.</p>"},{"location":"features/autonomous_execution/#5-workflow-checklist","title":"5. Workflow Checklist","text":"<ul> <li>[x] Branch: <code>feat/autonomous-execution</code></li> <li>[x] Work: Implemented execution loop in Controller</li> <li>[x] Test: <code>pytest</code> passes</li> <li>[x] Doc: Updated docs</li> <li>[x] Merge: <code>develop</code></li> </ul>"},{"location":"features/cli_packaging/","title":"Feature: CLI Packaging (Global Install)","text":""},{"location":"features/cli_packaging/#1-overview","title":"1. Overview","text":"<p>Branch: <code>feat/cli-packaging</code></p> <p>Package Nebulus Atom as a proper Python CLI tool that can be installed globally (e.g., <code>pip install .</code> or <code>pipx install .</code>). This allows the user to open a terminal in any project directory and run <code>nebulus-atom</code> (or alias <code>mn</code>) to start the agent in that context.</p>"},{"location":"features/cli_packaging/#2-requirements","title":"2. Requirements","text":"<ul> <li>[x] Make the project installable via <code>pyproject.toml</code> (or <code>setup.py</code>).</li> <li>[x] Define entry points for the CLI command <code>nebulus-atom</code> and <code>mn</code>.</li> <li>[x] Ensure the agent uses <code>os.getcwd()</code> as the project root, not the package installation directory.</li> <li>[x] Ensure <code>.env</code> is loaded from the current working directory (or user home), not the package dir.</li> <li>[x] Ensure <code>CONTEXT.md</code> is looked for in the current working directory.</li> </ul>"},{"location":"features/cli_packaging/#3-technical-implementation","title":"3. Technical Implementation","text":"<ul> <li>Modules:<ul> <li><code>pyproject.toml</code>: Define <code>project.scripts</code> section.</li> <li><code>nebulus_atom/main.py</code>: Ensure <code>typer</code> app is exposed correctly as an entry point function.</li> <li><code>nebulus_atom/config.py</code>: Update logic to look for <code>.env</code> in <code>os.getcwd()</code> first.</li> <li><code>nebulus_atom/services/file_service.py</code>: Verify path resolution uses CWD.</li> </ul> </li> <li>Dependencies: <code>setuptools</code>, <code>wheel</code> (standard build tools).</li> <li>Data: None.</li> </ul>"},{"location":"features/cli_packaging/#4-verification-plan","title":"4. Verification Plan","text":"<ul> <li>[x] Run <code>pip install -e .</code> in the project root.</li> <li>[x] Open a new terminal tab (or change dir).</li> <li>[x] Run <code>nebulus-atom --help</code>.</li> <li>[x] Verify it runs and shows the help message.</li> <li>[x] Go to a test folder with <code>CONTEXT.md</code>.</li> <li>[x] Run <code>mn start \"Analyze this folder\"</code>.</li> <li>[x] Verify it reads the local <code>CONTEXT.md</code>.</li> </ul>"},{"location":"features/cli_packaging/#5-workflow-checklist","title":"5. Workflow Checklist","text":"<ul> <li>[x] Create branch <code>feat/cli-packaging</code></li> <li>[x] Implementation</li> <li>[x] Verification</li> </ul>"},{"location":"features/codebase_cartographer/","title":"Feature: Codebase Cartographer (AST Analysis)","text":""},{"location":"features/codebase_cartographer/#1-overview","title":"1. Overview","text":"<p>Branch: <code>feat/codebase-cartographer</code></p> <p>Uses Python's <code>ast</code> module to generate a structural map of classes, functions, and imports. This complements the RAG system by providing precise, deterministic answers about the codebase structure (e.g., \"Where is X defined?\").</p>"},{"location":"features/codebase_cartographer/#2-requirements","title":"2. Requirements","text":"<ul> <li>[x] AST Parsing: Recursively scan all <code>.py</code> files in the project.</li> <li>[x] Symbol Extraction: Extract Class names, Function names, Docstrings, and Imports.</li> <li>[x] Map Generation: Generate a JSON or textual representation of the project structure.</li> <li>[x] Tool Integration: Add <code>map_codebase</code> tool to return this structure to the agent.</li> <li>[x] Search Integration: Allow searching the map for specific symbols (e.g., <code>find_symbol Task</code>).</li> </ul>"},{"location":"features/codebase_cartographer/#3-technical-implementation","title":"3. Technical Implementation","text":"<ul> <li>Modules:<ul> <li><code>nebulus_atom/services/ast_service.py</code>: New service for parsing.</li> <li><code>nebulus_atom/services/tool_executor.py</code>: Add <code>map_codebase</code> and <code>find_symbol</code> tools.</li> </ul> </li> <li>Dependencies: None (Standard <code>ast</code> library).</li> <li>Data: In-memory cache of the AST map.</li> </ul>"},{"location":"features/codebase_cartographer/#4-verification-plan","title":"4. Verification Plan","text":"<p>Automated Tests: - [x] <code>tests/test_ast_service.py</code>:     - Parse a dummy python file.     - Verify it extracts class and function names correctly.</p> <p>Manual Verification: - [x] Run <code>nebulus-atom start</code>. - [x] Command: \"Map the codebase\". - [x] Verify the agent sees the structure of <code>nebulus_atom/</code>.</p>"},{"location":"features/codebase_cartographer/#5-workflow-checklist","title":"5. Workflow Checklist","text":"<ul> <li>[x] Branch: Created <code>feat/codebase-cartographer</code> branch?</li> <li>[x] Work: Implemented changes?</li> <li>[x] Test: All tests pass (<code>pytest</code>)?</li> <li>[x] Doc: Updated <code>README.md</code>?</li> <li>[x] Data: <code>git add .</code>, <code>git commit</code>?</li> </ul>"},{"location":"features/context_aware_assistant/","title":"Feature: Context-Aware Command Assistant","text":""},{"location":"features/context_aware_assistant/#1-overview","title":"1. Overview","text":"<p>Branch: <code>feat/context-aware-commands</code></p> <p>This feature enhances the CLI with semantic search capabilities over the command history and potentially the codebase (via RAG). It allows the user or the agent to recall past commands, plans, or relevant code snippets using natural language queries.</p>"},{"location":"features/context_aware_assistant/#2-requirements","title":"2. Requirements","text":"<ul> <li>[x] History Indexing: Automatically index user commands and agent responses into ChromaDB.</li> <li>[x] Semantic Search: Provide a tool <code>search_history</code> to query this index.</li> <li>[x] Integration: Hook into <code>AgentController</code> to index turns automatically.</li> <li>[x] Command Suggestion: (Optional) Provide suggestions based on current context.</li> </ul>"},{"location":"features/context_aware_assistant/#3-technical-implementation","title":"3. Technical Implementation","text":"<ul> <li>Modules:<ul> <li><code>nebulus_atom/services/rag_service.py</code>: Add <code>index_history(turn)</code> and <code>search_history(query)</code> methods.</li> <li><code>nebulus_atom/controllers/agent_controller.py</code>: Call <code>index_history</code> in <code>process_turn</code>.</li> <li><code>nebulus_atom/services/tool_executor.py</code>: Add <code>search_history</code> tool.</li> </ul> </li> <li>Data:<ul> <li>ChromaDB collection: <code>command_history</code>.</li> </ul> </li> </ul>"},{"location":"features/context_aware_assistant/#4-verification-plan","title":"4. Verification Plan","text":"<p>Automated Tests: - [x] <code>tests/test_rag.py</code>:     - Index a mock turn.     - Search for a keyword.     - Assert the turn is returned.</p> <p>Manual Verification: - [x] Run <code>python -m nebulus_atom.main start</code>. - [x] Execute: \"Create a file named hello.txt\". - [x] Execute: \"Search history for 'file creation'\". - [x] Verify the previous command is returned.</p>"},{"location":"features/context_aware_assistant/#5-workflow-checklist","title":"5. Workflow Checklist","text":"<ul> <li>[x] Branch: <code>feat/context-aware-commands</code></li> <li>[x] Work: Implement RAG updates and Controller integration</li> <li>[x] Test: <code>pytest</code> passes</li> <li>[x] Doc: Updated docs</li> <li>[x] Merge: <code>develop</code></li> </ul>"},{"location":"features/context_manager/","title":"Feature: Context Manager (File Pinning)","text":""},{"location":"features/context_manager/#1-overview","title":"1. Overview","text":"<p>Branch: <code>feat/context-manager</code></p> <p>Implement a system to \"pin\" specific files or directories to the agent s active context. This ensures critical file contents are always visible to the LLM without requiring repetitive <code>read_file</code> calls, improving efficiency for complex refactoring tasks.</p>"},{"location":"features/context_manager/#2-requirements","title":"2. Requirements","text":"<p>List specific, testable requirements: - [x] User can pin a file using <code>pin_file &lt;path&gt;</code>. - [x] User can unpin a file using <code>unpin_file &lt;path&gt;</code>. - [x] User can list currently pinned files using <code>list_context</code>. - [x] Pinned file content is automatically injected into the System Prompt or context window. - [x] System checks for token limits and warns or truncates if pinned content is too large.</p>"},{"location":"features/context_manager/#3-technical-implementation","title":"3. Technical Implementation","text":"<ul> <li>Modules:<ul> <li><code>nebulus_atom/models/history.py</code> (Update to store pinned paths).</li> <li><code>nebulus_atom/controllers/agent_controller.py</code> (Update context injection logic).</li> <li><code>nebulus_atom/services/context_service.py</code> (New service for managing pins).</li> </ul> </li> <li>Dependencies: None.</li> <li>Data: In-memory state within <code>History</code> or <code>ContextService</code>.</li> </ul>"},{"location":"features/context_manager/#4-verification-plan","title":"4. Verification Plan","text":"<p>Automated Tests: - [x] Script/Test: <code>pytest tests/test_context_manager.py</code> - [x] Logic Verified: Verify that <code>pin_file</code> adds to state and <code>unpin_file</code> removes it. Verify context injection string construction.</p> <p>Manual Verification: - [x] Step 1: Run <code>nebulus-atom start</code> - [x] Step 2: Execute <code>pin_file README.md</code> - [x] Step 3: Ask \"What is in the pinned file?\" without using <code>read_file</code> tool. - [x] Step 4: Verify the agent answers correctly based on context.</p>"},{"location":"features/context_manager/#5-workflow-checklist","title":"5. Workflow Checklist","text":"<p>Follow the AI Behavior strict workflow: - [x] Branch: Created <code>feat/context-manager</code> branch? - [x] Work: Implemented changes? - [x] Test: All tests pass (<code>pytest</code>)? - [x] Doc: Updated <code>README.md</code> and <code>walkthrough.md</code>? - [x] Data: <code>git add .</code>, <code>git commit</code>, <code>git push</code>?</p>"},{"location":"features/embedded_documentation/","title":"Feature: Embedded Documentation Dashboard","text":""},{"location":"features/embedded_documentation/#1-overview","title":"1. Overview","text":"<p>Branch: <code>feat/embedded-docs</code></p> <p>This feature provides in-terminal access to the project s documentation. Users can list, read, and search documentation files directly from the CLI or via the agent, reducing the need to switch context to a browser or file explorer.</p>"},{"location":"features/embedded_documentation/#2-requirements","title":"2. Requirements","text":"<ul> <li>[x] Doc Listing: List available documentation files in <code>docs/</code> and its subdirectories.</li> <li>[x] Doc Viewing: Render Markdown files in the terminal using <code>rich</code>.</li> <li>[x] Doc Searching: Simple keyword search or RAG-based search (reusing <code>RagService</code> if applicable) for documentation.</li> <li>[x] CLI Command: <code>nebulus-atom docs</code> subcommand.</li> <li>[x] Agent Tool: <code>read_doc</code> and <code>list_docs</code> tools for the agent.</li> </ul>"},{"location":"features/embedded_documentation/#3-technical-implementation","title":"3. Technical Implementation","text":"<ul> <li>Modules:<ul> <li><code>nebulus_atom/services/doc_service.py</code>: Service to scan and read docs.</li> <li><code>nebulus_atom/main.py</code>: Add <code>docs</code> subcommand.</li> <li><code>nebulus_atom/services/tool_executor.py</code>: Add <code>read_doc</code>, <code>list_docs</code> tools.</li> </ul> </li> <li>Dependencies: <code>rich</code> (already installed).</li> </ul>"},{"location":"features/embedded_documentation/#4-verification-plan","title":"4. Verification Plan","text":"<p>Automated Tests: - [x] <code>tests/test_doc_service.py</code>:     - Verify file listing.     - Verify file reading.</p> <p>Manual Verification: - [x] Run <code>python -m nebulus_atom.main docs list</code>. - [x] Run <code>python -m nebulus_atom.main docs read features/task_management.md</code>.</p>"},{"location":"features/embedded_documentation/#5-workflow-checklist","title":"5. Workflow Checklist","text":"<ul> <li>[x] Branch: <code>feat/embedded-docs</code></li> <li>[x] Work: Implement DocService and CLI commands</li> <li>[x] Test: <code>pytest</code> passes</li> <li>[x] Doc: Updated docs</li> <li>[x] Merge: <code>develop</code></li> </ul>"},{"location":"features/gemini_features/","title":"Top 5 Features Based on Project Influences","text":""},{"location":"features/gemini_features/#1-task-management-system-gsd-inspired","title":"1. Task Management System (GSD-Inspired)","text":"<ul> <li>Persistent task storage with progress tracking</li> <li>CLI commands: <code>task add</code>, <code>task complete</code>, <code>task list</code></li> <li>Integrates with MVC architecture (Model: task persistence, View: Rich CLI interface)</li> </ul>"},{"location":"features/gemini_features/#2-autonomous-execution-engine-moltbot-inspired","title":"2. Autonomous Execution Engine (Moltbot-Inspired)","text":"<ul> <li>Self-executing tasks after initial setup</li> <li>Uses controller logic to handle task delegation</li> <li>Configurable rules for task priority and timing</li> </ul>"},{"location":"features/gemini_features/#3-context-aware-command-assistant-gemini-cli-inspired","title":"3. Context-Aware Command Assistant (Gemini CLI-Inspired)","text":"<ul> <li>Semantic search of command history using ChromaDB</li> <li>Real-time suggestions based on current terminal context</li> <li>Leverages <code>search_code</code> and <code>index_codebase</code> tools</li> </ul>"},{"location":"features/gemini_features/#4-adaptive-preference-learning-moltbotgemini-fusion","title":"4. Adaptive Preference Learning (Moltbot/Gemini Fusion)","text":"<ul> <li>Learns user patterns from command history</li> <li>Auto-adjusts default behavior based on usage</li> <li>Implements <code>all-MiniLM-L6-v2</code> sentence transformers</li> </ul>"},{"location":"features/gemini_features/#5-embedded-documentation-dashboard-gemini-cli-inspired","title":"5. Embedded Documentation Dashboard (Gemini CLI-Inspired)","text":"<ul> <li>In-terminal documentation viewer with contextual help</li> <li>Uses <code>feature_search</code> to display relevant examples</li> <li>Shows usage patterns for commands like <code>start --tui</code></li> </ul>"},{"location":"features/github_integration/","title":"Feature: GitHub Integration (via MCP)","text":""},{"location":"features/github_integration/#1-overview","title":"1. Overview","text":"<p>Branch: <code>feat/github-mcp</code></p> <p>Leverage the MCP Client to connect to the official GitHub MCP server. This allows the agent to read Issues, check PRs, and map tasks directly to GitHub tickets, bridging the gap between \"Local Lab\" and \"Real World Workflow\".</p>"},{"location":"features/github_integration/#2-requirements","title":"2. Requirements","text":"<ul> <li>[x] MCP Connection: Connect to the GitHub MCP server using <code>connect_mcp_server</code>.</li> <li>[x] Auth Handling: Handle GitHub Personal Access Token (PAT) via env vars or the MCP server's auth flow.</li> <li>[x] Tool Exposure: Expose GitHub tools (e.g., <code>github_create_issue</code>, <code>github_list_prs</code>) to the agent.</li> <li>[x] Task Linking: (Optional) Allow <code>Task</code> objects to link to GitHub Issue IDs.</li> </ul>"},{"location":"features/github_integration/#3-technical-implementation","title":"3. Technical Implementation","text":"<ul> <li>Modules:<ul> <li><code>mini_nebulus/services/mcp_service.py</code>: Ensure environment variables are passed correctly to the server.</li> <li><code>mini_nebulus/config.py</code>: Add <code>GITHUB_TOKEN</code>.</li> </ul> </li> <li>Dependencies: None (Uses existing MCP client).</li> <li>Data: None.</li> </ul>"},{"location":"features/github_integration/#4-verification-plan","title":"4. Verification Plan","text":"<p>Automated Tests: - [x] <code>tests/test_github_mcp.py</code>:     - Mock the MCP server connection.     - Verify tools are registered.</p> <p>Manual Verification: - [x] Set <code>GITHUB_TOKEN</code>. - [x] Run <code>mini-nebulus start</code>. - [x] Command: \"Connect to GitHub MCP\". - [x] Command: \"List my open PRs\". - [x] Verify real data from GitHub is returned.</p>"},{"location":"features/github_integration/#5-workflow-checklist","title":"5. Workflow Checklist","text":"<ul> <li>[x] Branch: Created <code>feat/github-mcp</code> branch?</li> <li>[x] Work: Implemented changes?</li> <li>[x] Test: All tests pass (<code>pytest</code>)?</li> <li>[x] Doc: Updated <code>README.md</code>?</li> <li>[x] Data: <code>git add .</code>, <code>git commit</code>?</li> </ul>"},{"location":"features/interactive_clarification/","title":"Feature: Interactive Clarification (Human-in-the-Loop)","text":""},{"location":"features/interactive_clarification/#1-overview","title":"1. Overview","text":"<p>Branch: <code>feat/interactive-clarification</code></p> <p>Allow the autonomous agent to pause execution and ask the user for guidance when it encounters ambiguity, rather than guessing or aborting. This \"Human-in-the-Loop\" capability ensures higher accuracy for complex tasks.</p>"},{"location":"features/interactive_clarification/#2-requirements","title":"2. Requirements","text":"<p>List specific, testable requirements: - [x] Agent can call <code>ask_user(question=\"...\")</code> as a tool. - [x] Execution loop pauses when <code>ask_user</code> is called. - [x] In CLI mode, the user is prompted to type an answer. - [x] In Discord mode, the bot sends a message and waits for a reply from the same user/channel. (Note: Discord view implementation is pending, but architectural support is added in BaseView) - [x] The user s answer is returned to the agent as the result of the tool call.</p>"},{"location":"features/interactive_clarification/#3-technical-implementation","title":"3. Technical Implementation","text":"<ul> <li>Modules:<ul> <li><code>nebulus_atom/services/tool_executor.py</code> (Add <code>ask_user</code> tool definition).</li> <li><code>nebulus_atom/controllers/agent_controller.py</code> (Handle the pause/resume logic).</li> <li><code>nebulus_atom/views/base_view.py</code> (Add <code>ask_user_input</code> method).</li> <li><code>nebulus_atom/gateways/discord_gateway.py</code> (Implement wait_for logic).</li> </ul> </li> <li>Dependencies: None.</li> <li>Data: Temporary state for the pending question.</li> </ul>"},{"location":"features/interactive_clarification/#4-verification-plan","title":"4. Verification Plan","text":"<p>Automated Tests: - [x] Script/Test: <code>pytest tests/test_interactive.py</code> (Temporary test passed) - [x] Logic Verified: Mock the view input and verify the agent receives it correctly.</p> <p>Manual Verification: - [x] Step 1: Run <code>nebulus-atom start</code> - [x] Step 2: Instruct agent: \"If you don t know my name, ask me.\" - [x] Step 3: Agent calls <code>ask_user(\"What is your name?\")</code>. - [x] Step 4: User types name. - [x] Step 5: Agent confirms \"Hello [Name]\".</p>"},{"location":"features/interactive_clarification/#5-workflow-checklist","title":"5. Workflow Checklist","text":"<p>Follow the AI Behavior strict workflow: - [x] Branch: Created <code>feat/interactive-clarification</code> branch? - [x] Work: Implemented changes? - [x] Test: All tests pass (<code>pytest</code>)? - [x] Doc: Updated <code>README.md</code> and <code>walkthrough.md</code>? - [x] Data: <code>git add .</code>, <code>git commit</code>, <code>git push</code>?</p>"},{"location":"features/interactive_dashboard/","title":"Feature: Interactive Dashboard (TUI)","text":""},{"location":"features/interactive_dashboard/#1-overview","title":"1. Overview","text":"<p>Branch: <code>feat/interactive-dashboard</code></p> <p>Upgrade the User Interface from a scrolling CLI to a full Terminal User Interface (TUI). This provides a persistent dashboard with dedicated panels for Chat, Plan/Graph, and Context, mimicking a \"Mission Control\" center.</p>"},{"location":"features/interactive_dashboard/#2-requirements","title":"2. Requirements","text":"<ul> <li>[x] Replace <code>CLIView</code> (Rich) with a <code>Textual</code> App.</li> <li>[x] Layout:<ul> <li>Sidebar (Left): Active Plan (Tree), Pinned Files list.</li> <li>Main (Center): Chat History (Scrollable).</li> <li>Input (Bottom): Multi-line prompt area.</li> </ul> </li> <li>[x] Real-time updates: When task status changes, the Sidebar updates immediately without re-printing the whole screen.</li> <li>[x] Keyboard shortcuts (e.g., Ctrl+C to stop, Ctrl+L to clear).</li> </ul>"},{"location":"features/interactive_dashboard/#3-technical-implementation","title":"3. Technical Implementation","text":"<ul> <li>Modules:<ul> <li><code>nebulus_atom/views/tui_view.py</code> (New view implementation using Textual).</li> <li><code>nebulus_atom/main.py</code> (Switch to TUI mode).</li> </ul> </li> <li>Dependencies: <code>textual</code>.</li> <li>Data: Async event loop integration with Textual.</li> </ul>"},{"location":"features/interactive_dashboard/#4-verification-plan","title":"4. Verification Plan","text":"<ul> <li>[x] Run <code>nebulus-atom start --tui</code>.</li> <li>[x] Verify layout renders correctly.</li> <li>[x] Run a complex plan.</li> <li>[x] Verify the Plan Panel updates task icons in real-time.</li> <li>[x] Verify chat interaction works smoothly.</li> </ul>"},{"location":"features/interactive_dashboard/#5-workflow-checklist","title":"5. Workflow Checklist","text":"<ul> <li>[x] Create branch <code>feat/interactive-dashboard</code></li> <li>[x] Implementation</li> <li>[x] Verification</li> </ul>"},{"location":"features/logging/","title":"Feature: System Logging","text":""},{"location":"features/logging/#1-overview","title":"1. Overview","text":"<p>Branch: <code>feat/logging</code></p> <p>Nebulus Atom currently lacks a centralized logging system. Errors and execution details are printed to stdout/stderr or the TUI logs, but there is no persistent record for debugging or auditing. This feature implements a robust, rotating file-based logging system.</p>"},{"location":"features/logging/#2-requirements","title":"2. Requirements","text":"<ul> <li>[x] Create a centralized logging configuration.</li> <li>[x] Logs should be saved to <code>logs/nebulus_atom.log</code>.</li> <li>[x] Implement log rotation (e.g., 5MB max size, keep 3 backups).</li> <li>[x] Ensure logs capture timestamp, level, logger name, and message.</li> <li>[x] Integrate logging into key components:<ul> <li><code>AgentController</code> (Task lifecycle events)</li> <li><code>ToolExecutor</code> (Tool execution inputs/outputs/errors)</li> <li><code>OpenAIService</code> (LLM request/response stats - careful with PII/Tokens)</li> <li><code>Config</code> (Startup/Environment issues)</li> </ul> </li> <li>[x] Ensure logging works seamlessly with TUI/CLI (i.e., doesn't pollute stdout in CLI mode unexpectedly).</li> </ul>"},{"location":"features/logging/#3-technical-implementation","title":"3. Technical Implementation","text":"<ul> <li>Modules:<ul> <li><code>nebulus_atom/utils/logger.py</code>: New module for logging setup.</li> <li><code>nebulus_atom/config.py</code>: Add logging constants (path, level).</li> <li><code>nebulus_atom/controllers/agent_controller.py</code>: Add log calls.</li> <li><code>nebulus_atom/services/tool_executor.py</code>: Add log calls.</li> <li><code>nebulus_atom/services/openai_service.py</code>: Add log calls.</li> </ul> </li> <li>Dependencies: Standard library <code>logging</code> and <code>logging.handlers</code>. No new external deps.</li> <li>Data: <code>logs/</code> directory creation.</li> </ul>"},{"location":"features/logging/#4-verification-plan","title":"4. Verification Plan","text":"<p>Automated Tests: - [x] Script/Test: <code>pytest tests/test_logging.py</code> - [x] Logic Verified: Check if log file is created, rotation works, and specific messages appear.</p> <p>Manual Verification: - [x] Step 1: Run <code>python -m nebulus_atom.main start</code> - [x] Step 2: Execute a command (e.g., \"list files\") - [x] Step 3: Check <code>logs/nebulus_atom.log</code> for correct entries.</p>"},{"location":"features/logging/#5-workflow-checklist","title":"5. Workflow Checklist","text":"<p>Follow the AI Behavior strict workflow: - [x] Branch: Created <code>feat/logging</code> branch? - [x] Work: Implemented changes? - [x] Test: All tests pass (<code>pytest</code>)? - [x] Doc: Updated <code>README.md</code> if necessary? - [x] Data: <code>git add .</code>, <code>git commit</code>, <code>git push</code>?</p>"},{"location":"features/mcp_client/","title":"Feature: MCP Client (Model Context Protocol)","text":""},{"location":"features/mcp_client/#1-overview","title":"1. Overview","text":"<p>Branch: <code>feat/mcp-client</code></p> <p>Implement a client for the Model Context Protocol (MCP). This allows Nebulus Atom to dynamically discover and use tools provided by external MCP Servers (e.g., PostgreSQL, GitHub, Slack) without hardcoding integration logic.</p>"},{"location":"features/mcp_client/#2-requirements","title":"2. Requirements","text":"<ul> <li>[x] Implement <code>MCPClient</code> service to connect to stdio/SSE MCP servers.</li> <li>[x] Add <code>connect_mcp_server &lt;command&gt;</code> tool to launch and connect to an MCP server.</li> <li>[x] Dynamically register tools discovered from MCP servers into <code>ToolExecutor</code>.</li> <li>[x] Convert Nebulus Atom internal tool calls to MCP JSON-RPC requests.</li> <li>[x] Handle MCP resources and prompts (optional, focus on tools first).</li> </ul>"},{"location":"features/mcp_client/#3-technical-implementation","title":"3. Technical Implementation","text":"<ul> <li>Modules:<ul> <li><code>nebulus_atom/services/mcp_service.py</code> (New service for protocol handling).</li> <li><code>nebulus_atom/services/tool_executor.py</code> (Integrate dynamic MCP tools).</li> </ul> </li> <li>Dependencies: <code>mcp</code> (python sdk) or implement basic JSON-RPC.</li> <li>Data: Config to store persistent server connections.</li> </ul>"},{"location":"features/mcp_client/#4-verification-plan","title":"4. Verification Plan","text":"<ul> <li>[x] Start a simple \"Echo\" MCP server.</li> <li>[x] Connect Nebulus Atom to it.</li> <li>[x] Ask agent to \"Use the echo tool\".</li> <li>[x] Verify agent calls the tool via MCP and receives response.</li> </ul>"},{"location":"features/mcp_client/#5-workflow-checklist","title":"5. Workflow Checklist","text":"<ul> <li>[x] Create branch <code>feat/mcp-client</code></li> <li>[x] Implementation</li> <li>[x] Verification</li> </ul>"},{"location":"features/multimodal_input/","title":"Feature: Multimodal Input (Image Understanding)","text":""},{"location":"features/multimodal_input/#1-overview","title":"1. Overview","text":"<p>Branch: <code>feat/multimodal-input</code></p> <p>Enable Nebulus Atom to process images alongside text. This allows users to share screenshots, diagrams, or mockups, which the agent can analyze using a vision-capable model (like Gemini Pro Vision or GPT-4o).</p>"},{"location":"features/multimodal_input/#2-requirements","title":"2. Requirements","text":"<ul> <li>[x] Add <code>scan_image &lt;path&gt;</code> tool to add an image to the context.</li> <li>[x] Update <code>OpenAIService</code> to support <code>image_url</code> or base64 image payloads in messages (via tool output).</li> <li>[ ] Handle CLI drag-and-drop file paths (detect if input is an image path).</li> <li>[ ] Support \"vision\" capability in the LLM configuration.</li> </ul>"},{"location":"features/multimodal_input/#3-technical-implementation","title":"3. Technical Implementation","text":"<ul> <li>Modules:<ul> <li><code>nebulus_atom/services/openai_service.py</code> (Modify payload construction).</li> <li><code>nebulus_atom/controllers/agent_controller.py</code> (Handle image inputs).</li> </ul> </li> <li>Dependencies: None (API support).</li> <li>Data: Temp storage for processed images if needed.</li> </ul>"},{"location":"features/multimodal_input/#4-verification-plan","title":"4. Verification Plan","text":"<ul> <li>[x] Download a test image (UI mockup).</li> <li>[x] Run <code>scan_image mockup.png</code>.</li> <li>[x] Ask \"Describe this UI\".</li> <li>[x] Verify accurate description from the agent.</li> </ul>"},{"location":"features/multimodal_input/#5-workflow-checklist","title":"5. Workflow Checklist","text":"<ul> <li>[x] Create branch <code>feat/multimodal-input</code></li> <li>[x] Implementation</li> <li>[x] Verification</li> </ul>"},{"location":"features/sandbox_execution/","title":"Feature: Sandboxed Execution (Docker)","text":""},{"location":"features/sandbox_execution/#1-overview","title":"1. Overview","text":"<p>Branch: <code>feat/sandbox-execution</code></p> <p>Secure the agent s execution environment by running potentially dangerous operations (shell commands, python scripts) inside an ephemeral Docker container. This prevents accidental system damage (e.g., <code>rm -rf /</code>).</p>"},{"location":"features/sandbox_execution/#2-requirements","title":"2. Requirements","text":"<ul> <li>[x] Check if Docker is available on startup.</li> <li>[x] Create a persistent <code>nebulus-atom-sandbox</code> container mounting the project dir.</li> <li>[x] Intercept <code>run_shell_command</code> to execute via <code>docker exec</code>.</li> <li>[x] Intercept <code>write_file</code>? (Optional, if mounted volume is used, host write is fine, but shell is the danger).</li> <li>[x] Provide a configuration option <code>SANDBOX_MODE=true/false</code>.</li> </ul>"},{"location":"features/sandbox_execution/#3-technical-implementation","title":"3. Technical Implementation","text":"<ul> <li>Modules:<ul> <li><code>nebulus_atom/services/docker_service.py</code> (Manage container lifecycle).</li> <li><code>nebulus_atom/services/tool_executor.py</code> (Route shell commands to DockerService).</li> </ul> </li> <li>Dependencies: <code>docker</code> (python sdk).</li> <li>Data: Dockerfile for the sandbox environment.</li> </ul>"},{"location":"features/sandbox_execution/#4-verification-plan","title":"4. Verification Plan","text":"<ul> <li>[x] Enable Sandbox Mode.</li> <li>[ ] Run <code>run_shell_command \"whoami\"</code>.</li> <li>[ ] Output should be <code>root</code> (inside docker) or <code>sandbox_user</code>, different from host user.</li> <li>[ ] Verify file changes inside docker reflect on host (volume mount).</li> </ul>"},{"location":"features/sandbox_execution/#5-workflow-checklist","title":"5. Workflow Checklist","text":"<ul> <li>[x] Create branch <code>feat/sandbox-execution</code></li> <li>[x] Implementation</li> <li>[x] Verification (Partially complete - Tests pass, runtime requires Docker)</li> </ul>"},{"location":"features/semantic_search/","title":"Feature: Semantic Code Search (RAG)","text":""},{"location":"features/semantic_search/#1-overview","title":"1. Overview","text":"<p>Branch: <code>feat/semantic-search</code></p> <p>Implement Retrieval-Augmented Generation (RAG) for the codebase. Instead of manually pinning files, the agent can semantically query the project to find relevant code snippets, improving autonomy on large codebases.</p>"},{"location":"features/semantic_search/#2-requirements","title":"2. Requirements","text":"<ul> <li>[x] Implement <code>index_codebase</code> tool to generate embeddings for all project files.</li> <li>[x] Implement <code>search_code &lt;query&gt;</code> tool to retrieve top-k relevant snippets.</li> <li>[x] Automatically update index on file writes (incremental indexing). (Partially implemented via manual index trigger, full auto-hook left for future optimization to avoid perf hit).</li> <li>[x] Use a lightweight local embedding model (e.g., <code>all-MiniLM-L6-v2</code>) and vector store (e.g., <code>chromadb</code> or simple memory/file store).</li> </ul>"},{"location":"features/semantic_search/#3-technical-implementation","title":"3. Technical Implementation","text":"<ul> <li>Modules:<ul> <li><code>nebulus_atom/services/rag_service.py</code> (Indexing and retrieval logic).</li> <li><code>nebulus_atom/services/tool_executor.py</code> (Add search tools).</li> </ul> </li> <li>Dependencies: <code>sentence-transformers</code>, <code>chromadb</code>.</li> <li>Data: <code>.nebulus_atom/db/</code> for vector storage.</li> </ul>"},{"location":"features/semantic_search/#4-verification-plan","title":"4. Verification Plan","text":"<ul> <li>[x] Run <code>index_codebase</code>.</li> <li>[x] Ask <code>search_code \"how are tools executed?\"</code>.</li> <li>[x] Verify <code>ToolExecutor.dispatch</code> code is returned.</li> </ul>"},{"location":"features/semantic_search/#5-workflow-checklist","title":"5. Workflow Checklist","text":"<ul> <li>[x] Create branch <code>feat/semantic-search</code></li> <li>[x] Implementation</li> <li>[x] Verification</li> </ul>"},{"location":"features/session_journal/","title":"Feature: Session Journal (Daily Standup)","text":""},{"location":"features/session_journal/#1-overview","title":"1. Overview","text":"<p>Branch: <code>feat/session-journal</code></p> <p>Generates a human-readable markdown summary of what was accomplished during a session (Tasks completed, Files changed, decisions made). It turns the \"black box\" of agent activity into a useful \"Standup Report\" for the user.</p>"},{"location":"features/session_journal/#2-requirements","title":"2. Requirements","text":"<ul> <li>[x] Activity Tracking: Record high-level events (Task Completion, File Edits, User Interactions).</li> <li>[x] Summary Generation: Generate a Markdown report at the end of a session (or on demand).</li> <li>[x] Persistence: Save journals to <code>.nebulus_atom/journals/YYYY-MM-DD.md</code>.</li> <li>[x] Tool Integration: Add <code>generate_journal</code> tool.</li> </ul>"},{"location":"features/session_journal/#3-technical-implementation","title":"3. Technical Implementation","text":"<ul> <li>Modules:<ul> <li><code>nebulus_atom/services/journal_service.py</code>: Service to aggregate <code>History</code> and <code>Task</code> data.</li> <li><code>nebulus_atom/models/history.py</code>: Potentially add metadata to turns for easier summarization.</li> <li><code>nebulus_atom/services/tool_executor.py</code>: Add <code>generate_journal</code>.</li> </ul> </li> <li>Dependencies: None.</li> <li>Data: Journal files.</li> </ul>"},{"location":"features/session_journal/#4-verification-plan","title":"4. Verification Plan","text":"<p>Automated Tests: - [x] <code>tests/test_journal_service.py</code>:     - Simulate a session with 1 completed task and 1 file edit.     - Call generation.     - Verify the markdown string contains the task description and file path.</p> <p>Manual Verification: - [x] Run <code>nebulus-atom start</code>. - [x] Perform some actions. - [x] Run <code>generate_journal</code>. - [x] Read the output file.</p>"},{"location":"features/session_journal/#5-workflow-checklist","title":"5. Workflow Checklist","text":"<ul> <li>[x] Branch: Created <code>feat/session-journal</code> branch?</li> <li>[x] Work: Implemented changes?</li> <li>[x] Test: All tests pass (<code>pytest</code>)?</li> <li>[x] Doc: Updated <code>README.md</code>?</li> <li>[x] Data: <code>git add .</code>, <code>git commit</code>?</li> </ul>"},{"location":"features/shell_macro_generator/","title":"Feature: Shell Macro Generator","text":""},{"location":"features/shell_macro_generator/#1-overview","title":"1. Overview","text":"<p>Branch: <code>feat/shell-macro</code></p> <p>Allow the agent to take a successful tool execution or plan and \"compile\" it into a standard Bash script or shell alias for the user to use without the agent next time. This reduces latency for repetitive tasks.</p>"},{"location":"features/shell_macro_generator/#2-requirements","title":"2. Requirements","text":"<ul> <li>[x] Macro Creation: Agent can generate a shell script from a sequence of <code>run_shell_command</code> calls.</li> <li>[x] Alias Suggestion: Agent can suggest an alias (e.g., <code>alias clean-docker=\"...\"</code>).</li> <li>[x] Persistence: Save macros to <code>~/.nebulus_atom/macros/</code> or append to <code>.bashrc</code> (with user permission).</li> <li>[x] Tool Integration: Add <code>create_macro</code> tool.</li> </ul>"},{"location":"features/shell_macro_generator/#3-technical-implementation","title":"3. Technical Implementation","text":"<ul> <li>Modules:<ul> <li><code>nebulus_atom/services/macro_service.py</code>: Logic to extract shell commands from history and format them.</li> <li><code>nebulus_atom/services/tool_executor.py</code>: Add <code>create_macro</code>.</li> </ul> </li> <li>Dependencies: None.</li> <li>Data: Macro files.</li> </ul>"},{"location":"features/shell_macro_generator/#4-verification-plan","title":"4. Verification Plan","text":"<p>Automated Tests: - [x] <code>tests/test_macro_service.py</code>:     - Pass a history of commands.     - Verify generated script content.</p> <p>Manual Verification: - [x] Run <code>nebulus-atom start</code>. - [x] Ask: \"Create a macro to clean all docker containers\". - [x] Verify <code>clean_docker.sh</code> is created and executable.</p>"},{"location":"features/shell_macro_generator/#5-workflow-checklist","title":"5. Workflow Checklist","text":"<ul> <li>[x] Branch: Created <code>feat/shell-macro</code> branch?</li> <li>[x] Work: Implemented changes?</li> <li>[x] Test: All tests pass (<code>pytest</code>)?</li> <li>[x] Doc: Updated <code>README.md</code>?</li> <li>[x] Data: <code>git add .</code>, <code>git commit</code>?</li> </ul>"},{"location":"features/skill_library/","title":"Feature: Skill Library (Persistent Skills)","text":""},{"location":"features/skill_library/#1-overview","title":"1. Overview","text":"<p>Branch: <code>feat/skill-library</code></p> <p>Expand the \"Skill System\" to allow generated skills to be persisted, categorized, and reused across different sessions or even different projects globally.</p>"},{"location":"features/skill_library/#2-requirements","title":"2. Requirements","text":"<p>List specific, testable requirements: - [x] Skills created via <code>create_skill</code> are stored persistently. - [x] Users can <code>publish_skill</code> to move a local project skill to a global user library (e.g., <code>~/.nebulus_atom/skills</code>). - [x] <code>list_skills</code> displays both project-specific and global skills. (Implemented implicitly by <code>load_skills</code> combining them). - [x] The agent can load and use global skills in any directory. - [x] Skills are namespaced to avoid conflicts (e.g., <code>global.calculator</code>).</p>"},{"location":"features/skill_library/#3-technical-implementation","title":"3. Technical Implementation","text":"<ul> <li>Modules:<ul> <li><code>nebulus_atom/services/skill_service.py</code> (Update <code>load_skills</code> to scan multiple paths).</li> <li><code>nebulus_atom/config.py</code> (Add <code>GLOBAL_SKILLS_PATH</code> configuration).</li> </ul> </li> <li>Dependencies: None.</li> <li>Data: Filesystem storage at <code>~/.nebulus_atom/skills/</code>.</li> </ul>"},{"location":"features/skill_library/#4-verification-plan","title":"4. Verification Plan","text":"<p>Automated Tests: - [x] Script/Test: <code>pytest tests/test_skill_library.py</code> - [x] Logic Verified:     - Create local skill -&gt; Publish -&gt; Verify file exists in global dir.     - Start new session in different dir -&gt; Verify global skill is loaded.</p> <p>Manual Verification: - [x] Step 1: Run <code>nebulus-atom start</code> - [x] Step 2: Create a skill \"hello_world\". - [x] Step 3: Run <code>publish_skill hello_world</code>. - [x] Step 4: Change directory and start agent. - [x] Step 5: Ask agent to use \"hello_world\".</p>"},{"location":"features/skill_library/#5-workflow-checklist","title":"5. Workflow Checklist","text":"<p>Follow the AI Behavior strict workflow: - [x] Branch: Created <code>feat/skill-library</code> branch? - [x] Work: Implemented changes? - [x] Test: All tests pass (<code>pytest</code>)? - [x] Doc: Updated <code>README.md</code> and <code>walkthrough.md</code>? - [x] Data: <code>git add .</code>, <code>git commit</code>, <code>git push</code>?</p>"},{"location":"features/smart_undo/","title":"Feature: Smart Undo (Transactional Filesystem)","text":""},{"location":"features/smart_undo/#1-overview","title":"1. Overview","text":"<p>Branch: <code>feat/smart-undo</code></p> <p>Provide a safety net for autonomous operations by creating restore points before risky changes. This allows the user or agent to rollback changes if a plan fails or corrupts the codebase, utilizing a transactional approach.</p>"},{"location":"features/smart_undo/#2-requirements","title":"2. Requirements","text":"<p>List specific, testable requirements: - [x] Agent automatically creates a checkpoint (backup) before overwriting any file via <code>write_file</code>. - [x] Agent automatically creates a checkpoint before running shell commands that modify the filesystem. (Decided to restrict auto-checkpoint to <code>write_file</code> for performance/precision, manual available for shell). - [x] User can manually trigger <code>create_checkpoint</code>. - [x] User can trigger <code>rollback_checkpoint</code> to restore the state to the last safe point. - [x] The system uses Git stash or a hidden backup directory without interfering with user s manual Git workflow. (Implemented using <code>.nebulus_atom/checkpoints</code> directory).</p>"},{"location":"features/smart_undo/#3-technical-implementation","title":"3. Technical Implementation","text":"<ul> <li>Modules:<ul> <li><code>nebulus_atom/services/checkpoint_service.py</code> (New service).</li> <li><code>nebulus_atom/services/tool_executor.py</code> (Integrate checkpoint hook before execution).</li> </ul> </li> <li>Dependencies: Git (system requirement).</li> <li>Data: <code>.git/</code> operations or <code>.nebulus_atom/backups/</code>.</li> </ul>"},{"location":"features/smart_undo/#4-verification-plan","title":"4. Verification Plan","text":"<p>Automated Tests: - [x] Script/Test: <code>pytest tests/test_smart_undo.py</code> - [x] Logic Verified:     - Create a file -&gt; Checkpoint -&gt; Modify file -&gt; Rollback -&gt; Verify original content.     - Verify multiple checkpoints work (stack).</p> <p>Manual Verification: - [x] Step 1: Run <code>nebulus-atom start</code> - [x] Step 2: Ask agent to \"Overwrite README.md with broken content\" - [x] Step 3: Verify it did so. - [x] Step 4: Run <code>rollback_checkpoint</code>. - [x] Step 5: Verify <code>README.md</code> is restored.</p>"},{"location":"features/smart_undo/#5-workflow-checklist","title":"5. Workflow Checklist","text":"<p>Follow the AI Behavior strict workflow: - [x] Branch: Created <code>feat/smart-undo</code> branch? - [x] Work: Implemented changes? - [x] Test: All tests pass (<code>pytest</code>)? - [x] Doc: Updated <code>README.md</code> and <code>walkthrough.md</code>? - [x] Data: <code>git add .</code>, <code>git commit</code>, <code>git push</code>?</p>"},{"location":"features/task_management/","title":"Feature: Persistent Task Management","text":""},{"location":"features/task_management/#1-overview","title":"1. Overview","text":"<p>Branch: <code>feat/persistent-tasks</code></p> <p>Currently, <code>TaskService</code> stores plans and tasks in memory. If the agent restarts, the plan is lost. This feature implements file-based persistence (JSON) so that the agent can resume long-running plans across sessions, maximizing the utility of the local LLM setup.</p>"},{"location":"features/task_management/#2-requirements","title":"2. Requirements","text":"<ul> <li>[x] Storage Location: Store plans in <code>.nebulus_atom/sessions/&lt;session_id&gt;/plan.json</code>.</li> <li>[x] Auto-Save: Automatically save the plan whenever a task is created, added, or updated.</li> <li>[x] Auto-Load: Automatically load the existing plan for the session upon <code>TaskService</code> initialization.</li> <li>[x] Data Model: Ensure <code>Plan</code> and <code>Task</code> objects can be serialized/deserialized cleanly.</li> <li>[x] Tools: No new tools needed, but existing tools (<code>create_plan</code>, <code>add_task</code>, <code>update_task</code>) must trigger persistence.</li> </ul>"},{"location":"features/task_management/#3-technical-implementation","title":"3. Technical Implementation","text":"<ul> <li>Modules:<ul> <li><code>nebulus_atom/services/task_service.py</code>: Add <code>load()</code> and <code>save()</code> methods; hook them into modification methods.</li> <li><code>nebulus_atom/models/task.py</code>: Ensure <code>to_dict()</code> and <code>from_dict()</code> (or standard serialization) are robust.</li> </ul> </li> <li>Data:<ul> <li>Directory structure: <code>.nebulus_atom/sessions/{session_id}/</code></li> </ul> </li> </ul>"},{"location":"features/task_management/#4-verification-plan","title":"4. Verification Plan","text":"<p>Automated Tests: - [x] <code>tests/test_task_persistence.py</code>:     - Create a plan.     - Assert file exists on disk.     - Re-instantiate <code>TaskService</code>.     - Assert plan is loaded correctly.</p> <p>Manual Verification: - [x] Run <code>python -m nebulus_atom.main start --tui</code>. - [x] Ask: \"Create a plan to count to 3\". - [x] Exit the app (<code>Ctrl+C</code>). - [x] Restart the app. - [x] Verify the Plan Tree still shows the plan.</p>"},{"location":"features/task_management/#5-workflow-checklist","title":"5. Workflow Checklist","text":"<ul> <li>[x] Branch: <code>feat/persistent-tasks</code></li> <li>[x] Work: Implemented changes</li> <li>[x] Test: <code>pytest</code> passes</li> <li>[x] Doc: Updated docs if needed</li> <li>[x] Merge: <code>develop</code></li> </ul>"},{"location":"features/visual_task_graph/","title":"Feature: Visual Task Graph","text":""},{"location":"features/visual_task_graph/#1-overview","title":"1. Overview","text":"<p>Branch: <code>feat/visual-task-graph</code></p> <p>Visualize the dependencies and execution flow of the agent's plan. This helps users understand the \"why\" and \"when\" of complex autonomous missions, seeing parallel vs sequential tasks.</p>"},{"location":"features/visual_task_graph/#2-requirements","title":"2. Requirements","text":"<p>List specific, testable requirements: - [x] <code>Task</code> model supports a <code>dependencies</code> list (Task IDs). - [x] <code>add_task</code> tool accepts a list of dependency IDs. - [x] CLI displays the plan as a nested Tree structure using <code>rich.tree</code>. - [x] Discord displays the plan as a Mermaid.js diagram (wrapped in code block) or ASCII tree. (Discord view support pending, implemented generic structure). - [x] Tasks are topologically sorted for execution (agent doesn't start a task until deps are done). (Implicit in tree visualization logic, execution logic respects this via agent planning).</p>"},{"location":"features/visual_task_graph/#3-technical-implementation","title":"3. Technical Implementation","text":"<ul> <li>Modules:<ul> <li><code>nebulus_atom/models/task.py</code> (Add dependencies field).</li> <li><code>nebulus_atom/views/cli_view.py</code> (Implement <code>print_plan_tree</code>).</li> <li><code>nebulus_atom/views/discord_view.py</code> (Implement Xprint_plan_mermaid`).</li> </ul> </li> <li>Dependencies: <code>rich</code> (already included).</li> <li>Data: Task graph structure in memory.</li> </ul>"},{"location":"features/visual_task_graph/#4-verification-plan","title":"4. Verification Plan","text":"<p>Automated Tests: - [x] Script/Test: <code>pytest tests/test_task_graph.py</code> - [x] Logic Verified:     - Create Task A and Task T (dep on A).     - Verify visualization output string contains correct hierarchy.</p> <p>Manual Verification: - [x] Step 1: Run <code>nebulus-atom start</code> - [x] Step 2: Create a plan with dependencies (e.g. \"Build\" depends on \"Compile\"). - [x] Step 3: Run <code>visualize_plan</code>. - [x] Step 4: Verify the tree structure in the terminal.</p>"},{"location":"features/visual_task_graph/#5-workflow-checklist","title":"5. Workflow Checklist","text":"<p>Follow the AI Behavior strict workflow: - [x] Branch: Created <code>feat/visual-task-graph</code> branch? - [x] Work: Implemented changes? - [x] Test: All tests pass (<code>pytest</code>)? - [x] Doc: Updated VREADME.md<code>and</code>walkthrough.md<code>? - [x] **Data**:</code>git add .<code>,</code>git commit<code>,</code>git push`?</p>"},{"location":"plans/2026-02-02-minion-agent-design/","title":"Phase 7: Minion Agent Design","text":""},{"location":"plans/2026-02-02-minion-agent-design/#overview","title":"Overview","text":"<p>The Minion agent is the autonomous coding brain that powers Nebulus Swarm Minions. It runs inside Docker containers, receives GitHub issues, and implements solutions through iterative LLM-guided tool execution.</p>"},{"location":"plans/2026-02-02-minion-agent-design/#key-decisions","title":"Key Decisions","text":"Decision Choice Rationale Autonomy level Full agentic workflow Matches Claude Code capability Skill storage Repo at <code>.nebulus/skills/</code> Version controlled, PR reviewable Tool implementation Reuse nebulus_atom ToolRegistry Proven, consistent behavior Completion signal Explicit <code>task_complete</code> tool Clear, agent-controlled Skill PR approval Human required Safety for privileged operation"},{"location":"plans/2026-02-02-minion-agent-design/#architecture","title":"Architecture","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502              MinionAgent                     \u2502\n\u2502  (orchestrates the agentic loop)            \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502              ToolExecutor                    \u2502\n\u2502  (executes tools, manages file state)       \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502              LLMClient                       \u2502\n\u2502  (OpenAI-compatible API to Nebulus)         \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"plans/2026-02-02-minion-agent-design/#agent-loop","title":"Agent Loop","text":"<pre><code>1. Build messages: system prompt + conversation history\n              \u2193\n2. Call LLM with tools schema\n              \u2193\n3. Parse response for tool calls\n              \u2193\n4. If task_complete \u2192 return success with summary\n   Else \u2192 execute tools, append results, check limits, loop\n</code></pre> <p>Safety nets: - Turn limit: 50 turns (configurable) - Timeout: 30 minutes (container-level) - Error threshold: 3 consecutive tool errors \u2192 stop</p>"},{"location":"plans/2026-02-02-minion-agent-design/#tool-schema","title":"Tool Schema","text":"<p>File Operations: - <code>read_file</code> - Read file contents with line range support - <code>write_file</code> - Create or overwrite a file - <code>edit_file</code> - Replace specific text in a file - <code>list_directory</code> - List files/folders in a path - <code>search_files</code> - Grep for pattern across files - <code>glob_files</code> - Find files matching a pattern</p> <p>Execution: - <code>run_command</code> - Execute shell command with timeout</p> <p>Completion: - <code>task_complete</code> - Signal work is done - <code>task_blocked</code> - Signal cannot proceed, explain why</p> <p>Skills: - <code>list_skills</code> - Show available skills - <code>use_skill</code> - Load skill instructions into context</p> <p>All file operations scoped to <code>/workspace</code>.</p>"},{"location":"plans/2026-02-02-minion-agent-design/#skill-system","title":"Skill System","text":"<p>Skills stored in <code>.nebulus/skills/</code> as YAML:</p> <pre><code>name: add-api-endpoint\ndescription: Add a new REST API endpoint\nversion: 1.0.0\ntags: [api, backend]\n\ntriggers:\n  keywords: [endpoint, api, route]\n  file_patterns: [\"**/routes.py\"]\n\ninstructions: |\n  When adding an API endpoint:\n  1. Check existing patterns\n  2. Create route handler\n  3. Add validation\n  4. Write tests\n</code></pre>"},{"location":"plans/2026-02-02-minion-agent-design/#skill-guardrails","title":"Skill Guardrails","text":"<p>When PR touches <code>.nebulus/skills/</code>: 1. Auto-apply <code>skill-change</code> label 2. Block auto-merge 3. Run schema validation 4. Run security scan for forbidden patterns 5. Require human approval</p> <p>Forbidden patterns in skill instructions: - Destructive commands: <code>rm -rf</code> - Remote code execution: <code>curl|sh</code> - Dynamic code execution functions - Dynamic import functions - System paths: <code>/etc/</code>, <code>~/.ssh</code> - Token references: <code>GITHUB_TOKEN</code></p>"},{"location":"plans/2026-02-02-minion-agent-design/#error-handling","title":"Error Handling","text":"Scenario Tool/Action Overlord Response Work complete <code>task_complete</code> Create PR, trigger review Cannot proceed <code>task_blocked</code> Comment on issue, remove label Tool errors (3x) Auto-detected Mark <code>needs-attention</code> Turn limit Auto-detected Mark <code>needs-attention</code> Timeout Watchdog Mark <code>needs-attention</code>"},{"location":"plans/2026-02-02-minion-agent-design/#file-structure","title":"File Structure","text":"<pre><code>nebulus_swarm/minion/\n\u251c\u2500\u2500 agent/\n\u2502   \u251c\u2500\u2500 minion_agent.py    # Core agent loop\n\u2502   \u251c\u2500\u2500 llm_client.py      # OpenAI SDK wrapper\n\u2502   \u251c\u2500\u2500 tool_executor.py   # Tool execution\n\u2502   \u251c\u2500\u2500 prompt_builder.py  # System prompt construction\n\u2502   \u2514\u2500\u2500 tools.py           # Tool definitions\n\u2514\u2500\u2500 skills/\n    \u251c\u2500\u2500 loader.py          # Skill loading\n    \u251c\u2500\u2500 validator.py       # Schema validation\n    \u2514\u2500\u2500 schema.py          # Pydantic models\n</code></pre>"},{"location":"plans/2026-02-02-minion-agent-design/#implementation-steps","title":"Implementation Steps","text":"<ol> <li>LLM Client &amp; Basic Agent Loop</li> <li>Tool Definitions &amp; Executor</li> <li>Prompt Builder &amp; Integration</li> <li>Skill System Foundation</li> <li>Skill Guardrails</li> <li>Tests &amp; Documentation</li> </ol>"},{"location":"plans/2026-02-02-minion-agent-design/#estimated-size","title":"Estimated Size","text":"<p>~1,300 new lines + ~100 lines modified</p>"},{"location":"plans/2026-02-02-nebulus-swarm-design/","title":"Nebulus Swarm: Overlord + Minions Architecture","text":"<p>Date: 2026-02-02 Status: APPROVED Authors: @jlwestsr, Claude Opus 4.5</p>"},{"location":"plans/2026-02-02-nebulus-swarm-design/#executive-summary","title":"Executive Summary","text":"<p>Nebulus Swarm extends Nebulus Atom into an autonomous, distributed agent system. An always-on Overlord monitors Slack and GitHub, spawning ephemeral Minion containers to implement features and fix bugs autonomously. Work is queued via GitHub Issues with labels, and humans interact through Slack.</p>"},{"location":"plans/2026-02-02-nebulus-swarm-design/#1-high-level-architecture","title":"1. High-Level Architecture","text":""},{"location":"plans/2026-02-02-nebulus-swarm-design/#components","title":"Components","text":"Component Role Runtime LLM Overlord Control plane - watches Slack, manages Minions, maintains state Always-on (lightweight container) Small/cheap (8B) or rule-based Minion Worker - pulls repo, implements features, creates PRs Ephemeral (container per job) Large (30B+ for quality) GitHub Work queue (Issues with labels) + code storage External service N/A Slack Human interface - chat with Overlord, receive updates External service N/A"},{"location":"plans/2026-02-02-nebulus-swarm-design/#system-diagram","title":"System Diagram","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                   NEBULUS OVERLORD                       \u2502\n\u2502            (lightweight, always-on, watches Slack)       \u2502\n\u2502                                                          \u2502\n\u2502  \u2022 Listens to Slack 24/7                                \u2502\n\u2502  \u2022 Spawns/stops Minions on demand                       \u2502\n\u2502  \u2022 Relays status updates from Minions                   \u2502\n\u2502  \u2022 Manages cron-triggered queue sweeps                  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                       \u2502 spawns/manages\n        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n        \u25bc              \u25bc              \u25bc\n   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n   \u2502 Minion  \u2502    \u2502 Minion  \u2502    \u2502 Minion  \u2502\n   \u2502    1    \u2502    \u2502    2    \u2502    \u2502    3    \u2502\n   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n      repo A         repo A         repo B\n</code></pre>"},{"location":"plans/2026-02-02-nebulus-swarm-design/#trigger-modes","title":"Trigger Modes","text":"<ol> <li>Cron - Overlord wakes Minions on schedule to check for <code>nebulus-ready</code> issues</li> <li>On-demand - User tells Overlord via Slack: \"Start working on issue #42 now\"</li> <li>Hybrid - Cron runs overnight, user can interrupt/prioritize via Slack during the day</li> </ol>"},{"location":"plans/2026-02-02-nebulus-swarm-design/#2-overlord-design","title":"2. Overlord Design","text":""},{"location":"plans/2026-02-02-nebulus-swarm-design/#responsibilities","title":"Responsibilities","text":"<ul> <li>Watch Slack channel 24/7 for messages</li> <li>Parse commands (\"start #42\", \"status\", \"pause\", \"list queue\")</li> <li>Maintain state of all active Minions and their current tasks</li> <li>Spawn/stop Minion containers via Docker API</li> <li>Query GitHub Issues API for <code>nebulus-ready</code> labeled items</li> <li>Relay Minion progress updates to Slack</li> <li>Handle cron-triggered sweeps</li> </ul>"},{"location":"plans/2026-02-02-nebulus-swarm-design/#tech-stack","title":"Tech Stack","text":"Concern Choice Why Runtime Python asyncio Matches existing Nebulus Atom codebase Slack Integration Slack Bolt SDK Official, handles websocket connection Container Management Docker SDK for Python Spawn/stop Minion containers State Storage SQLite Simple, file-based, survives restarts LLM (optional) Local 8B or regex/rules Only needs to understand commands, not code"},{"location":"plans/2026-02-02-nebulus-swarm-design/#state-schema","title":"State Schema","text":"<pre><code>CREATE TABLE minions (\n    id TEXT PRIMARY KEY,\n    container_id TEXT,\n    repo TEXT,\n    issue_number INTEGER,\n    status TEXT,  -- 'working', 'idle', 'error'\n    started_at TIMESTAMP,\n    last_heartbeat TIMESTAMP\n);\n\nCREATE TABLE work_history (\n    id INTEGER PRIMARY KEY AUTOINCREMENT,\n    minion_id TEXT,\n    repo TEXT,\n    issue_number INTEGER,\n    pr_number INTEGER,\n    status TEXT,  -- 'success', 'failed', 'timeout'\n    started_at TIMESTAMP,\n    completed_at TIMESTAMP,\n    error_message TEXT\n);\n</code></pre>"},{"location":"plans/2026-02-02-nebulus-swarm-design/#slack-commands","title":"Slack Commands","text":"<p>Natural language, parsed by Overlord:</p> Command Action \"What's the status?\" List active Minions and their tasks \"Work on issue #42\" Spawn Minion for that issue \"Stop the minion on #42\" Kill that container \"What's in the queue?\" Query GitHub for <code>nebulus-ready</code> issues \"Prioritize #45\" Bump priority (add <code>high-priority</code> label) \"Pause\" Stop processing queue (Minions finish current work) \"Resume\" Resume queue processing"},{"location":"plans/2026-02-02-nebulus-swarm-design/#3-minion-design","title":"3. Minion Design","text":""},{"location":"plans/2026-02-02-nebulus-swarm-design/#lifecycle","title":"Lifecycle","text":"<pre><code>Spawned \u2192 Clone Repo \u2192 Read Issue \u2192 Work \u2192 Commit \u2192 Push \u2192 Create PR \u2192 Report \u2192 Die\n</code></pre>"},{"location":"plans/2026-02-02-nebulus-swarm-design/#characteristics","title":"Characteristics","text":"<ul> <li>Fresh Docker container (sandboxed, isolated)</li> <li>Based on existing Nebulus Atom image</li> <li>Single mission: one GitHub issue</li> <li>Ephemeral - does its job and shuts down</li> </ul>"},{"location":"plans/2026-02-02-nebulus-swarm-design/#environment-variables","title":"Environment Variables","text":"<pre><code>MINION_ID: \"minion-abc123\"\nGITHUB_REPO: \"west_ai_labs/nebulus-atom\"\nGITHUB_ISSUE: \"42\"\nGITHUB_TOKEN: \"${secret}\"\nOVERLORD_CALLBACK_URL: \"http://overlord:8080/minion/report\"\nNEBULUS_BASE_URL: \"http://192.168.4.30:8080/v1\"\nNEBULUS_MODEL: \"qwen3-coder-30b\"\nNEBULUS_TIMEOUT: \"600\"\nNEBULUS_STREAMING: \"false\"\n</code></pre>"},{"location":"plans/2026-02-02-nebulus-swarm-design/#workflow-steps","title":"Workflow Steps","text":"<ol> <li>Clone - <code>git clone</code> the repo into <code>/workspace</code></li> <li>Branch - <code>git checkout -b minion/issue-42</code></li> <li>Read Issue - Fetch issue body + comments from GitHub API</li> <li>Analyze - Use cognition system to understand task complexity</li> <li>Work - Implement the feature/fix using existing Nebulus Atom tools</li> <li>Test - Run <code>pytest</code> if tests exist</li> <li>Commit - Commit changes with message referencing issue</li> <li>Push - Push branch to origin</li> <li>PR - Create pull request via GitHub API, link to issue</li> <li>Report - POST to Overlord: <code>{status: \"complete\", pr_url: \"...\"}</code></li> <li>Die - Container exits, gets cleaned up</li> </ol>"},{"location":"plans/2026-02-02-nebulus-swarm-design/#communication-with-overlord","title":"Communication with Overlord","text":"<ul> <li>Heartbeat - Every 60s: \"Still working on #42...\"</li> <li>Progress - When milestones hit: \"Tests passing, creating PR...\"</li> <li>Completion - Final report with PR link or error details</li> </ul>"},{"location":"plans/2026-02-02-nebulus-swarm-design/#4-data-flow-communication","title":"4. Data Flow &amp; Communication","text":""},{"location":"plans/2026-02-02-nebulus-swarm-design/#sequence-diagram","title":"Sequence Diagram","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2510       \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 You \u2502       \u2502 Slack \u2502      \u2502 Overlord \u2502      \u2502 Minion \u2502      \u2502 GitHub \u2502\n\u2514\u2500\u2500\u252c\u2500\u2500\u2518       \u2514\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2518      \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518      \u2514\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518      \u2514\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518\n   \u2502              \u2502               \u2502                \u2502               \u2502\n   \u2502 \"work on #42\"\u2502               \u2502                \u2502               \u2502\n   \u2502\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500&gt;\u2502               \u2502                \u2502               \u2502\n   \u2502              \u2502  message      \u2502                \u2502               \u2502\n   \u2502              \u2502\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500&gt;\u2502                \u2502               \u2502\n   \u2502              \u2502               \u2502  fetch issue   \u2502               \u2502\n   \u2502              \u2502               \u2502\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500&gt;\u2502\n   \u2502              \u2502               \u2502  issue details \u2502               \u2502\n   \u2502              \u2502               \u2502&lt;\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2502\n   \u2502              \u2502               \u2502                \u2502               \u2502\n   \u2502              \u2502               \u2502  spawn container               \u2502\n   \u2502              \u2502               \u2502\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500&gt;\u2502               \u2502\n   \u2502              \u2502  \"Starting!\"  \u2502                \u2502               \u2502\n   \u2502              \u2502&lt;\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2502                \u2502               \u2502\n   \u2502 notification \u2502               \u2502                \u2502  clone repo   \u2502\n   \u2502&lt;\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2502               \u2502                \u2502\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500&gt;\u2502\n   \u2502              \u2502               \u2502                \u2502               \u2502\n   \u2502              \u2502               \u2502  heartbeat     \u2502  [working...] \u2502\n   \u2502              \u2502               \u2502&lt;\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2502               \u2502\n   \u2502              \u2502  \"Progress...\u2502                \u2502               \u2502\n   \u2502              \u2502&lt;\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2502                \u2502               \u2502\n   \u2502              \u2502               \u2502                \u2502  push + PR    \u2502\n   \u2502              \u2502               \u2502                \u2502\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500&gt;\u2502\n   \u2502              \u2502               \u2502  complete!     \u2502               \u2502\n   \u2502              \u2502               \u2502&lt;\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2502               \u2502\n   \u2502              \u2502  \"PR ready!\"  \u2502                \u2502               \u2502\n   \u2502              \u2502&lt;\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2502                \u2502               \u2502\n   \u2502 notification \u2502               \u2502  cleanup       \u2502               \u2502\n   \u2502&lt;\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2502               \u2502\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500&gt;\u2502 (dies)        \u2502\n   \u2502              \u2502               \u2502                \u2502               \u2502\n</code></pre>"},{"location":"plans/2026-02-02-nebulus-swarm-design/#message-formats","title":"Message Formats","text":"<p>Minion \u2192 Overlord (HTTP POST): <pre><code>{\n    \"minion_id\": \"minion-abc123\",\n    \"event\": \"heartbeat | progress | complete | error\",\n    \"issue\": 42,\n    \"message\": \"Running tests...\",\n    \"data\": {\"pr_url\": \"...\", \"branch\": \"...\"}\n}\n</code></pre></p> <p>Overlord \u2192 Slack: <pre><code>\ud83e\udd16 Minion working on #42: Running tests...\n\u2705 Minion completed #42: PR ready \u2192 github.com/org/repo/pull/43\n\u274c Minion failed on #42: Tests failing - see logs\n</code></pre></p>"},{"location":"plans/2026-02-02-nebulus-swarm-design/#overlord-internal-api","title":"Overlord Internal API","text":"Endpoint Purpose <code>POST /minion/report</code> Receive Minion heartbeats/completion <code>GET /status</code> Health check + active Minion list <code>POST /spawn</code> Manually trigger a Minion (internal)"},{"location":"plans/2026-02-02-nebulus-swarm-design/#5-error-handling-recovery","title":"5. Error Handling &amp; Recovery","text":""},{"location":"plans/2026-02-02-nebulus-swarm-design/#failure-modes","title":"Failure Modes","text":"Failure Mode Detection Recovery Minion crashes mid-work No heartbeat for 5 min Overlord kills container, reports to Slack, marks issue as <code>needs-attention</code> LLM times out (cold start) Minion reports timeout Retry once with warm-up ping first, then fail gracefully Tests fail Minion detects pytest exit code Create PR anyway but mark as <code>draft</code>, report failure details Git push rejected Minion gets error Pull latest, rebase, retry once, then fail with details GitHub API rate limit 403 response Back off, notify Overlord, pause queue processing Overlord crashes Systemd/Docker restart policy Auto-restart, Minions continue (they're independent), reconnect to Slack"},{"location":"plans/2026-02-02-nebulus-swarm-design/#minion-self-protection","title":"Minion Self-Protection","text":"<pre><code># Max runtime to prevent runaway Minions\nMINION_TIMEOUT = 30 * 60  # 30 minutes max per issue\n\n# If task seems too big, bail early\nif cognition_result.complexity == \"COMPLEX\" and cognition_result.estimated_steps &gt; 20:\n    report_to_overlord(\"error\", \"Task too complex for autonomous work, needs human breakdown\")\n    sys.exit(1)\n</code></pre>"},{"location":"plans/2026-02-02-nebulus-swarm-design/#overlord-watchdog","title":"Overlord Watchdog","text":"<pre><code>async def watchdog_loop():\n    while True:\n        for minion in get_active_minions():\n            if time_since(minion.last_heartbeat) &gt; timedelta(minutes=5):\n                kill_container(minion.container_id)\n                notify_slack(f\"\u2620\ufe0f Minion on #{minion.issue} went silent, terminated\")\n                update_github_label(minion.issue, \"needs-attention\")\n        await asyncio.sleep(60)\n</code></pre>"},{"location":"plans/2026-02-02-nebulus-swarm-design/#graceful-degradation","title":"Graceful Degradation","text":"<ul> <li>If GitHub is down \u2192 Overlord reports \"GitHub unavailable\", pauses queue, retries hourly</li> <li>If LLM backend is down \u2192 Same pattern, with option to try fallback model</li> <li>If Slack is down \u2192 Overlord logs locally, queues messages, delivers when reconnected</li> </ul>"},{"location":"plans/2026-02-02-nebulus-swarm-design/#6-deployment-configuration","title":"6. Deployment &amp; Configuration","text":""},{"location":"plans/2026-02-02-nebulus-swarm-design/#container-architecture","title":"Container Architecture","text":"<pre><code>nebulus-server\n\u251c\u2500\u2500 overlord (always running)\n\u2502   \u251c\u2500\u2500 Port 8080 (internal API)\n\u2502   \u251c\u2500\u2500 Volume: /var/lib/overlord/state.db\n\u2502   \u2514\u2500\u2500 Network: nebulus-swarm\n\u2502\n\u2514\u2500\u2500 minion-pool (spawned on demand)\n    \u251c\u2500\u2500 minion-abc123 (working on #42)\n    \u251c\u2500\u2500 minion-def456 (working on #45)\n    \u2514\u2500\u2500 ... (up to MAX_CONCURRENT_MINIONS)\n</code></pre>"},{"location":"plans/2026-02-02-nebulus-swarm-design/#docker-compose","title":"Docker Compose","text":"<pre><code>version: \"3.8\"\nservices:\n  overlord:\n    image: nebulus-overlord:latest\n    container_name: overlord\n    restart: unless-stopped\n    environment:\n      - SLACK_BOT_TOKEN=${SLACK_BOT_TOKEN}\n      - SLACK_CHANNEL_ID=${SLACK_CHANNEL_ID}\n      - GITHUB_TOKEN=${GITHUB_TOKEN}\n      - NEBULUS_BASE_URL=http://192.168.4.30:8080/v1\n      - NEBULUS_MODEL=qwen3-coder-30b\n      - MAX_CONCURRENT_MINIONS=3\n      - CRON_SCHEDULE=0 2 * * *\n    volumes:\n      - overlord-state:/var/lib/overlord\n      - /var/run/docker.sock:/var/run/docker.sock\n    networks:\n      - nebulus-swarm\n\nvolumes:\n  overlord-state:\n\nnetworks:\n  nebulus-swarm:\n    driver: bridge\n</code></pre>"},{"location":"plans/2026-02-02-nebulus-swarm-design/#configuration-file","title":"Configuration File","text":"<pre><code># /etc/nebulus/swarm.yaml\noverlord:\n  slack:\n    bot_token: ${SLACK_BOT_TOKEN}\n    channel_id: \"C07XXXXXX\"\n\n  github:\n    token: ${GITHUB_TOKEN}\n    watched_repos:\n      - west_ai_labs/nebulus-atom\n      - west_ai_labs/other-project\n    work_label: \"nebulus-ready\"\n\n  minions:\n    max_concurrent: 3\n    timeout_minutes: 30\n    image: nebulus-minion:latest\n\n  cron:\n    enabled: true\n    schedule: \"0 2 * * *\"\n\n  llm:\n    base_url: http://192.168.4.30:8080/v1\n    model: qwen3-coder-30b\n    timeout: 600\n    streaming: false\n</code></pre>"},{"location":"plans/2026-02-02-nebulus-swarm-design/#7-testing-verification","title":"7. Testing &amp; Verification","text":""},{"location":"plans/2026-02-02-nebulus-swarm-design/#testing-layers","title":"Testing Layers","text":"Layer What We Test How Unit Overlord command parsing, state management pytest with mocks Unit Minion workflow steps (clone, branch, PR) pytest with mocks Integration Overlord \u2194 Minion communication Docker Compose test environment Integration GitHub API interactions VCR cassettes (recorded responses) End-to-End Full flow: Slack \u2192 Overlord \u2192 Minion \u2192 PR Test repo + test Slack channel"},{"location":"plans/2026-02-02-nebulus-swarm-design/#test-repository","title":"Test Repository","text":"<p>Create <code>west_ai_labs/nebulus-swarm-testbed</code> with: - Simple Python project - Pre-written \"easy win\" issues (<code>Add a multiply function</code>) - Used exclusively for E2E testing</p>"},{"location":"plans/2026-02-02-nebulus-swarm-design/#verification-checklist","title":"Verification Checklist","text":"<p>Overlord: - [ ] Connects to Slack and responds to \"ping\" - [ ] Parses natural language commands correctly - [ ] Spawns Minion container on demand - [ ] Receives heartbeats and updates state - [ ] Cleans up dead Minions after timeout - [ ] Survives restart (state persists in SQLite)</p> <p>Minion: - [ ] Clones repo successfully - [ ] Creates branch with correct naming - [ ] Reads issue and understands task - [ ] Makes appropriate code changes - [ ] Runs tests if present - [ ] Creates PR linked to issue - [ ] Reports completion to Overlord - [ ] Dies cleanly after work</p> <p>Full Flow: - [ ] Slack \"work on #1\" \u2192 PR created in &lt; 10 min - [ ] Cron trigger processes queue correctly - [ ] Multiple Minions can run in parallel - [ ] Failed Minion gets cleaned up, Slack notified</p>"},{"location":"plans/2026-02-02-nebulus-swarm-design/#smoke-test-script","title":"Smoke Test Script","text":"<pre><code>#!/bin/bash\n# scripts/smoke-test.sh\n# Quick validation after deployment\n\necho \"1. Testing Overlord ping...\"\n# Posts \"ping\" to Slack \u2192 expects \"pong\"\n\necho \"2. Creating test issue...\"\n# Creates test issue with nebulus-ready label\n\necho \"3. Triggering Minion...\"\n# Tells Overlord to work on it\n\necho \"4. Waiting for PR...\"\n# Waits for PR (timeout 15 min)\n\necho \"5. Verifying PR...\"\n# Verifies PR exists and links to issue\n</code></pre>"},{"location":"plans/2026-02-02-nebulus-swarm-design/#8-implementation-roadmap","title":"8. Implementation Roadmap","text":""},{"location":"plans/2026-02-02-nebulus-swarm-design/#phase-1-overlord-foundation-week-1","title":"Phase 1: Overlord Foundation (Week 1)","text":"<ul> <li>[ ] Create <code>nebulus_swarm/</code> package structure</li> <li>[ ] Slack Bolt integration - connect, listen, respond to \"ping\"</li> <li>[ ] Basic command parser (regex-based, upgrade to LLM later)</li> <li>[ ] SQLite state management</li> <li>[ ] Docker SDK integration - spawn/kill containers</li> <li>[ ] Health check endpoint</li> </ul>"},{"location":"plans/2026-02-02-nebulus-swarm-design/#phase-2-minion-mvp-week-2","title":"Phase 2: Minion MVP (Week 2)","text":"<ul> <li>[ ] Minion Dockerfile based on Nebulus Atom</li> <li>[ ] Entrypoint script: clone \u2192 branch \u2192 read issue</li> <li>[ ] Integrate existing Nebulus Atom agent for the \"work\" phase</li> <li>[ ] Heartbeat reporting to Overlord</li> <li>[ ] Git push + PR creation via GitHub API</li> <li>[ ] Completion reporting</li> </ul>"},{"location":"plans/2026-02-02-nebulus-swarm-design/#phase-3-wire-it-together-week-3","title":"Phase 3: Wire It Together (Week 3)","text":"<ul> <li>[ ] Overlord spawns Minion on Slack command</li> <li>[ ] Minion reports back, Overlord relays to Slack</li> <li>[ ] Watchdog for stuck Minions</li> <li>[ ] End-to-end test with testbed repo</li> <li>[ ] Error handling for common failures</li> </ul>"},{"location":"plans/2026-02-02-nebulus-swarm-design/#phase-4-cron-polish-week-4","title":"Phase 4: Cron &amp; Polish (Week 4)","text":"<ul> <li>[ ] Cron-triggered queue sweeps</li> <li>[ ] GitHub label management (<code>nebulus-ready</code> \u2192 <code>in-progress</code> \u2192 <code>in-review</code>)</li> <li>[ ] Multiple repo support</li> <li>[ ] Concurrent Minion limits</li> <li>[ ] LLM warm-up ping before heavy work</li> </ul>"},{"location":"plans/2026-02-02-nebulus-swarm-design/#phase-5-production-hardening-week-5","title":"Phase 5: Production Hardening (Week 5)","text":"<ul> <li>[ ] Logging and observability (integrate with existing telemetry)</li> <li>[ ] Graceful shutdown handling</li> <li>[ ] Rate limiting for GitHub API</li> <li>[ ] Documentation and runbooks</li> <li>[ ] Deployment scripts for Nebulus server</li> </ul>"},{"location":"plans/2026-02-02-nebulus-swarm-design/#phase-6-automated-pr-review-future","title":"Phase 6: Automated PR Review (Future)","text":"<p>Note: Initially, all Minion PRs will be reviewed manually by the human operator. This allows us to catch local LLM quirks, build a \"failure corpus\" for prompt tuning, and understand when 8B vs 30B models are appropriate. Phase 6 should only be implemented after sufficient manual review experience.</p> <ul> <li>[ ] AI Reviewer agent examines Minion PRs before notifying human</li> <li>[ ] Automated checks: tests pass, code style, security scan, complexity analysis</li> <li>[ ] Reviewer adds inline comments on concerning code</li> <li>[ ] Reviewer posts summary: approval or request-changes with reasoning</li> <li>[ ] Human receives pre-screened PRs with AI review notes</li> <li>[ ] Configurable auto-merge for low-risk, reviewer-approved PRs</li> </ul> <p>Review Flow (Phase 6): <pre><code>Minion creates PR \u2192 AI Reviewer examines \u2192 Posts review notes \u2192 Human final approval\n</code></pre></p>"},{"location":"plans/2026-02-02-nebulus-swarm-design/#stretch-goals-future","title":"Stretch Goals (Future)","text":"<ul> <li>[ ] Overlord uses LLM for natural language understanding</li> <li>[ ] Minion can ask clarifying questions via Slack before starting</li> <li>[ ] Web dashboard showing Minion status and history</li> <li>[ ] Multi-LLM support (route simple tasks to 8B, complex to 30B)</li> </ul>"},{"location":"plans/2026-02-02-nebulus-swarm-design/#9-references","title":"9. References","text":"<ul> <li>OpenClaw - Proactive AI agent with cron jobs and persistent memory</li> <li>SWE-agent - GitHub issue solving agent from Princeton/Stanford</li> <li>AWS Remote SWE Agents - GitHub Actions triggered agent system</li> <li>Slack Bolt SDK - Official Python framework for Slack apps</li> <li>Docker SDK for Python - Programmatic container management</li> </ul> <p>Document History: | Date | Author | Change | |------|--------|--------| | 2026-02-02 | @jlwestsr, Claude | Initial design | | 2026-02-02 | @jlwestsr, Claude | Added Phase 6: Automated PR Review |</p>"},{"location":"plans/2026-02-03-minion-clarifying-questions-design/","title":"Minion Clarifying Questions via Slack Design","text":"<p>Date: 2026-02-03 Status: APPROVED Authors: @jlwestsr, Claude Opus 4.5</p>"},{"location":"plans/2026-02-03-minion-clarifying-questions-design/#goal","title":"Goal","text":"<p>Allow Minions to ask clarifying questions via Slack when issue requirements are unclear, pause for human input, and resume work with the answer. Questions are an optimization, never a blocker - every failure path ends with the Minion continuing using its best judgment.</p>"},{"location":"plans/2026-02-03-minion-clarifying-questions-design/#key-decisions","title":"Key Decisions","text":"Decision Choice Rationale When to ask Before or during work, max 3 questions Covers both vague issues and mid-work decisions Wait mechanism Keep alive, poll, 10 min timeout Simple, no idle waste due to timeout Answer routing QUESTION event + GET endpoint Extends existing Reporter pattern Slack UX Threaded conversations Natural, supports multiple Minions Agent mechanism Reuse task_blocked, change lifecycle Minimal new components"},{"location":"plans/2026-02-03-minion-clarifying-questions-design/#architecture","title":"Architecture","text":"<pre><code>BEFORE:\nAgent calls task_blocked(question) \u2192 Minion dies \u2192 Human sees GitHub comment\n\nAFTER:\nAgent calls task_blocked(question) \u2192 Minion pauses\n  \u2192 Reporter sends QUESTION event to Overlord\n  \u2192 Overlord posts to Slack thread\n  \u2192 Human replies in thread\n  \u2192 Overlord stores answer\n  \u2192 Minion polls, gets answer\n  \u2192 Answer injected into agent conversation\n  \u2192 Agent resumes work\n  \u2192 (after 3 questions OR 10 min timeout \u2192 auto-continue)\n</code></pre>"},{"location":"plans/2026-02-03-minion-clarifying-questions-design/#reporter-changes","title":"Reporter Changes","text":"<p>New event type and two new methods:</p> <pre><code>class EventType(Enum):\n    HEARTBEAT = \"heartbeat\"\n    PROGRESS = \"progress\"\n    COMPLETE = \"complete\"\n    ERROR = \"error\"\n    QUESTION = \"question\"  # NEW\n\nasync def question(\n    self, question_text: str, blocker_type: str, question_id: str\n) -&gt; bool:\n    \"\"\"Send a question to the Overlord for human input.\"\"\"\n    payload = ReportPayload(\n        minion_id=self.minion_id,\n        event=EventType.QUESTION,\n        issue=self.issue_number,\n        message=question_text,\n        data={\n            \"blocker_type\": blocker_type,\n            \"question_id\": question_id,\n        },\n    )\n    return await self._send_report(payload)\n\nasync def poll_answer(\n    self, question_id: str, timeout: int = 600, interval: int = 15\n) -&gt; Optional[str]:\n    \"\"\"Poll the Overlord for an answer to a pending question.\"\"\"\n    elapsed = 0\n    while elapsed &lt; timeout:\n        session = await self._get_session()\n        url = f\"{self.callback_url.rsplit('/', 1)[0]}/answer/{self.minion_id}\"\n        async with session.get(url, params={\"question_id\": question_id}) as resp:\n            if resp.status == 200:\n                data = await resp.json()\n                if data.get(\"answered\"):\n                    return data[\"answer\"]\n        await asyncio.sleep(interval)\n        elapsed += interval\n    return None  # Timed out\n</code></pre>"},{"location":"plans/2026-02-03-minion-clarifying-questions-design/#overlord-changes","title":"Overlord Changes","text":""},{"location":"plans/2026-02-03-minion-clarifying-questions-design/#pending-questions-store","title":"Pending Questions Store","text":"<pre><code>@dataclass\nclass PendingQuestion:\n    minion_id: str\n    question_id: str\n    issue_number: int\n    repo: str\n    question_text: str\n    thread_ts: str  # Slack thread timestamp for matching replies\n    asked_at: datetime\n    answer: Optional[str] = None\n    answered: bool = False\n</code></pre>"},{"location":"plans/2026-02-03-minion-clarifying-questions-design/#question-event-handler","title":"QUESTION Event Handler","text":"<p>In <code>/minion/report</code>:</p> <pre><code>elif event == \"question\":\n    question_id = report_data.get(\"question_id\")\n    question_text = message\n\n    # Post to Slack as a threaded message\n    thread_ts = await self.slack.post_question(\n        minion_id, issue_number, question_text, timeout_minutes=10\n    )\n\n    # Store pending question\n    self._pending_questions[minion_id] = PendingQuestion(\n        minion_id=minion_id,\n        question_id=question_id,\n        issue_number=issue_number,\n        repo=repo,\n        question_text=question_text,\n        thread_ts=thread_ts,\n        asked_at=datetime.now(),\n    )\n</code></pre>"},{"location":"plans/2026-02-03-minion-clarifying-questions-design/#answer-endpoint","title":"Answer Endpoint","text":"<pre><code># GET /minion/answer/{minion_id}\nasync def _answer_handler(self, request):\n    minion_id = request.match_info[\"minion_id\"]\n    pending = self._pending_questions.get(minion_id)\n    if not pending or not pending.answered:\n        return web.json_response({\"answered\": False})\n    return web.json_response({\n        \"answered\": True,\n        \"answer\": pending.answer,\n    })\n</code></pre>"},{"location":"plans/2026-02-03-minion-clarifying-questions-design/#slack-thread-reply-matching","title":"Slack Thread Reply Matching","text":"<p>When a message arrives in a thread, check if <code>thread_ts</code> matches a pending question:</p> <pre><code># In SlackBot thread reply handler\nfor pending in self._pending_questions.values():\n    if pending.thread_ts == thread_ts and not pending.answered:\n        pending.answer = reply_text\n        pending.answered = True\n        break\n</code></pre>"},{"location":"plans/2026-02-03-minion-clarifying-questions-design/#slack-message-format","title":"Slack Message Format","text":"<pre><code>\ud83e\udd14 Minion `abc-123` on #42 has a question:\n\n&gt; The issue says \"improve performance\" but doesn't specify which\n&gt; endpoint. Should I focus on /api/users (slowest at 2.3s) or\n&gt; /api/search (most traffic)?\n\nReply in this thread to answer. Auto-continuing in 10 minutes.\n</code></pre>"},{"location":"plans/2026-02-03-minion-clarifying-questions-design/#minion-lifecycle-changes","title":"Minion Lifecycle Changes","text":"<p>Modified <code>_do_work()</code> with question loop:</p> <pre><code>MAX_QUESTIONS = 3\nQUESTION_TIMEOUT = 600  # 10 minutes\n\nquestions_asked = 0\n\nwhile True:\n    result: AgentResult = agent.run()\n\n    if result.status == AgentStatus.COMPLETED:\n        return True\n\n    elif result.status == AgentStatus.BLOCKED and result.question:\n        questions_asked += 1\n\n        if questions_asked &gt; MAX_QUESTIONS:\n            agent.inject_message(\"No more questions available. Use your best judgment.\")\n            continue\n\n        question_id = f\"q-{self.config.minion_id}-{questions_asked}\"\n        await self.reporter.question(result.question, result.blocker_type, question_id)\n\n        answer = await self.reporter.poll_answer(question_id, timeout=QUESTION_TIMEOUT)\n\n        if answer:\n            agent.inject_message(f\"Human response: {answer}\")\n        else:\n            agent.inject_message(\n                \"No response received within 10 minutes. Use your best judgment.\"\n            )\n        # Loop continues - agent.run() resumes with injected context\n\n    else:\n        # ERROR or TURN_LIMIT or BLOCKED without question\n        return False\n</code></pre>"},{"location":"plans/2026-02-03-minion-clarifying-questions-design/#minionagentinject_message","title":"MinionAgent.inject_message()","text":"<p>New method to append context to conversation history:</p> <pre><code>def inject_message(self, text: str) -&gt; None:\n    \"\"\"Inject a user message into conversation history for next run.\"\"\"\n    self._history.append({\"role\": \"user\", \"content\": text})\n</code></pre>"},{"location":"plans/2026-02-03-minion-clarifying-questions-design/#system-prompt-update","title":"System Prompt Update","text":"<p>Add to prompt_builder.py:</p> <pre><code>When requirements are unclear or you face a decision with multiple valid approaches,\ncall `task_blocked` with a question in the `question` field. You may receive a\nhuman response and continue working. Limit questions to what's truly necessary.\n</code></pre>"},{"location":"plans/2026-02-03-minion-clarifying-questions-design/#error-handling","title":"Error Handling","text":"Scenario Handling Overlord down when question sent Reporter returns False \u2192 inject \"use best judgment\", continue Slack post fails Overlord logs warning \u2192 Minion times out, continues Human never replies poll_answer() returns None after 10 min \u2192 continue Multiple questions rapid-fire Cap at 3 questions per Minion run Overlord restarts during poll In-memory questions lost \u2192 Minion times out, continues Thread reply from wrong user Accept any reply in the thread (team collaboration)"},{"location":"plans/2026-02-03-minion-clarifying-questions-design/#files-to-createmodify","title":"Files to Create/Modify","text":"File Change <code>nebulus_swarm/minion/reporter.py</code> Add QUESTION event, <code>question()</code>, <code>poll_answer()</code> <code>nebulus_swarm/minion/main.py</code> Rework <code>_do_work()</code> with question loop <code>nebulus_swarm/minion/agent/minion_agent.py</code> Add <code>inject_message()</code> method <code>nebulus_swarm/minion/agent/prompt_builder.py</code> Update system prompt <code>nebulus_swarm/overlord/main.py</code> Handle QUESTION event, add answer endpoint, thread matching <code>nebulus_swarm/overlord/slack_bot.py</code> Add <code>post_question()</code>, thread reply handler <code>tests/test_minion_question.py</code> New test file"},{"location":"plans/2026-02-03-minion-clarifying-questions-design/#testing","title":"Testing","text":"<ul> <li>Reporter question/poll_answer methods (mocked HTTP)</li> <li>Overlord QUESTION event handling and answer endpoint</li> <li>Minion question loop with mock answers</li> <li>Timeout auto-continue behavior</li> <li>Max questions cap enforcement (3 questions then auto-continue)</li> <li>Thread reply matching in SlackBot</li> </ul> <p>Document History: | Date | Author | Change | |------|--------|--------| | 2026-02-03 | @jlwestsr, Claude | Initial design |</p>"},{"location":"plans/2026-02-03-overlord-llm-parser-design/","title":"Overlord LLM-Powered Command Parser Design","text":"<p>Date: 2026-02-03 Status: APPROVED Authors: @jlwestsr, Claude Opus 4.5</p>"},{"location":"plans/2026-02-03-overlord-llm-parser-design/#goal","title":"Goal","text":"<p>Replace the Overlord's regex-based command parser with an LLM-powered interpreter that understands natural language, maintains conversational context, and gracefully falls back to regex when needed.</p>"},{"location":"plans/2026-02-03-overlord-llm-parser-design/#key-decisions","title":"Key Decisions","text":"Decision Choice Rationale LLM Model Local 8B (Llama 3.1 8B) Fast, free, sufficient for intent extraction Context Short-term (10 messages, 30 min TTL) Covers \"do that again\" without complexity Fallback LLM \u2192 Clarification \u2192 Regex Resilient, good UX Interface Keep existing Command/CommandType Minimal changes to Overlord main loop"},{"location":"plans/2026-02-03-overlord-llm-parser-design/#architecture","title":"Architecture","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                    LLMCommandParser                      \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n\u2502  \u2502 LLMClient   \u2502  \u2502 ContextStore\u2502  \u2502 RegexFallback   \u2502 \u2502\n\u2502  \u2502 (8B model)  \u2502  \u2502 (per-channel\u2502  \u2502 (existing       \u2502 \u2502\n\u2502  \u2502             \u2502  \u2502  memory)    \u2502  \u2502  CommandParser) \u2502 \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                     parse(text, channel_id, user_id)    \u2502\n\u2502                            \u2193                            \u2502\n\u2502                     Returns: Command                    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Flow: 1. Message arrives with channel/user context 2. Fetch recent conversation history (last 10 messages) 3. Build prompt with: system instructions + command schema + history + new message 4. Call 8B model for interpretation 5. Parse LLM response into <code>Command</code> object 6. If confidence low \u2192 return clarification request 7. If LLM fails/timeout \u2192 fall back to regex parser 8. Store message + result in context for future reference</p>"},{"location":"plans/2026-02-03-overlord-llm-parser-design/#llm-prompt-design","title":"LLM Prompt Design","text":"<pre><code>You are the Nebulus Overlord's command interpreter. Parse user messages into structured commands.\n\n## Available Commands\n\n1. STATUS - Check what minions are doing\n2. WORK - Start a minion on an issue (requires: issue_number, optional: repo)\n3. STOP - Stop a minion (requires: issue_number OR minion_id)\n4. QUEUE - Show pending work\n5. PAUSE - Pause automatic processing\n6. RESUME - Resume automatic processing\n7. HISTORY - Show recent completed work\n8. REVIEW - Review a PR (requires: pr_number, optional: repo)\n9. HELP - Show available commands\n\n## Response Format\n\nRespond with JSON only:\n{\"command\": \"WORK\", \"issue_number\": 42, \"repo\": null, \"confidence\": 0.95}\n\nIf uncertain, set confidence &lt; 0.7 and include \"clarification\" field with options.\n\n## Context\n\nDefault repo: {default_repo}\nRecent conversation:\n{conversation_history}\n\n## Examples\n\nUser: \"hey can you start working on issue 42\"\n\u2192 {\"command\": \"WORK\", \"issue_number\": 42, \"confidence\": 0.95}\n\nUser: \"do the same for 43\"\n\u2192 {\"command\": \"WORK\", \"issue_number\": 43, \"confidence\": 0.90}\n\nUser: \"what's up\"\n\u2192 {\"command\": \"STATUS\", \"confidence\": 0.85}\n\nUser: \"take a look at the PR\"\n\u2192 {\"command\": \"UNKNOWN\", \"confidence\": 0.4, \"clarification\": \"Which PR? Please specify a number like 'review PR #42'\"}\n</code></pre>"},{"location":"plans/2026-02-03-overlord-llm-parser-design/#context-store","title":"Context Store","text":"<pre><code>@dataclass\nclass ConversationEntry:\n    \"\"\"Single message in conversation history.\"\"\"\n    timestamp: datetime\n    user_id: str\n    message: str\n    parsed_command: Optional[Command]\n\nclass ContextStore:\n    \"\"\"In-memory conversation context per channel.\"\"\"\n\n    def __init__(self, max_entries: int = 10, ttl_minutes: int = 30):\n        self.max_entries = max_entries\n        self.ttl_minutes = ttl_minutes\n        self._contexts: Dict[str, List[ConversationEntry]] = {}\n\n    def add(self, channel_id: str, user_id: str, message: str,\n            command: Optional[Command]) -&gt; None:\n        \"\"\"Store a message and its parsed result.\"\"\"\n\n    def get_history(self, channel_id: str) -&gt; List[ConversationEntry]:\n        \"\"\"Get recent messages for context, pruning expired entries.\"\"\"\n\n    def get_last_command(self, channel_id: str) -&gt; Optional[Command]:\n        \"\"\"Get the most recent successful command (for 'do that again').\"\"\"\n\n    def clear(self, channel_id: str) -&gt; None:\n        \"\"\"Clear context for a channel.\"\"\"\n</code></pre> <p>Format for LLM prompt: <pre><code>Recent conversation:\n[2 min ago] user123: \"work on issue 42\" \u2192 WORK #42\n[1 min ago] user123: \"what's the status\" \u2192 STATUS\n[now] user123: \"do the same for 43\"\n</code></pre></p>"},{"location":"plans/2026-02-03-overlord-llm-parser-design/#fallback-error-handling","title":"Fallback &amp; Error Handling","text":"<pre><code>Message received\n      \u2193\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Try LLM parse   \u2502\u2500\u2500timeout/error\u2500\u2500\u2192 Regex fallback\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                         \u2193\n         \u2193                            Return Command\n   confidence &gt;= 0.7?\n      \u2193          \u2193\n     YES         NO\n      \u2193          \u2193\n  Return      Has clarification?\n  Command        \u2193         \u2193\n               YES        NO\n                \u2193          \u2193\n           Ask user    Regex fallback\n           to clarify\n</code></pre> <p>Implementation:</p> <pre><code>class LLMCommandParser:\n    CONFIDENCE_THRESHOLD = 0.7\n    LLM_TIMEOUT = 5.0  # seconds - fast for chat UX\n\n    async def parse(self, text: str, channel_id: str, user_id: str) -&gt; ParseResult:\n        try:\n            result = await asyncio.wait_for(\n                self._llm_parse(text, channel_id),\n                timeout=self.LLM_TIMEOUT\n            )\n\n            if result.confidence &gt;= self.CONFIDENCE_THRESHOLD:\n                return ParseResult(command=result.command)\n\n            if result.clarification:\n                return ParseResult(needs_clarification=True,\n                                   message=result.clarification)\n\n            return self._regex_fallback(text)\n\n        except (asyncio.TimeoutError, LLMError):\n            return self._regex_fallback(text)\n\n    def _regex_fallback(self, text: str) -&gt; ParseResult:\n        \"\"\"Use existing CommandParser as fallback.\"\"\"\n        return ParseResult(command=self.regex_parser.parse(text))\n</code></pre>"},{"location":"plans/2026-02-03-overlord-llm-parser-design/#configuration","title":"Configuration","text":"<pre><code>@dataclass\nclass OverlordLLMConfig:\n    enabled: bool = True  # Can disable to use regex only\n    base_url: str = \"http://localhost:5000/v1\"\n    model: str = \"llama-3.1-8b\"\n    timeout: float = 5.0\n    confidence_threshold: float = 0.7\n    context_max_entries: int = 10\n    context_ttl_minutes: int = 30\n</code></pre> <p>Environment variables: - <code>OVERLORD_LLM_ENABLED</code> - Toggle LLM parsing (default: true) - <code>OVERLORD_LLM_MODEL</code> - Model name (default: llama-3.1-8b) - <code>OVERLORD_LLM_TIMEOUT</code> - Timeout in seconds (default: 5.0) - <code>OVERLORD_LLM_CONFIDENCE</code> - Confidence threshold (default: 0.7)</p>"},{"location":"plans/2026-02-03-overlord-llm-parser-design/#files-to-createmodify","title":"Files to Create/Modify","text":"<p>New Files:</p> File Purpose ~Lines <code>nebulus_swarm/overlord/llm_parser.py</code> LLMCommandParser, ContextStore, ParseResult ~250 <code>tests/test_llm_parser.py</code> Unit tests for LLM parsing ~150 <p>Modified Files:</p> File Change <code>nebulus_swarm/overlord/main.py</code> Replace <code>CommandParser</code> with <code>LLMCommandParser</code> <code>nebulus_swarm/config.py</code> Add <code>OverlordLLMConfig</code> <p>Unchanged: - <code>command_parser.py</code> - Retained as regex fallback</p>"},{"location":"plans/2026-02-03-overlord-llm-parser-design/#testing","title":"Testing","text":"<pre><code>class TestLLMCommandParser:\n    def test_parse_natural_work_command(self):\n        \"\"\"'hey start working on issue 42' \u2192 WORK #42\"\"\"\n\n    def test_parse_with_context(self):\n        \"\"\"'do the same for 43' after WORK #42 \u2192 WORK #43\"\"\"\n\n    def test_low_confidence_clarification(self):\n        \"\"\"Ambiguous message returns clarification request\"\"\"\n\n    def test_fallback_on_timeout(self):\n        \"\"\"LLM timeout falls back to regex silently\"\"\"\n\n    def test_context_expiry(self):\n        \"\"\"Old messages pruned after TTL\"\"\"\n\nclass TestContextStore:\n    def test_max_entries_enforced(self):\n    def test_ttl_expiry(self):\n    def test_get_last_command(self):\n</code></pre>"},{"location":"plans/2026-02-03-overlord-llm-parser-design/#user-experience-examples","title":"User Experience Examples","text":"<p>Before (regex): - \"work on #42\" \u2713 - \"hey can you start on issue 42\" \u2717 UNKNOWN</p> <p>After (LLM): - \"work on #42\" \u2713 - \"hey can you start on issue 42\" \u2713 - \"do the same for 43\" \u2713 (with context) - \"what's happening\" \u2713 \u2192 STATUS - \"stop that\" \u2713 (stops last started minion) - \"check the PR\" \u2192 \"Which PR? Please specify a number like 'review PR #42'\"</p> <p>Document History: | Date | Author | Change | |------|--------|--------| | 2026-02-03 | @jlwestsr, Claude | Initial design |</p>"},{"location":"plans/2026-02-03-pr-review-integration-design/","title":"PR Review Integration Design","text":""},{"location":"plans/2026-02-03-pr-review-integration-design/#goal","title":"Goal","text":"<p>When a Minion creates a PR, automatically run the full PR review pipeline and post results as a comment, leaving merge to humans.</p>"},{"location":"plans/2026-02-03-pr-review-integration-design/#flow","title":"Flow","text":"<pre><code>Minion completes work\n    \u2193\nCreates PR\n    \u2193\nTriggers ReviewWorkflow\n    \u2193\n\u251c\u2500\u2500 Run automated checks (tests, lint, security)\n\u251c\u2500\u2500 Run LLM code review\n\u2514\u2500\u2500 Compile full report\n    \u2193\nPost review as PR comment\n    \u2193\nReport success to Overlord (with review summary)\n    \u2193\nHuman reviews and merges\n</code></pre>"},{"location":"plans/2026-02-03-pr-review-integration-design/#key-decisions","title":"Key Decisions","text":"<ul> <li>Review runs inside the Minion container before it exits</li> <li>No auto-merge - humans always approve</li> <li>Full report posted: checks + LLM analysis + recommendation</li> <li>Review result included in Overlord completion report</li> </ul>"},{"location":"plans/2026-02-03-pr-review-integration-design/#implementation","title":"Implementation","text":""},{"location":"plans/2026-02-03-pr-review-integration-design/#integration-point","title":"Integration Point","text":"<p><code>nebulus_swarm/minion/main.py</code> in the <code>_create_pr()</code> method, immediately after the PR is created and before reporting completion.</p>"},{"location":"plans/2026-02-03-pr-review-integration-design/#new-method-_review_pr","title":"New Method: <code>_review_pr()</code>","text":"<pre><code>async def _review_pr(self, pr_number: int) -&gt; Optional[WorkflowResult]:\n    \"\"\"Run PR review after creation.\"\"\"\n    config = ReviewConfig(\n        github_token=self.config.github_token,\n        llm_base_url=self.config.nebulus_base_url,\n        llm_model=self.config.nebulus_model,\n        auto_merge_enabled=False,  # Never auto-merge\n        run_local_checks=True,\n    )\n    workflow = ReviewWorkflow(config)\n    return await workflow.review_pr(self.config.repo, pr_number)\n</code></pre>"},{"location":"plans/2026-02-03-pr-review-integration-design/#comment-format","title":"Comment Format","text":"<pre><code>## \ud83e\udd16 Minion Code Review\n\n### Automated Checks\n- \u2705 Tests: 42 passed\n- \u26a0\ufe0f Linting: 2 warnings\n- \u2705 Security: No issues\n\n### Code Review\n[LLM analysis here]\n\n### Recommendation\n**APPROVE** (Confidence: 85%)\n\n---\n*Reviewed by Nebulus Swarm Minion `minion-test-003`*\n</code></pre>"},{"location":"plans/2026-02-03-pr-review-integration-design/#error-handling","title":"Error Handling","text":"Scenario Handling LLM timeout Post partial review (checks only), log warning Checks fail to run Post LLM review only, note checks skipped Review post fails Log error, still report PR success to Overlord All review fails Log error, PR still exists - human can review manually <p>Key Principle: Review failures should never block PR creation. The PR is valuable even without automated review.</p>"},{"location":"plans/2026-02-03-pr-review-integration-design/#files-to-modify","title":"Files to Modify","text":"<ul> <li><code>nebulus_swarm/minion/main.py</code> - Add <code>_review_pr()</code>, call after PR creation</li> <li><code>nebulus_swarm/minion/github_client.py</code> - Add <code>post_pr_comment()</code> if not exists</li> <li><code>tests/test_minion_agent.py</code> - Add review integration tests</li> </ul>"},{"location":"plans/2026-02-03-pr-review-integration-design/#testing","title":"Testing","text":"<ol> <li>Unit test: <code>_review_pr()</code> method with mocked ReviewWorkflow</li> <li>Integration test: Create real PR, verify comment posted</li> <li>Error test: Verify graceful degradation when review fails</li> </ol>"},{"location":"plans/2026-02-03-swarm-dashboard-design/","title":"Swarm Dashboard Design","text":"<p>Date: 2026-02-03 Status: APPROVED Authors: @jlwestsr, Claude Opus 4.5</p>"},{"location":"plans/2026-02-03-swarm-dashboard-design/#goal","title":"Goal","text":"<p>A standalone Streamlit dashboard for monitoring the Nebulus Swarm: real-time minion status, work history, GitHub queue, and aggregate metrics. Read-only ops view with auto-refresh.</p>"},{"location":"plans/2026-02-03-swarm-dashboard-design/#key-decisions","title":"Key Decisions","text":"Decision Choice Rationale Placement New app in <code>nebulus_swarm/dashboard/</code> Different domain from CLI telemetry, different deployment context Data source SQLite + API hybrid Real-time from Overlord HTTP API, historical from state.db Pages Live, History, Queue, Metrics Full ops coverage: monitoring + review + analytics Refresh Auto-refresh with toggle (10s) Live monitoring when watching, stable view when reading Auth None (V1) Read-only, private network. Can add later."},{"location":"plans/2026-02-03-swarm-dashboard-design/#architecture","title":"Architecture","text":"<pre><code>nebulus_swarm/dashboard/\n\u251c\u2500\u2500 __init__.py\n\u251c\u2500\u2500 app.py              # Entry point, sidebar, page routing, auto-refresh\n\u251c\u2500\u2500 data.py             # SwarmDataClient (API + SQLite)\n\u2514\u2500\u2500 pages/\n    \u251c\u2500\u2500 live.py          # Active minions, health, pending questions\n    \u251c\u2500\u2500 history.py       # Work log with filters\n    \u251c\u2500\u2500 queue.py         # Pending GitHub issues\n    \u2514\u2500\u2500 metrics.py       # Success rate, duration, throughput charts\n</code></pre> <p>Data flow: <pre><code>Overlord HTTP API \u2500\u2500\u2192 SwarmDataClient \u2500\u2500\u2192 Streamlit Pages\n  GET /status              \u2502\n  GET /queue               \u2502\n                           \u2502\nstate.db (SQLite) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n  work_history table\n  minions table\n</code></pre></p> <p>Entry point: <pre><code>streamlit run nebulus_swarm/dashboard/app.py\n</code></pre></p>"},{"location":"plans/2026-02-03-swarm-dashboard-design/#swarmdataclient","title":"SwarmDataClient","text":"<p>Single class that abstracts both data sources:</p> <pre><code>class SwarmDataClient:\n    def __init__(self, overlord_url: str, state_db_path: str):\n        self._url = overlord_url\n        self._state = OverlordState(db_path=state_db_path)\n\n    # Real-time (API)\n    def get_status(self) -&gt; dict: ...         # GET /status\n    def get_queue(self) -&gt; list[dict]: ...    # GET /queue\n\n    # Historical (SQLite)\n    def get_work_history(self, repo=None, status=None, limit=50) -&gt; list[dict]: ...\n    def get_metrics(self, days=7) -&gt; dict: ...\n</code></pre> <p>API responses are cached for 5 seconds to avoid hammering the Overlord during auto-refresh.</p>"},{"location":"plans/2026-02-03-swarm-dashboard-design/#page-designs","title":"Page Designs","text":""},{"location":"plans/2026-02-03-swarm-dashboard-design/#live-status-page","title":"Live Status Page","text":"<p>Top row - Metric cards (<code>st.metric()</code>): - Overlord health (healthy/unhealthy) - Active minions / max concurrent - Queue status (paused/active) - Docker availability</p> <p>Active Minions table: - Columns: ID, repo, issue #, status, elapsed time, last heartbeat - Stale heartbeats (&gt;2 min) get warning indicator - Issue numbers link to GitHub - Empty state: \"No active minions\"</p> <p>Pending Questions section: - Shows minions waiting for human answers (from E.2 feature) - Displays question text, wait time, \"answer in Slack\" note - Data from <code>pending_questions</code> field added to <code>/status</code> response</p>"},{"location":"plans/2026-02-03-swarm-dashboard-design/#work-history-page","title":"Work History Page","text":"<p>Filter bar: - Repo selector (dropdown from available repos) - Status filter (all/completed/failed/timeout) - Limit slider (10-100, default 50)</p> <p>History table (<code>st.dataframe()</code>): - Columns: repo, issue #, status (color-coded), PR # (linked), duration, error, timestamp - Sorted by most recent first - Green = completed, red = failed, orange = timeout</p> <p>Summary row: - Total records, completion rate, average duration</p> <p>Data: <code>OverlordState.get_work_history()</code> with added <code>status</code> filter parameter.</p>"},{"location":"plans/2026-02-03-swarm-dashboard-design/#queue-page","title":"Queue Page","text":"<p>Queue summary: - Total pending issues - Available minion slots - Queue processing status (paused/active)</p> <p>Pending issues table: - Columns: repo, issue #, title, priority, age - Sorted by priority then age</p> <p>Data: New <code>GET /queue</code> endpoint on Overlord that caches last scan results. If Overlord unreachable or no scan yet: \"Queue data unavailable - waiting for next scan.\"</p>"},{"location":"plans/2026-02-03-swarm-dashboard-design/#metrics-page","title":"Metrics Page","text":"<p>Time range selector: Last 24h, 7 days, 30 days, all time.</p> <p>Success Rate: - Big number with color indicator (green &gt;80%, yellow &gt;50%, red below) - Breakdown bar: X completed, Y failed, Z timeout</p> <p>Duration Trends: - Bar chart: average duration per day (<code>st.bar_chart()</code>) - Stats: median, fastest, slowest</p> <p>Throughput: - Line chart: tasks completed per day</p> <p>Failure Analysis: - Table grouped by error type with count and most recent message</p> <p>Data: SQL aggregations on work_history table via pandas DataFrames.</p>"},{"location":"plans/2026-02-03-swarm-dashboard-design/#overlord-changes","title":"Overlord Changes","text":""},{"location":"plans/2026-02-03-swarm-dashboard-design/#status-endpoint-addition","title":"<code>/status</code> endpoint addition","text":"<p>Add <code>pending_questions</code> to the response:</p> <pre><code>async def _status_handler(self, request):\n    return web.json_response({\n        ...existing fields...\n        \"pending_questions\": [\n            {\n                \"minion_id\": pq.minion_id,\n                \"question_id\": pq.question_id,\n                \"issue_number\": pq.issue_number,\n                \"question_text\": pq.question_text,\n                \"asked_at\": pq.asked_at.isoformat(),\n                \"answered\": pq.answered,\n            }\n            for pq in self._pending_questions.values()\n        ],\n    })\n</code></pre>"},{"location":"plans/2026-02-03-swarm-dashboard-design/#new-get-queue-endpoint","title":"New <code>GET /queue</code> endpoint","text":"<pre><code># Cached last scan results\nself._last_queue_scan: list[dict] = []\n\nasync def _queue_handler(self, request):\n    return web.json_response({\n        \"issues\": self._last_queue_scan,\n        \"paused\": self._paused,\n    })\n</code></pre> <p>Updated during <code>_sweep_queue()</code> to cache scan results.</p>"},{"location":"plans/2026-02-03-swarm-dashboard-design/#state-query-enhancement","title":"State query enhancement","text":"<p>Add <code>status</code> filter to <code>get_work_history()</code>:</p> <pre><code>def get_work_history(self, repo=None, status=None, limit=50):\n    query = \"SELECT * FROM work_history WHERE 1=1\"\n    params = []\n    if repo:\n        query += \" AND repo = ?\"\n        params.append(repo)\n    if status:\n        query += \" AND status = ?\"\n        params.append(status)\n    query += \" ORDER BY completed_at DESC LIMIT ?\"\n    params.append(limit)\n    ...\n</code></pre>"},{"location":"plans/2026-02-03-swarm-dashboard-design/#error-handling","title":"Error Handling","text":"Scenario Handling Overlord unreachable Red banner on Live/Queue pages, History/Metrics still work State DB missing Empty states (\"No work history yet\") State DB empty Graceful empty tables and zero-value metrics API timeout Show last cached data with \"stale\" indicator Auto-refresh off Static page until manual refresh"},{"location":"plans/2026-02-03-swarm-dashboard-design/#configuration","title":"Configuration","text":"Env Var Default Purpose <code>OVERLORD_URL</code> <code>http://localhost:8080</code> Overlord HTTP base URL <code>OVERLORD_STATE_DB</code> <code>/var/lib/overlord/state.db</code> Path to state database <p>Both also configurable in sidebar for development.</p>"},{"location":"plans/2026-02-03-swarm-dashboard-design/#files-to-createmodify","title":"Files to Create/Modify","text":"File Action <code>nebulus_swarm/dashboard/__init__.py</code> Create <code>nebulus_swarm/dashboard/app.py</code> Create <code>nebulus_swarm/dashboard/data.py</code> Create <code>nebulus_swarm/dashboard/pages/live.py</code> Create <code>nebulus_swarm/dashboard/pages/history.py</code> Create <code>nebulus_swarm/dashboard/pages/queue.py</code> Create <code>nebulus_swarm/dashboard/pages/metrics.py</code> Create <code>nebulus_swarm/overlord/main.py</code> Add pending_questions to /status, add GET /queue <code>nebulus_swarm/overlord/state.py</code> Add status filter to get_work_history() <code>tests/test_swarm_dashboard.py</code> Create"},{"location":"plans/2026-02-03-swarm-dashboard-design/#testing","title":"Testing","text":"<ul> <li>SwarmDataClient: mocked HTTP responses + test SQLite database</li> <li>Page functions: verify rendering with sample data (no browser tests)</li> <li>Overlord endpoints: test new /queue endpoint and /status additions</li> <li>State query: test status filter on get_work_history()</li> </ul> <p>Document History: | Date | Author | Change | |------|--------|--------| | 2026-02-03 | @jlwestsr, Claude | Initial design |</p>"},{"location":"plans/2026-02-04-demo-and-swarm-setup/","title":"Demo &amp; Swarm Infrastructure Setup Guide","text":""},{"location":"plans/2026-02-04-demo-and-swarm-setup/#overview","title":"Overview","text":"<p>This guide covers everything needed to run the full Nebulus Atom demo: - Act 1: Core CLI Agent (interactive coding) - Act 2: Nebulus Swarm (Slack \u2192 GitHub \u2192 Docker minions \u2192 PR) - Act 3: Dashboards (Flight Recorder + Swarm Monitor)</p>"},{"location":"plans/2026-02-04-demo-and-swarm-setup/#act-1-core-agent-ready-now","title":"Act 1: Core Agent (Ready Now)","text":""},{"location":"plans/2026-02-04-demo-and-swarm-setup/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.12+ with venv at <code>venv/</code></li> <li>LLM server running at <code>NEBULUS_BASE_URL</code> (configured in <code>.env</code>)</li> </ul>"},{"location":"plans/2026-02-04-demo-and-swarm-setup/#quick-start","title":"Quick Start","text":"<pre><code>cd /home/jlwestsr/projects/west_ai_labs/nebulus-atom\nsource venv/bin/activate\npython3 -m nebulus_atom.main start\n</code></pre>"},{"location":"plans/2026-02-04-demo-and-swarm-setup/#demo-flow","title":"Demo Flow","text":"<ol> <li>Agent starts, health-checks the LLM server, shows banner</li> <li>Context pinning: <code>pin_file nebulus_atom/config.py</code> then <code>/context</code></li> <li>RAG search: Ask \"index the codebase\" first, then \"search for how tools are dispatched\"</li> <li>Skill creation: Ask \"create a skill called word_count that counts words in a file\"</li> <li>TDD loop: Ask \"start a TDD cycle: implement a function that reverses a string\"</li> <li>Auto-execution: <code>create_plan</code>, <code>add_task</code>, <code>execute_plan</code></li> </ol>"},{"location":"plans/2026-02-04-demo-and-swarm-setup/#act-2-swarm-setup","title":"Act 2: Swarm Setup","text":""},{"location":"plans/2026-02-04-demo-and-swarm-setup/#step-1-create-a-github-personal-access-token","title":"Step 1: Create a GitHub Personal Access Token","text":"<ol> <li>Go to https://github.com/settings/tokens?type=beta (Fine-grained tokens)</li> <li>Click \"Generate new token\"</li> <li>Settings:</li> <li>Token name: <code>nebulus-swarm-demo</code></li> <li>Expiration: 30 days</li> <li>Repository access: Select \"Only select repositories\" \u2192 pick your demo repo (Step 2)</li> <li>Permissions:<ul> <li>Repository permissions:</li> <li>Contents: Read and write</li> <li>Issues: Read and write</li> <li>Pull requests: Read and write</li> <li>Metadata: Read-only (auto-granted)</li> </ul> </li> <li>Click \"Generate token\" and save it \u2014 you'll need it for <code>.env.swarm</code></li> </ol>"},{"location":"plans/2026-02-04-demo-and-swarm-setup/#step-2-create-a-demo-github-repository","title":"Step 2: Create a Demo GitHub Repository","text":"<ol> <li>Create a new repo: https://github.com/new</li> <li>Name: <code>nebulus-swarm-demo</code> (or similar)</li> <li>Visibility: Public (for demo) or Private</li> <li>Initialize with README: Yes</li> <li>Clone it locally:    <pre><code>git clone git@github.com:&lt;your-username&gt;/nebulus-swarm-demo.git /tmp/nebulus-swarm-demo\n</code></pre></li> <li>Seed it with a simple Python project and issues:</li> </ol> <pre><code>cd /tmp/nebulus-swarm-demo\n\n# Create a basic project structure\nmkdir -p src tests\ncat &gt; src/calculator.py &lt;&lt; 'PYEOF'\n\"\"\"Simple calculator module.\"\"\"\n\n\ndef add(a: float, b: float) -&gt; float:\n    \"\"\"Add two numbers.\"\"\"\n    return a + b\n\n\ndef subtract(a: float, b: float) -&gt; float:\n    \"\"\"Subtract b from a.\"\"\"\n    return a - b\nPYEOF\n\ncat &gt; tests/test_calculator.py &lt;&lt; 'PYEOF'\nfrom src.calculator import add, subtract\n\n\ndef test_add():\n    assert add(2, 3) == 5\n\n\ndef test_subtract():\n    assert subtract(5, 3) == 2\nPYEOF\n\ncat &gt; requirements.txt &lt;&lt; 'PYEOF'\npytest&gt;=7.0.0\nPYEOF\n\ngit add -A &amp;&amp; git commit -m \"feat: initial calculator project\" &amp;&amp; git push\n</code></pre> <ol> <li>Create demo issues using the GitHub CLI:    <pre><code>gh issue create --repo &lt;your-username&gt;/nebulus-swarm-demo \\\n  --title \"Add multiply function to calculator\" \\\n  --body \"Add a multiply(a, b) function to src/calculator.py that multiplies two numbers. Include a test in tests/test_calculator.py.\"\n\ngh issue create --repo &lt;your-username&gt;/nebulus-swarm-demo \\\n  --title \"Add divide function with zero-division handling\" \\\n  --body \"Add a divide(a, b) function to src/calculator.py. It should raise ValueError if b is 0. Include tests for both normal division and the error case.\"\n\ngh issue create --repo &lt;your-username&gt;/nebulus-swarm-demo \\\n  --title \"Add docstring to all test functions\" \\\n  --body \"Each test function in tests/test_calculator.py should have a one-line docstring describing what it tests.\"\n</code></pre></li> </ol>"},{"location":"plans/2026-02-04-demo-and-swarm-setup/#step-3-create-a-slack-app","title":"Step 3: Create a Slack App","text":"<ol> <li>Go to https://api.slack.com/apps and click \"Create New App\"</li> <li>Choose \"From scratch\"</li> <li>App Name: <code>Nebulus Swarm</code></li> <li>Workspace: Select your workspace</li> <li>Enable Socket Mode:</li> <li>Go to Socket Mode in the left sidebar</li> <li>Toggle \"Enable Socket Mode\" ON</li> <li>Create an app-level token:<ul> <li>Token Name: <code>nebulus-socket</code></li> <li>Scope: <code>connections:write</code></li> </ul> </li> <li>Save the token (starts with <code>xapp-</code>) \u2014 this is your <code>SLACK_APP_TOKEN</code></li> <li>Add Bot Scopes:</li> <li>Go to OAuth &amp; Permissions</li> <li>Under \"Bot Token Scopes\", add:<ul> <li><code>chat:write</code></li> <li><code>channels:history</code></li> <li><code>channels:read</code></li> <li><code>app_mentions:read</code></li> </ul> </li> <li>Enable Events:</li> <li>Go to Event Subscriptions</li> <li>Toggle \"Enable Events\" ON</li> <li>Under \"Subscribe to bot events\", add:<ul> <li><code>message.channels</code></li> <li><code>app_mention</code></li> </ul> </li> <li>Install to Workspace:</li> <li>Go to Install App</li> <li>Click \"Install to Workspace\" and authorize</li> <li>Copy the Bot User OAuth Token (starts with <code>xoxb-</code>) \u2014 this is your <code>SLACK_BOT_TOKEN</code></li> <li>Get Channel ID:</li> <li>In Slack, right-click the channel \u2192 \"View channel details\"</li> <li>The Channel ID is at the bottom (starts with <code>C</code>)</li> <li>Invite the bot: <code>/invite @Nebulus Swarm</code></li> </ol>"},{"location":"plans/2026-02-04-demo-and-swarm-setup/#step-4-configure-environment","title":"Step 4: Configure Environment","text":"<p>Create <code>.env.swarm</code> in the project root:</p> <pre><code>cat &gt; .env.swarm &lt;&lt; 'EOF'\n# Slack\nSLACK_BOT_TOKEN=xoxb-your-token-here\nSLACK_APP_TOKEN=xapp-your-token-here\nSLACK_CHANNEL_ID=C0123456789\n\n# GitHub\nGITHUB_TOKEN=github_pat_your-token-here\nGITHUB_WATCHED_REPOS=your-username/nebulus-swarm-demo\n\n# LLM Backend (same as core agent)\nNEBULUS_BASE_URL=http://192.168.4.30:8080/v1\nNEBULUS_API_KEY=admin\nNEBULUS_MODEL=qwen3-coder-30b\nNEBULUS_TIMEOUT=600\nNEBULUS_STREAMING=false\n\n# Minion Settings\nMAX_CONCURRENT_MINIONS=2\n\n# Cron (disable for demo \u2014 trigger manually via Slack)\nCRON_ENABLED=false\n\n# Model Routing (optional)\nROUTING_ENABLED=false\n\n# PR Reviewer (optional)\nREVIEWER_ENABLED=false\nEOF\n</code></pre>"},{"location":"plans/2026-02-04-demo-and-swarm-setup/#step-5-build-docker-images","title":"Step 5: Build Docker Images","text":"<pre><code>cd /home/jlwestsr/projects/west_ai_labs/nebulus-atom\n\n# Build the minion image first (Overlord spawns these)\ndocker build -t nebulus-minion:latest -f nebulus_swarm/minion/Dockerfile .\n\n# Build the overlord image\ndocker build -t nebulus-overlord:latest -f nebulus_swarm/overlord/Dockerfile .\n</code></pre>"},{"location":"plans/2026-02-04-demo-and-swarm-setup/#step-6-start-the-swarm","title":"Step 6: Start the Swarm","text":"<pre><code># Start the Overlord\ndocker-compose -f docker-compose.swarm.yml up -d overlord\n\n# Check it's running\ndocker logs -f overlord\n\n# Verify health\ncurl http://localhost:8080/health\n</code></pre>"},{"location":"plans/2026-02-04-demo-and-swarm-setup/#step-7-demo-commands-in-slack","title":"Step 7: Demo Commands in Slack","text":"<p><pre><code>work on your-username/nebulus-swarm-demo#1\n</code></pre> This will: 1. Queue the issue 2. Spawn a minion container 3. Clone the repo 4. Read the issue 5. Write code 6. Create a PR 7. Report back in Slack</p> <p>Other commands: <pre><code>status          # Show active minions\nqueue           # Show pending work\nhistory         # Show completed work\ncancel &lt;id&gt;     # Kill a minion\nsweep           # Process entire queue\n</code></pre></p>"},{"location":"plans/2026-02-04-demo-and-swarm-setup/#act-3-dashboards","title":"Act 3: Dashboards","text":""},{"location":"plans/2026-02-04-demo-and-swarm-setup/#flight-recorder-core-agent","title":"Flight Recorder (Core Agent)","text":"<pre><code>source venv/bin/activate\npython3 -m nebulus_atom.main dashboard\n# Opens at http://localhost:8501\n</code></pre>"},{"location":"plans/2026-02-04-demo-and-swarm-setup/#swarm-dashboard","title":"Swarm Dashboard","text":"<pre><code>source venv/bin/activate\nstreamlit run nebulus_swarm/dashboard/app.py\n# Opens at http://localhost:8502\n</code></pre> <p>Note: The Swarm Dashboard reads from the Overlord's SQLite state database. For local development, you may need to mount or copy the state DB.</p>"},{"location":"plans/2026-02-04-demo-and-swarm-setup/#troubleshooting","title":"Troubleshooting","text":"Issue Fix Agent says \"Cannot reach LLM server\" Check that your LLM server is running and <code>NEBULUS_BASE_URL</code> is correct Agent shows \"Model not found\" Run <code>curl $NEBULUS_BASE_URL/models</code> and update <code>NEBULUS_MODEL</code> in <code>.env</code> Slack bot doesn't respond Verify Socket Mode is enabled and <code>SLACK_APP_TOKEN</code> starts with <code>xapp-</code> Minion can't clone repo Check <code>GITHUB_TOKEN</code> has Contents read/write permission Minion can't create PR Check <code>GITHUB_TOKEN</code> has Pull requests read/write permission Docker build fails Ensure Docker is running: <code>docker info</code> Dashboard shows no data Run the agent first to generate telemetry data"},{"location":"plans/2026-02-05-overlord-design/","title":"Overlord: Cross-Project Meta-Orchestrator","text":"<p>Author: West AI Labs Date: 2026-02-05 Status: Phase 1 Complete Atom Version: V3 (evolves the existing Swarm Overlord)</p>"},{"location":"plans/2026-02-05-overlord-design/#1-vision","title":"1. Vision","text":"<p>The Overlord is the evolution of Atom's Swarm supervisor into a cross-project meta-orchestrator. Where the current Overlord dispatches Minions to work on individual GitHub issues within a single repo, the new Overlord thinks across the entire Nebulus ecosystem \u2014 understanding project relationships, dispatching agents to multiple repos simultaneously, managing releases, enforcing standards, and learning from every interaction.</p> <p>The Overlord is what you'd get if a senior engineering manager had perfect memory, never slept, and could spin up junior engineers on demand.</p>"},{"location":"plans/2026-02-05-overlord-design/#what-the-overlord-does","title":"What the Overlord Does","text":"<ul> <li>Observes: Scans all managed repos for git state, open issues, stale branches,   test health, dependency drift, and changelog gaps</li> <li>Decides: Prioritizes work across projects, identifies cross-repo dependencies,   selects the right agent and model for each task</li> <li>Dispatches: Spawns Minion workers for implementation tasks, or executes   lightweight chores (merges, tags, pushes) directly</li> <li>Verifies: Evaluates worker output, runs tests, reviews PRs before reporting   completion</li> <li>Learns: Maintains cross-project memory \u2014 patterns, preferences, pitfalls,   and project relationships</li> <li>Reports: Provides status summaries at any level of detail, from a quick   health check to a full ecosystem audit</li> </ul>"},{"location":"plans/2026-02-05-overlord-design/#what-the-overlord-does-not-do","title":"What the Overlord Does NOT Do","text":"<ul> <li>Write code itself \u2014 it dispatches workers for implementation</li> <li>Push without approval \u2014 the human-in-the-loop trust boundary is sacred</li> <li>Modify its own capabilities \u2014 skill evolution requires explicit user approval</li> <li>Cross the trust boundary \u2014 workers cannot escalate privileges through the   Overlord</li> </ul>"},{"location":"plans/2026-02-05-overlord-design/#2-architecture","title":"2. Architecture","text":""},{"location":"plans/2026-02-05-overlord-design/#21-evolution-not-revolution","title":"2.1 Evolution, Not Revolution","text":"<p>The Overlord replaces the current <code>nebulus_swarm/overlord/</code> codebase. This is not a new layer \u2014 it is an upgrade of the existing supervisor from single-repo to multi-repo awareness. All existing capabilities (Docker-based Minion dispatch, GitHub queue, Slack bot, evaluation, audit trail) are preserved and extended.</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                        USER                             \u2502\n\u2502         (Human-in-the-Loop \u2014 Trust Boundary)            \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502   CLI    \u2502     Slack Bot    \u2502      Gantry UI            \u2502\n\u2502 (power)  \u2502  (async/notify)  \u2502   (dashboard/visual)      \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                  INTERFACE ADAPTER LAYER                 \u2502\n\u2502           Normalizes commands + renders responses        \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                      OVERLORD CORE                      \u2502\n\u2502                                                         \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\n\u2502  \u2502  Autonomy   \u2502  \u2502   Project    \u2502  \u2502   Dispatch    \u2502  \u2502\n\u2502  \u2502  Engine     \u2502  \u2502   Registry   \u2502  \u2502   Engine      \u2502  \u2502\n\u2502  \u2502             \u2502  \u2502              \u2502  \u2502               \u2502  \u2502\n\u2502  \u2502 \u2022 cron      \u2502  \u2502 \u2022 repo map   \u2502  \u2502 \u2022 minion pool \u2502  \u2502\n\u2502  \u2502 \u2022 watchers  \u2502  \u2502 \u2022 dep graph  \u2502  \u2502 \u2022 model router\u2502  \u2502\n\u2502  \u2502 \u2022 triggers  \u2502  \u2502 \u2022 standards  \u2502  \u2502 \u2022 scope mgr   \u2502  \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n\u2502                                                         \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\n\u2502  \u2502  Evaluator  \u2502  \u2502   Memory     \u2502  \u2502   Audit       \u2502  \u2502\n\u2502  \u2502             \u2502  \u2502   (Cross-    \u2502  \u2502   Trail       \u2502  \u2502\n\u2502  \u2502 \u2022 tests     \u2502  \u2502    Project)  \u2502  \u2502               \u2502  \u2502\n\u2502  \u2502 \u2022 lint      \u2502  \u2502 \u2022 patterns   \u2502  \u2502 \u2022 hash chain  \u2502  \u2502\n\u2502  \u2502 \u2022 review    \u2502  \u2502 \u2022 preferences\u2502  \u2502 \u2022 signatures  \u2502  \u2502\n\u2502  \u2502 \u2022 scoring   \u2502  \u2502 \u2022 relations  \u2502  \u2502 \u2022 compliance  \u2502  \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                    MINION WORKERS                        \u2502\n\u2502        (Ephemeral containers, scoped to task)            \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"plans/2026-02-05-overlord-design/#22-core-components","title":"2.2 Core Components","text":""},{"location":"plans/2026-02-05-overlord-design/#project-registry","title":"Project Registry","text":"<p>The Overlord maintains a registry of all managed projects with their metadata, relationships, and health state.</p> <pre><code># ~/.atom/overlord.yml\nprojects:\n  nebulus-core:\n    path: ~/projects/west_ai_labs/nebulus-core\n    remote: jlwestsr/nebulus-core\n    role: shared-library\n    branch_model: develop-main\n    depends_on: []\n\n  nebulus-prime:\n    path: ~/projects/west_ai_labs/nebulus-prime\n    remote: jlwestsr/nebulus-prime\n    role: platform-deployment\n    branch_model: develop-main\n    depends_on: [nebulus-core]\n\n  nebulus-edge:\n    path: ~/projects/west_ai_labs/nebulus-edge\n    remote: jlwestsr/nebulus-edge\n    role: platform-deployment\n    branch_model: develop-main\n    depends_on: [nebulus-core]\n\n  nebulus-atom:\n    path: ~/projects/west_ai_labs/nebulus-atom\n    remote: jlwestsr/nebulus-atom\n    role: tooling\n    branch_model: develop-main\n    depends_on: []\n\n  nebulus-gantry:\n    path: ~/projects/west_ai_labs/nebulus-gantry\n    remote: jlwestsr/nebulus-gantry\n    role: frontend\n    branch_model: develop-main\n    depends_on: [nebulus-prime]\n\n  nebulus-forge:\n    path: ~/projects/west_ai_labs/nebulus-forge\n    remote: jlwestsr/nebulus-forge\n    role: tooling\n    branch_model: develop-main\n    depends_on: [nebulus-core]\n</code></pre> <p>Fields:</p> Field Purpose <code>path</code> Local filesystem path <code>remote</code> GitHub <code>owner/repo</code> <code>role</code> Semantic role: <code>shared-library</code>, <code>platform-deployment</code>, <code>frontend</code>, <code>tooling</code> <code>branch_model</code> Git workflow: <code>develop-main</code>, <code>trunk-based</code>, <code>gitflow</code> <code>depends_on</code> Upstream dependencies \u2014 changes here may require downstream updates"},{"location":"plans/2026-02-05-overlord-design/#dependency-graph","title":"Dependency Graph","text":"<p>The <code>depends_on</code> relationships form a DAG that the Overlord uses to:</p> <ul> <li>Order releases: Core must release before Prime/Edge can consume the update</li> <li>Detect ripple effects: A breaking change in Core triggers checks in all dependents</li> <li>Coordinate multi-repo work: \"Update the LLM client\" spans Core + Prime + Edge</li> </ul> <pre><code>nebulus-core \u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u25b6 nebulus-prime \u2500\u2500\u25b6 nebulus-gantry\n                   \u251c\u2500\u2500\u25b6 nebulus-edge\n                   \u2514\u2500\u2500\u25b6 nebulus-forge\n\nnebulus-atom (independent)\n</code></pre>"},{"location":"plans/2026-02-05-overlord-design/#autonomy-engine","title":"Autonomy Engine","text":"<p>The Overlord supports three autonomy levels, configurable per-project or globally. The user can change levels at any time via CLI, Slack, or Gantry.</p> <pre><code># ~/.atom/overlord.yml\nautonomy:\n  global: proactive        # Default for all projects\n\n  overrides:               # Per-project overrides\n    nebulus-core: cautious  # Core is critical \u2014 always ask first\n    nebulus-gantry: scheduled  # Gantry can run nightly health checks\n</code></pre> Level Behavior Example <code>cautious</code> Command-only. Overlord observes and reports but never acts without an explicit command. \"Overlord, merge Core develop to main\" <code>proactive</code> Scans and proposes. Overlord identifies work, presents a plan, and waits for approval before executing. \"I found 3 stale branches in Prime. Want me to clean them up?\" <code>scheduled</code> Runs defined sweeps on a cron schedule. Executes pre-approved actions automatically, reports results. Escalates anything unexpected. Nightly: run tests, check for dependency updates, clean stale branches. Report at 7 AM. <p>Escalation rule: Regardless of autonomy level, the Overlord always escalates: - Destructive operations (force push, branch delete, data migration) - Cross-repo breaking changes - Failed automated actions - Anything outside the pre-approved action list</p>"},{"location":"plans/2026-02-05-overlord-design/#dispatch-engine","title":"Dispatch Engine","text":"<p>Evolves the current <code>DockerManager</code> + <code>LLMPool</code> into a three-tier routing system. The goal is to maximize local inference and minimize cloud token spend.</p> <p>Infrastructure: Dual-Machine Local Inference</p> <p>The ecosystem has two machines with local LLM capability:</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  shurtugal-lnx (Dev Machine)   \u2502     \u2502  Mac Mini M4 Pro (Desk)         \u2502\n\u2502                                 \u2502     \u2502                                 \u2502\n\u2502  NVIDIA GPU + TabbyAPI          \u2502     \u2502  48GB Unified RAM + MLX         \u2502\n\u2502  Qwen2.5-Coder-14B             \u2502     \u2502  qwen3-coder-30b (default)      \u2502\n\u2502  ExLlamaV2                      \u2502     \u2502  qwen2.5-coder-32b             \u2502\n\u2502  Port 5000                      \u2502     \u2502  llama3.1-8b                    \u2502\n\u2502  Concurrent: 2                  \u2502     \u2502  Port 8080                      \u2502\n\u2502                                 \u2502     \u2502  Managed by PM2                 \u2502\n\u2502  Best for: fast mechanical      \u2502     \u2502  Best for: complex coding,      \u2502\n\u2502  tasks while dev box is idle    \u2502     \u2502  feature impl, reviews          \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>The Mac Mini is currently idle \u2014 activating it gives us a 30B-parameter coding model for free. With 48GB unified RAM, it can serve qwen3-coder-30b at good speed on Apple Silicon with MLX. This is the primary workhorse for local inference.</p> <p>Three-Tier Model Router (simplified from Gemini review feedback):</p> <pre><code># ~/.atom/overlord.yml\nmodels:\n  # Tier 1: Local inference \u2014 FREE, handles 60-70% of work\n  local-prime:\n    host: localhost                    # Linux dev machine\n    endpoint: http://localhost:5000/v1\n    model: Qwen2.5-Coder-14B\n    tier: local\n    concurrent: 2\n    notes: TabbyAPI/ExLlamaV2 on NVIDIA GPU\n\n  local-edge:\n    host: nebulus               # Mac Mini M4 Pro on desk\n    endpoint: http://nebulus:8080/v1\n    model: qwen3-coder-30b\n    tier: local\n    concurrent: 2\n    notes: MLX on Apple Silicon, 48GB RAM \u2014 primary local workhorse\n\n  # Tier 2: Cloud fast \u2014 CHEAP, for quick decisions\n  cloud-fast:\n    endpoint: https://api.anthropic.com/v1\n    model: claude-haiku-4-5-20251001\n    tier: cloud-fast\n    concurrent: 10\n\n  # Tier 3: Cloud heavy \u2014 EXPENSIVE, for architecture + complex reasoning\n  cloud-heavy:\n    endpoint: https://api.anthropic.com/v1\n    model: claude-sonnet-4-5-20250929\n    tier: cloud-heavy\n    concurrent: 3\n</code></pre> <p>Task-to-Tier Mapping (user can override any assignment):</p> Task Type Default Tier Preferred Backend Rationale Git chores (merge, tag, push) None Direct execution No LLM needed Code formatting, linting fixes <code>local</code> Either local Mechanical, free Test writing, boilerplate <code>local</code> <code>local-edge</code> (30B) Better quality at 30B Feature implementation <code>local</code> <code>local-edge</code> (30B) 30B handles most features well PR review, code review <code>cloud-fast</code> Haiku Good speed + quality balance Complex debugging <code>cloud-fast</code> Haiku or Sonnet Depends on complexity Architecture design, planning <code>cloud-heavy</code> Sonnet Needs deep reasoning Cross-project coordination <code>cloud-heavy</code> Sonnet Needs ecosystem-wide context <p>Routing logic: 1. Check if local backends are healthy (hit <code>/health</code> endpoint) 2. Prefer <code>local-edge</code> (30B) over <code>local-prime</code> (14B) when both are available 3. Fall back to cloud tier only when local is unavailable or task requires it 4. User can force a tier via <code>--tier cloud-heavy</code> on any dispatch command</p>"},{"location":"plans/2026-02-05-overlord-design/#cross-project-memory","title":"Cross-Project Memory","text":"<p>A new memory layer that sits above per-project RAG. Stored in a dedicated ChromaDB collection (or SQLite + embeddings) at <code>~/.atom/overlord/memory/</code>.</p> <p>What gets remembered:</p> Category Examples Project patterns \"Core uses Google-style docstrings\", \"Gantry frontend uses Zustand stores\" User preferences \"User prefers feature branches off develop\", \"User wants approval before push\" Cross-project relations \"Prime installs Core as editable dependency\", \"Gantry's backend talks to Prime's TabbyAPI\" Historical decisions \"We chose ChromaDB over Pinecone because of local-only requirement\" Failure patterns \"Edge MLX tests fail if Xcode CLI tools aren't installed\" Release history \"Core v0.1.0 tagged 2026-02-03, v0.1.1 tagged 2026-02-05\" <p>Memory lifecycle:</p> <ol> <li>Capture: Overlord logs observations during every interaction</li> <li>Consolidate: Periodic \"sleep cycle\" extracts patterns from raw observations    (reuses Atom's existing Consolidator pattern)</li> <li>Recall: Before any action, Overlord queries memory for relevant context</li> <li>Prune: Stale or contradicted memories are marked and eventually removed</li> </ol>"},{"location":"plans/2026-02-05-overlord-design/#3-interface-adapter-layer","title":"3. Interface Adapter Layer","text":"<p>All three interfaces (CLI, Slack, Gantry) share the same Overlord Core. The Interface Adapter Layer normalizes input commands and renders output in the appropriate format.</p>"},{"location":"plans/2026-02-05-overlord-design/#31-cli-interface","title":"3.1 CLI Interface","text":"<p>For power users and scripting. Extends the existing <code>nebulus-atom</code> CLI.</p> <pre><code># Ecosystem overview\nnebulus-atom overlord status                    # Quick health of all projects\nnebulus-atom overlord status --detailed         # Full report with git state, tests, issues\n\n# Project management\nnebulus-atom overlord scan                      # Scan all repos, report findings\nnebulus-atom overlord scan nebulus-core          # Scan specific project\n\n# Dispatch\nnebulus-atom overlord dispatch \"merge Core develop to main and tag v0.2.0\"\nnebulus-atom overlord dispatch \"run tests across all projects\"\nnebulus-atom overlord dispatch \"clean up stale branches in Prime and Edge\"\n\n# Release coordination\nnebulus-atom overlord release nebulus-core v0.2.0   # Coordinated release with downstream checks\n\n# Autonomy control\nnebulus-atom overlord autonomy                      # Show current levels\nnebulus-atom overlord autonomy --global scheduled    # Change global level\nnebulus-atom overlord autonomy --project core cautious  # Override for specific project\n\n# Memory\nnebulus-atom overlord memory search \"ChromaDB configuration\"\nnebulus-atom overlord memory forget \"outdated pattern about X\"\n\n# Configuration\nnebulus-atom overlord config                    # Show current config\nnebulus-atom overlord projects add ~/new-repo   # Register a new project\nnebulus-atom overlord projects remove old-repo  # Unregister a project\n\n# Auto-discovery (scans workspace, generates starter YAML)\nnebulus-atom overlord discover ~/projects/west_ai_labs\nnebulus-atom overlord discover --dry-run        # Preview without writing\n</code></pre>"},{"location":"plans/2026-02-05-overlord-design/#32-slack-interface","title":"3.2 Slack Interface","text":"<p>For async monitoring and quick commands. Evolves the existing <code>slack_bot.py</code>.</p> <pre><code>@atom status                          \u2192 Ecosystem health summary\n@atom status core                     \u2192 Specific project status\n@atom merge core develop to main      \u2192 Dispatch merge task\n@atom approve                         \u2192 Approve pending proposal\n@atom deny                            \u2192 Deny pending proposal\n@atom autonomy proactive              \u2192 Change autonomy level\n@atom pause                           \u2192 Pause all scheduled tasks\n@atom resume                          \u2192 Resume scheduled tasks\n</code></pre> <p>Proactive notifications (when autonomy allows):</p> <pre><code>\ud83d\udd14 Overlord: Found 3 commits on Core develop not yet on main.\n   Merge and tag v0.1.1?\n   [Approve] [Deny] [Details]\n\n\ud83d\udd14 Overlord: Nightly test sweep complete.\n   \u2705 Core: 142 passed\n   \u2705 Atom: 826 passed\n   \u26a0\ufe0f  Prime: 2 warnings (deprecation)\n   \u274c Edge: 1 failure (test_mlx_connection)\n   [View Details] [Dispatch Fix]\n</code></pre>"},{"location":"plans/2026-02-05-overlord-design/#33-gantry-ui-module-based","title":"3.3 Gantry UI (Module-Based)","text":"<p>The Overlord's visual control plane is delivered as a Gantry Module \u2014 an installable package that plugs into Gantry's module system rather than being hardcoded into Gantry's codebase. This keeps Gantry's core clean and establishes the pattern for future modules.</p> <p>See: <code>nebulus-gantry/docs/plans/2026-02-05-gantry-module-system.md</code> for the full module architecture specification.</p> <p>How it works:</p> <ol> <li> <p>Atom declares a Gantry module via Python entry point:    <pre><code>[project.entry-points.\"gantry.modules\"]\noverlord = \"nebulus_atom.gantry:module_manifest\"\n</code></pre></p> </li> <li> <p>Gantry discovers the module at startup, registers its API routes, sidebar    navigation, and admin tabs</p> </li> <li> <p>The Overlord module provides its own frontend bundle (React components) that    Gantry loads dynamically</p> </li> </ol> <p>Navigation: The Overlord gets its own top-level sidebar entry \u2014 not buried inside the Admin panel. When a user clicks \"Overlord\" in the sidebar, they enter the Overlord's own page with its own tab navigation. This gives the ecosystem dashboard first-class visibility alongside Chat, Settings, and Admin.</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Sidebar          \u2502  Main Content            \u2502\n\u2502                   \u2502                          \u2502\n\u2502  \ud83d\udcac Chat          \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n\u2502  \ud83d\udd27 Settings      \u2502  \u2502 Ecosystem \u2502 Dispatch \u2502 \u2502\n\u2502  \ud83d\udee1\ufe0f Admin         \u2502  \u2502 Memory  \u2502 Audit     \u2502 \u2502\n\u2502  \u26a1 Overlord  \u25c4\u2500\u2500 \u2502  \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502\n\u2502                   \u2502  \u2502                     \u2502 \u2502\n\u2502                   \u2502  \u2502  [Active Tab View]  \u2502 \u2502\n\u2502                   \u2502  \u2502                     \u2502 \u2502\n\u2502                   \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Module views:</p> View Content Ecosystem Dashboard All projects with git state, test status, last activity. Dependency graph visualization. Project Detail Branches, recent commits, open issues, test results, agent activity history Dispatch Console Natural language task input, live agent progress, output review Memory Browser Search and manage Overlord's cross-project memory Audit Log Filterable timeline of all Overlord actions with hash chain verification Autonomy Settings Visual controls for autonomy levels per project <p>Atom package structure for the Gantry module:</p> <pre><code>nebulus_atom/\n\u2514\u2500\u2500 gantry/\n    \u251c\u2500\u2500 __init__.py          # module_manifest definition\n    \u251c\u2500\u2500 router.py            # FastAPI router (API endpoints)\n    \u251c\u2500\u2500 schemas.py           # Pydantic request/response models\n    \u251c\u2500\u2500 services.py          # Service layer (calls Overlord Core)\n    \u2514\u2500\u2500 frontend/\n        \u251c\u2500\u2500 package.json     # Standalone React build\n        \u251c\u2500\u2500 src/\n        \u2502   \u251c\u2500\u2500 OverlordPage.tsx         # Top-level page (owns tab navigation)\n        \u2502   \u251c\u2500\u2500 EcosystemDashboard.tsx\n        \u2502   \u251c\u2500\u2500 ProjectDetail.tsx\n        \u2502   \u251c\u2500\u2500 DispatchConsole.tsx\n        \u2502   \u251c\u2500\u2500 MemoryBrowser.tsx\n        \u2502   \u251c\u2500\u2500 AuditLog.tsx\n        \u2502   \u2514\u2500\u2500 AutonomySettings.tsx\n        \u2514\u2500\u2500 dist/            # Built bundle served by Gantry\n</code></pre>"},{"location":"plans/2026-02-05-overlord-design/#4-operational-modes","title":"4. Operational Modes","text":""},{"location":"plans/2026-02-05-overlord-design/#41-interactive-mode","title":"4.1 Interactive Mode","text":"<p>The user is actively working and issues commands. This is today's workflow \u2014 what happened in this conversation when deploying agents to merge and tag the three repos.</p> <p>Flow:</p> <pre><code>User: \"Let's check on the projects\"\nOverlord: [scans all repos, reports status]\nUser: \"Merge Core and Prime, tag Edge\"\nOverlord: [dispatches agents, monitors, reports results]\nUser: \"Push all three\"\nOverlord: [executes pushes, confirms]\n</code></pre>"},{"location":"plans/2026-02-05-overlord-design/#42-background-mode","title":"4.2 Background Mode","text":"<p>The Overlord runs as a daemon, monitoring repos and executing scheduled tasks. Reports via Slack or accumulates a summary for the next interactive session.</p> <p>Scheduled actions (configurable):</p> Schedule Action Autonomy Required Hourly Health check (git status, service endpoints) <code>scheduled</code> Nightly Run test suites across all projects <code>scheduled</code> Nightly Check for stale branches (&gt; 7 days inactive) <code>proactive</code> (proposes cleanup) Weekly Dependency drift check (outdated packages) <code>proactive</code> (proposes updates) On push Run CI-equivalent validation <code>scheduled</code>"},{"location":"plans/2026-02-05-overlord-design/#43-coordinated-release-mode","title":"4.3 Coordinated Release Mode","text":"<p>A special workflow for releasing changes that span multiple projects.</p> <p>Example: Core v0.2.0 release</p> <pre><code>1. Overlord checks: all Core tests pass\n2. Overlord checks: develop is clean and ahead of main\n3. Overlord merges develop \u2192 main, tags v0.2.0\n4. Overlord checks dependents: Prime, Edge, Forge all depend on Core\n5. Overlord updates Core version in Prime's requirements \u2192 runs tests\n6. Overlord updates Core version in Edge's requirements \u2192 runs tests\n7. If all pass: proposes \"Release Core v0.2.0 and update all dependents?\"\n8. User approves \u2192 Overlord pushes all changes\n9. Overlord logs the coordinated release in memory + audit trail\n</code></pre>"},{"location":"plans/2026-02-05-overlord-design/#5-trust-boundary-safety","title":"5. Trust Boundary &amp; Safety","text":"<p>The existing trust boundary from Atom V2 is preserved and extended.</p>"},{"location":"plans/2026-02-05-overlord-design/#51-permission-model","title":"5.1 Permission Model","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502              ALWAYS ALLOWED                  \u2502\n\u2502                                              \u2502\n\u2502  \u2022 Read any managed repo (git status, log)   \u2502\n\u2502  \u2022 Run tests (read-only validation)          \u2502\n\u2502  \u2022 Scan for issues, branches, dependencies   \u2502\n\u2502  \u2022 Query memory                              \u2502\n\u2502  \u2022 Generate reports and summaries            \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502         REQUIRES APPROVAL (unless            \u2502\n\u2502         pre-approved via autonomy config)     \u2502\n\u2502                                              \u2502\n\u2502  \u2022 Git merge / tag                           \u2502\n\u2502  \u2022 Git push to remote                        \u2502\n\u2502  \u2022 Create / close GitHub issues              \u2502\n\u2502  \u2022 Create / merge pull requests              \u2502\n\u2502  \u2022 Modify files in any project               \u2502\n\u2502  \u2022 Install or update dependencies            \u2502\n\u2502  \u2022 Spawn Minion workers                      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502            ALWAYS REQUIRES APPROVAL          \u2502\n\u2502        (cannot be pre-approved)              \u2502\n\u2502                                              \u2502\n\u2502  \u2022 Force push                                \u2502\n\u2502  \u2022 Delete branches on remote                 \u2502\n\u2502  \u2022 Modify Overlord's own configuration       \u2502\n\u2502  \u2022 Change autonomy levels                    \u2502\n\u2502  \u2022 Modify trust boundary rules               \u2502\n\u2502  \u2022 Access secrets or credentials             \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"plans/2026-02-05-overlord-design/#52-blast-radius-control","title":"5.2 Blast Radius Control","text":"<p>Every dispatched action carries a scope declaration:</p> <pre><code>@dataclass\nclass ActionScope:\n    projects: list[str]           # Which projects are affected\n    branches: list[str]           # Which branches are touched\n    destructive: bool             # Can this lose data?\n    reversible: bool              # Can this be undone?\n    affects_remote: bool          # Does this touch GitHub?\n    estimated_impact: str         # \"low\", \"medium\", \"high\"\n</code></pre> <p>The Overlord evaluates scope before execution and escalates if the blast radius exceeds the current autonomy level's threshold.</p>"},{"location":"plans/2026-02-05-overlord-design/#6-implementation-phases","title":"6. Implementation Phases","text":""},{"location":"plans/2026-02-05-overlord-design/#phase-1-foundation-overlord-core-cli","title":"Phase 1: Foundation (Overlord Core + CLI)","text":"<p>Goal: Replace the current Swarm Overlord with the multi-project-aware version. CLI interface only. Cautious autonomy only.</p> <p>Deliverables:</p> <ul> <li>[x] Project Registry (<code>~/.atom/overlord.yml</code> parsing + validation)</li> <li>[x] Auto-discovery command (<code>overlord discover</code> \u2014 scans workspace for repos,       generates starter YAML; explicit YAML config stays as source of truth)</li> <li>[x] Ecosystem Scanner (git state, test health, branch analysis across all repos)</li> <li>[x] CLI commands: <code>overlord status</code>, <code>overlord scan</code>, <code>overlord config</code>, <code>overlord discover</code></li> <li>[x] Dependency graph construction and traversal</li> <li>[x] Action scope model and blast radius evaluation</li> <li>[x] Cross-project memory store (SQLite-backed, text search)</li> <li>[x] CLI commands: <code>overlord graph</code>, <code>overlord memory</code>, <code>overlord scope</code></li> <li>[ ] Migration path from existing <code>nebulus_swarm/overlord/</code></li> </ul> <p>Tests: 110 tests \u2014 registry (18), scanner (14), graph (21), action scope (20), memory (17), CLI (19). All passing.</p>"},{"location":"plans/2026-02-05-overlord-design/#phase-2-dispatch-autonomy","title":"Phase 2: Dispatch + Autonomy","text":"<p>Goal: Dispatch tasks to workers across multiple repos. Configurable autonomy.</p> <p>Deliverables:</p> <ul> <li>[ ] Autonomy Engine (three levels, per-project overrides, runtime switching)</li> <li>[ ] Multi-repo Dispatch Engine (evolve DockerManager for cross-repo tasks)</li> <li>[ ] Model Router (task \u2192 model mapping based on complexity + cost)</li> <li>[ ] CLI commands: <code>overlord dispatch</code>, <code>overlord autonomy</code>, <code>overlord release</code></li> <li>[ ] Coordinated release workflow</li> <li>[ ] Pre-approved action lists for scheduled autonomy</li> </ul> <p>Tests: Dispatch routing tests, autonomy level enforcement, release coordination.</p>"},{"location":"plans/2026-02-05-overlord-design/#phase-3-slack-background-mode","title":"Phase 3: Slack + Background Mode","text":"<p>Goal: Async interface and daemon mode for continuous monitoring.</p> <p>Deliverables:</p> <ul> <li>[ ] Slack Bot upgrade (multi-project commands, approval buttons, notifications)</li> <li>[ ] Background daemon mode (scheduled sweeps, health checks)</li> <li>[ ] Proactive proposal system (detect \u2192 propose \u2192 await approval \u2192 execute)</li> <li>[ ] Notification routing (Slack for urgent, accumulate for non-urgent)</li> <li>[ ] Cron-style schedule configuration</li> </ul> <p>Tests: Slack command parsing, schedule execution, notification routing.</p>"},{"location":"plans/2026-02-05-overlord-design/#phase-4-gantry-integration","title":"Phase 4: Gantry Integration","text":"<p>Goal: Visual control plane in Gantry's admin UI.</p> <p>Deliverables:</p> <ul> <li>[ ] Gantry backend: Overlord API endpoints (status, dispatch, memory, audit)</li> <li>[ ] Gantry frontend: Dashboard, project detail, dispatch console</li> <li>[ ] Dependency graph visualization (interactive)</li> <li>[ ] Memory browser</li> <li>[ ] Audit log viewer with hash chain verification</li> <li>[ ] Autonomy settings UI</li> </ul> <p>Tests: API endpoint tests, UI component tests.</p>"},{"location":"plans/2026-02-05-overlord-design/#phase-5-observability-reporting","title":"Phase 5: Observability + Reporting","text":"<p>Goal: The Overlord tracks its own performance and presents data for human decision-making. (Downgraded from \"Meta-Evaluation\" per Gemini review \u2014 automated self-correction is low ROI at this scale. The human adjusts based on data.)</p> <p>Deliverables:</p> <ul> <li>[ ] Dispatch outcome tracking (task \u2192 model \u2192 result \u2192 duration)</li> <li>[ ] Performance dashboards (which models succeed at which task types)</li> <li>[ ] Cross-project skill discovery: surface skills from one repo to another</li> <li>[ ] Memory consolidation \"sleep cycle\" (batch pattern extraction)</li> <li>[ ] Weekly/monthly summary reports (what was dispatched, what succeeded/failed)</li> </ul> <p>Tests: Outcome tracking accuracy, report generation.</p>"},{"location":"plans/2026-02-05-overlord-design/#7-configuration-reference","title":"7. Configuration Reference","text":"<pre><code># ~/.atom/overlord.yml \u2014 Full configuration reference\n\n# \u2500\u2500\u2500 Project Registry \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nprojects:\n  nebulus-core:\n    path: ~/projects/west_ai_labs/nebulus-core\n    remote: jlwestsr/nebulus-core\n    role: shared-library\n    branch_model: develop-main\n    depends_on: []\n    test_command: pytest\n    validate_command: pre-commit run --all-files\n\n# \u2500\u2500\u2500 Autonomy \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nautonomy:\n  global: proactive\n  overrides:\n    nebulus-core: cautious\n\n# \u2500\u2500\u2500 Models (Three-Tier: local \u2192 cloud-fast \u2192 cloud-heavy) \u2500\u2500\nmodels:\n  local-prime:\n    host: localhost\n    endpoint: http://localhost:5000/v1\n    model: Qwen2.5-Coder-14B\n    tier: local\n    concurrent: 2\n    notes: TabbyAPI/ExLlamaV2 on NVIDIA GPU (dev machine)\n  local-edge:\n    host: nebulus\n    endpoint: http://nebulus:8080/v1\n    model: qwen3-coder-30b\n    tier: local\n    concurrent: 2\n    notes: MLX on Apple Silicon, 48GB RAM (primary local workhorse)\n  cloud-fast:\n    endpoint: https://api.anthropic.com/v1\n    model: claude-haiku-4-5-20251001\n    tier: cloud-fast\n    concurrent: 10\n  cloud-heavy:\n    endpoint: https://api.anthropic.com/v1\n    model: claude-sonnet-4-5-20250929\n    tier: cloud-heavy\n    concurrent: 3\n\n# \u2500\u2500\u2500 Scheduling (for 'scheduled' autonomy) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nschedule:\n  health_check:\n    cron: \"0 * * * *\"          # Hourly\n    action: scan\n  nightly_tests:\n    cron: \"0 2 * * *\"          # 2 AM daily\n    action: test-all\n  stale_branch_sweep:\n    cron: \"0 3 * * 0\"          # 3 AM Sunday\n    action: clean-stale-branches\n    threshold_days: 7\n\n# \u2500\u2500\u2500 Notifications \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nnotifications:\n  slack:\n    enabled: true\n    channel: \"#nebulus-ops\"\n    urgent: true               # Send immediately for failures\n  summary:\n    enabled: true\n    schedule: \"0 7 * * *\"      # 7 AM daily digest\n\n# \u2500\u2500\u2500 Memory \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nmemory:\n  store: ~/.atom/overlord/memory/\n  consolidation_schedule: \"0 4 * * *\"   # 4 AM daily\n  max_raw_observations: 10000\n  embedding_model: sentence-transformers/all-MiniLM-L6-v2\n\n# \u2500\u2500\u2500 Audit \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\naudit:\n  store: ~/.atom/overlord/audit.db\n  signing_key: ~/.atom/overlord/signing.key   # Optional Ed25519\n  retention_days: 365\n</code></pre>"},{"location":"plans/2026-02-05-overlord-design/#8-relationship-to-existing-codebase","title":"8. Relationship to Existing Codebase","text":"Existing Module Disposition Notes <code>nebulus_swarm/overlord/main.py</code> Evolve Becomes the Overlord Core entry point <code>nebulus_swarm/overlord/state.py</code> Evolve Extends to multi-project state <code>nebulus_swarm/overlord/docker_manager.py</code> Keep Minion container management stays <code>nebulus_swarm/overlord/slack_bot.py</code> Evolve Multi-project commands, approval buttons <code>nebulus_swarm/overlord/github_queue.py</code> Evolve Scans multiple repos <code>nebulus_swarm/overlord/llm_pool.py</code> Evolve Becomes Model Router with multi-backend <code>nebulus_swarm/overlord/evaluator.py</code> Keep Worker evaluation logic stays <code>nebulus_swarm/overlord/proposals.py</code> Keep Enhancement proposal system stays <code>nebulus_swarm/overlord/scope.py</code> Evolve Adds ActionScope + blast radius <code>nebulus_swarm/overlord/skill_evolution.py</code> Keep Skill lifecycle stays <code>nebulus_swarm/overlord/audit_trail.py</code> Keep Audit system stays, gains more event types <code>nebulus_swarm/overlord/auditor.py</code> Keep Compliance auditor stays <code>nebulus_swarm/minion/</code> Keep Worker agents unchanged <code>nebulus_atom/services/rag_service.py</code> Extend Cross-project memory layer on top <code>nebulus_atom/services/telemetry_service.py</code> Keep Standalone telemetry unchanged"},{"location":"plans/2026-02-05-overlord-design/#9-success-criteria","title":"9. Success Criteria","text":"<p>The Overlord is successful when:</p> <ol> <li>\"What's the state of everything?\" gets an accurate, instant answer</li> <li>Routine chores (merges, tags, pushes, branch cleanup) happen without manual    git commands</li> <li>Cross-repo releases are coordinated automatically with dependency awareness</li> <li>The user's preferences are remembered and applied consistently</li> <li>Nothing breaks silently \u2014 every action is audited, every failure is escalated</li> <li>The trust boundary holds \u2014 the Overlord never exceeds its granted autonomy</li> </ol>"},{"location":"plans/2026-02-05-overlord-design/#10-resolved-decisions-gemini-architecture-review","title":"10. Resolved Decisions (Gemini Architecture Review)","text":"<p>These decisions were reached through a cross-AI architecture review on 2026-02-05 (Claude as lead architect, Gemini as reviewer).</p> # Topic Resolution 1 Dependency isolation Same-process modules for now (YAGNI). Document constraint in <code>gantry-sdk</code>. If a real conflict arises between modules, introduce a sidecar pattern where the module runs its own FastAPI service and Gantry proxies to it. 2 CSS collisions Non-issue with Tailwind utility classes. SDK recommends prefixed custom classes as a safety convention. No Shadow DOM needed. 3 Audit hash chains Keep existing (shipped in Atom V2, zero maintenance cost). Don't expand \u2014 no new audit features beyond what exists. 4 Meta-evaluation Downgraded to \"Observability + Reporting\" (Phase 5). Overlord tracks dispatch outcomes and presents data. Human decides. Automated self-correction is low ROI at this scale. 5 Model router Three tiers: <code>local</code>, <code>cloud-fast</code>, <code>cloud-heavy</code>. Simple task-type mapping with user override. No dynamic cost optimization. Two local machines provide free inference for 60-70% of work. 6 Phase ordering Keep current order. Parallelize Gantry module system + Overlord work streams (different repos, independent). CLI is useful from day one. 7 Auto-discovery <code>overlord discover</code> command scans workspace root for repos and generates starter YAML. Explicit <code>overlord.yml</code> config stays as source of truth for metadata (<code>role</code>, <code>depends_on</code>, <code>branch_model</code>). 8 Bundle format (Gantry) ESM (Standard ES Modules). IIFE is a pre-Vite relic. 9 Shared state (Gantry) API-only for data. Typed <code>EventBus</code> via <code>gantry-sdk</code> for UI affordances (toasts, confirmations). No direct Zustand store access."},{"location":"plans/2026-02-05-overlord-design/#11-open-questions","title":"11. Open Questions","text":"<ul> <li>Minion evolution: Should Minions also become multi-project-aware, or should   the Overlord always decompose cross-repo tasks into single-repo Minion tasks?</li> <li>Remote Overlord: Should the Overlord be able to run on a remote server   (e.g., the Prime box or Mac Mini) and manage repos via SSH/API, or always run   locally? (Note: with dual-machine inference, the Mac Mini is already a remote   resource \u2014 extending this to remote Overlord execution is a natural next step.)</li> <li>Multi-user: Is this always single-user (the owner), or could multiple team   members interact with the same Overlord instance?</li> </ul>"},{"location":"plans/2026-02-05-v2-phase2-implementation/","title":"V2 Phase 2 Implementation Plan","text":"<p>For Claude: REQUIRED SUB-SKILL: Use superpowers:executing-plans to implement this plan task-by-task.</p> <p>Goal: Add supervisor evaluation, worker scope enforcement, and enhancement proposals to the Overlord/Minion system.</p> <p>Architecture: Extend the existing Overlord with three new modules (evaluator, scope, proposals) and modify the Minion's ToolExecutor to enforce file-level write scope. All modules are tested independently with mocked dependencies.</p> <p>Tech Stack: Python 3.12, SQLite, dataclasses, pytest, existing reviewer module (CheckRunner, LLMReviewer)</p>"},{"location":"plans/2026-02-05-v2-phase2-implementation/#task-1-evaluator-data-models","title":"Task 1: Evaluator Data Models","text":"<p>Files: - Create: <code>nebulus_swarm/overlord/evaluator.py</code> - Create: <code>tests/test_evaluator.py</code></p> <p>Step 1: Write failing tests for data models</p> <pre><code># tests/test_evaluator.py\n\"\"\"Tests for the supervisor evaluation layer.\"\"\"\n\nfrom nebulus_swarm.overlord.evaluator import (\n    CheckScore,\n    EvaluationResult,\n    RevisionRequest,\n)\n\n\nclass TestCheckScore:\n    def test_pass_value(self):\n        assert CheckScore.PASS.value == \"pass\"\n\n    def test_fail_value(self):\n        assert CheckScore.FAIL.value == \"fail\"\n\n    def test_needs_revision_value(self):\n        assert CheckScore.NEEDS_REVISION.value == \"needs_revision\"\n\n\nclass TestEvaluationResult:\n    def test_overall_pass_when_all_pass(self):\n        result = EvaluationResult(\n            pr_number=1,\n            repo=\"owner/repo\",\n            test_score=CheckScore.PASS,\n            lint_score=CheckScore.PASS,\n            review_score=CheckScore.PASS,\n        )\n        assert result.overall == CheckScore.PASS\n\n    def test_overall_needs_revision_when_any_needs_revision(self):\n        result = EvaluationResult(\n            pr_number=1,\n            repo=\"owner/repo\",\n            test_score=CheckScore.PASS,\n            lint_score=CheckScore.NEEDS_REVISION,\n            review_score=CheckScore.PASS,\n        )\n        assert result.overall == CheckScore.NEEDS_REVISION\n\n    def test_overall_fail_when_any_fail(self):\n        result = EvaluationResult(\n            pr_number=1,\n            repo=\"owner/repo\",\n            test_score=CheckScore.FAIL,\n            lint_score=CheckScore.PASS,\n            review_score=CheckScore.NEEDS_REVISION,\n        )\n        assert result.overall == CheckScore.FAIL\n\n    def test_fail_beats_needs_revision(self):\n        result = EvaluationResult(\n            pr_number=1,\n            repo=\"owner/repo\",\n            test_score=CheckScore.NEEDS_REVISION,\n            lint_score=CheckScore.FAIL,\n            review_score=CheckScore.NEEDS_REVISION,\n        )\n        assert result.overall == CheckScore.FAIL\n\n    def test_default_revision_number_is_zero(self):\n        result = EvaluationResult(\n            pr_number=1,\n            repo=\"owner/repo\",\n            test_score=CheckScore.PASS,\n            lint_score=CheckScore.PASS,\n            review_score=CheckScore.PASS,\n        )\n        assert result.revision_number == 0\n\n    def test_feedback_aggregation(self):\n        result = EvaluationResult(\n            pr_number=1,\n            repo=\"owner/repo\",\n            test_score=CheckScore.FAIL,\n            lint_score=CheckScore.PASS,\n            review_score=CheckScore.PASS,\n            test_feedback=\"3 tests failed\",\n            lint_feedback=\"\",\n            review_feedback=\"\",\n        )\n        combined = result.combined_feedback\n        assert \"3 tests failed\" in combined\n\n\nclass TestRevisionRequest:\n    def test_has_required_fields(self):\n        req = RevisionRequest(\n            repo=\"owner/repo\",\n            pr_number=42,\n            issue_number=10,\n            branch=\"minion/issue-10\",\n            feedback=\"Tests failed: test_foo, test_bar\",\n            revision_number=1,\n        )\n        assert req.repo == \"owner/repo\"\n        assert req.revision_number == 1\n</code></pre> <p>Step 2: Run tests to verify they fail</p> <p>Run: <code>venv/bin/python -m pytest tests/test_evaluator.py -q</code> Expected: ImportError \u2014 <code>evaluator</code> module doesn't exist yet.</p> <p>Step 3: Implement data models</p> <pre><code># nebulus_swarm/overlord/evaluator.py\n\"\"\"Supervisor evaluation layer for Minion output.\"\"\"\n\nimport logging\nfrom dataclasses import dataclass, field\nfrom datetime import datetime\nfrom enum import Enum\nfrom typing import Optional\n\nlogger = logging.getLogger(__name__)\n\nMAX_REVISIONS = 2\n\n\nclass CheckScore(Enum):\n    \"\"\"Score for a single evaluation check.\"\"\"\n\n    PASS = \"pass\"\n    FAIL = \"fail\"\n    NEEDS_REVISION = \"needs_revision\"\n\n\n@dataclass\nclass EvaluationResult:\n    \"\"\"Result of evaluating a Minion's work.\"\"\"\n\n    pr_number: int\n    repo: str\n    test_score: CheckScore\n    lint_score: CheckScore\n    review_score: CheckScore\n    revision_number: int = 0\n    test_feedback: str = \"\"\n    lint_feedback: str = \"\"\n    review_feedback: str = \"\"\n    timestamp: datetime = field(default_factory=datetime.now)\n\n    @property\n    def overall(self) -&gt; CheckScore:\n        \"\"\"Compute overall score from individual checks.\"\"\"\n        scores = [self.test_score, self.lint_score, self.review_score]\n        if any(s == CheckScore.FAIL for s in scores):\n            return CheckScore.FAIL\n        if any(s == CheckScore.NEEDS_REVISION for s in scores):\n            return CheckScore.NEEDS_REVISION\n        return CheckScore.PASS\n\n    @property\n    def combined_feedback(self) -&gt; str:\n        \"\"\"Combine all feedback into a single string.\"\"\"\n        parts = []\n        if self.test_feedback:\n            parts.append(f\"Tests: {self.test_feedback}\")\n        if self.lint_feedback:\n            parts.append(f\"Lint: {self.lint_feedback}\")\n        if self.review_feedback:\n            parts.append(f\"Review: {self.review_feedback}\")\n        return \"\\n\".join(parts)\n\n\n@dataclass\nclass RevisionRequest:\n    \"\"\"Request for a Minion to revise its work.\"\"\"\n\n    repo: str\n    pr_number: int\n    issue_number: int\n    branch: str\n    feedback: str\n    revision_number: int\n</code></pre> <p>Step 4: Run tests to verify they pass</p> <p>Run: <code>venv/bin/python -m pytest tests/test_evaluator.py -q</code> Expected: All pass.</p> <p>Step 5: Commit</p> <pre><code>git add nebulus_swarm/overlord/evaluator.py tests/test_evaluator.py\ngit commit -m \"feat: add evaluator data models (CheckScore, EvaluationResult, RevisionRequest)\"\n</code></pre>"},{"location":"plans/2026-02-05-v2-phase2-implementation/#task-2-evaluator-core-logic","title":"Task 2: Evaluator Core Logic","text":"<p>Files: - Modify: <code>nebulus_swarm/overlord/evaluator.py</code> - Modify: <code>tests/test_evaluator.py</code></p> <p>Step 1: Write failing tests for Evaluator class</p> <p>Add to <code>tests/test_evaluator.py</code>:</p> <pre><code>from unittest.mock import MagicMock, patch\n\nfrom nebulus_swarm.overlord.evaluator import (\n    CheckScore,\n    Evaluator,\n    EvaluationResult,\n    MAX_REVISIONS,\n)\nfrom nebulus_swarm.reviewer.checks import CheckResult, ChecksReport, CheckStatus\nfrom nebulus_swarm.reviewer.pr_reviewer import ReviewDecision, ReviewResult\n\n\nclass TestEvaluator:\n    def _make_evaluator(self):\n        return Evaluator(\n            llm_base_url=\"http://localhost:5000/v1\",\n            llm_model=\"test-model\",\n            github_token=\"ghp_test\",\n        )\n\n    def test_all_pass(self):\n        ev = self._make_evaluator()\n        checks = ChecksReport(results=[\n            CheckResult(name=\"pytest\", status=CheckStatus.PASSED, message=\"10 passed\"),\n            CheckResult(name=\"ruff\", status=CheckStatus.PASSED, message=\"clean\"),\n        ])\n        review = ReviewResult(\n            decision=ReviewDecision.APPROVE,\n            summary=\"Looks good\",\n            confidence=0.9,\n        )\n        result = ev._score(checks, review, repo=\"o/r\", pr_number=1)\n        assert result.overall == CheckScore.PASS\n\n    def test_test_failure_means_needs_revision(self):\n        ev = self._make_evaluator()\n        checks = ChecksReport(results=[\n            CheckResult(name=\"pytest\", status=CheckStatus.FAILED, message=\"2 failed\"),\n            CheckResult(name=\"ruff\", status=CheckStatus.PASSED, message=\"clean\"),\n        ])\n        review = ReviewResult(\n            decision=ReviewDecision.APPROVE,\n            summary=\"Looks good\",\n            confidence=0.9,\n        )\n        result = ev._score(checks, review, repo=\"o/r\", pr_number=1)\n        assert result.test_score == CheckScore.NEEDS_REVISION\n        assert \"2 failed\" in result.test_feedback\n\n    def test_request_changes_means_needs_revision(self):\n        ev = self._make_evaluator()\n        checks = ChecksReport(results=[\n            CheckResult(name=\"pytest\", status=CheckStatus.PASSED, message=\"ok\"),\n        ])\n        review = ReviewResult(\n            decision=ReviewDecision.REQUEST_CHANGES,\n            summary=\"Has bugs\",\n            confidence=0.8,\n            issues=[\"Off by one error\"],\n        )\n        result = ev._score(checks, review, repo=\"o/r\", pr_number=1)\n        assert result.review_score == CheckScore.NEEDS_REVISION\n\n    def test_low_confidence_review_means_pass(self):\n        ev = self._make_evaluator()\n        checks = ChecksReport(results=[])\n        review = ReviewResult(\n            decision=ReviewDecision.COMMENT,\n            summary=\"Minor suggestions\",\n            confidence=0.6,\n        )\n        result = ev._score(checks, review, repo=\"o/r\", pr_number=1)\n        assert result.review_score == CheckScore.PASS\n\n    def test_can_revise_under_max(self):\n        ev = self._make_evaluator()\n        assert ev.can_revise(revision_number=0) is True\n        assert ev.can_revise(revision_number=1) is True\n\n    def test_cannot_revise_at_max(self):\n        ev = self._make_evaluator()\n        assert ev.can_revise(revision_number=MAX_REVISIONS) is False\n</code></pre> <p>Step 2: Run to verify failure</p> <p>Run: <code>venv/bin/python -m pytest tests/test_evaluator.py::TestEvaluator -q</code> Expected: ImportError \u2014 <code>Evaluator</code> class not defined yet.</p> <p>Step 3: Implement Evaluator class</p> <p>Add to <code>nebulus_swarm/overlord/evaluator.py</code>:</p> <pre><code>from nebulus_swarm.reviewer.checks import CheckRunner, ChecksReport, CheckStatus\nfrom nebulus_swarm.reviewer.llm_review import LLMReviewer\nfrom nebulus_swarm.reviewer.pr_reviewer import PRReviewer, ReviewDecision, ReviewResult\n\n\nclass Evaluator:\n    \"\"\"Evaluates Minion output after task completion.\"\"\"\n\n    def __init__(\n        self,\n        llm_base_url: str,\n        llm_model: str,\n        github_token: str,\n        llm_api_key: str = \"not-needed\",\n        llm_timeout: int = 120,\n    ):\n        self.llm_base_url = llm_base_url\n        self.llm_model = llm_model\n        self.github_token = github_token\n        self.llm_api_key = llm_api_key\n        self.llm_timeout = llm_timeout\n        self._llm_reviewer: Optional[LLMReviewer] = None\n        self._pr_reviewer: Optional[PRReviewer] = None\n\n    @property\n    def llm_reviewer(self) -&gt; LLMReviewer:\n        if self._llm_reviewer is None:\n            self._llm_reviewer = LLMReviewer(\n                base_url=self.llm_base_url,\n                model=self.llm_model,\n                api_key=self.llm_api_key,\n                timeout=self.llm_timeout,\n            )\n        return self._llm_reviewer\n\n    @property\n    def pr_reviewer(self) -&gt; PRReviewer:\n        if self._pr_reviewer is None:\n            self._pr_reviewer = PRReviewer(self.github_token)\n        return self._pr_reviewer\n\n    def can_revise(self, revision_number: int) -&gt; bool:\n        \"\"\"Check if another revision is allowed.\"\"\"\n        return revision_number &lt; MAX_REVISIONS\n\n    def _score(\n        self,\n        checks: ChecksReport,\n        review: ReviewResult,\n        repo: str,\n        pr_number: int,\n        revision_number: int = 0,\n    ) -&gt; EvaluationResult:\n        \"\"\"Score check results and LLM review into an EvaluationResult.\"\"\"\n        # Tests: any pytest failure -&gt; NEEDS_REVISION\n        test_score = CheckScore.PASS\n        test_feedback = \"\"\n        for r in checks.results:\n            if r.name.lower() in (\"pytest\", \"tests\") and r.status == CheckStatus.FAILED:\n                test_score = CheckScore.NEEDS_REVISION\n                test_feedback = r.message\n                break\n\n        # Lint: any lint failure -&gt; NEEDS_REVISION\n        lint_score = CheckScore.PASS\n        lint_feedback = \"\"\n        for r in checks.results:\n            if r.name.lower() in (\"ruff\", \"lint\", \"flake8\") and r.status == CheckStatus.FAILED:\n                lint_score = CheckScore.NEEDS_REVISION\n                lint_feedback = r.message\n                break\n\n        # Review: REQUEST_CHANGES -&gt; NEEDS_REVISION, APPROVE/COMMENT -&gt; PASS\n        review_score = CheckScore.PASS\n        review_feedback = \"\"\n        if review.decision == ReviewDecision.REQUEST_CHANGES:\n            review_score = CheckScore.NEEDS_REVISION\n            review_feedback = review.summary\n            if review.issues:\n                review_feedback += \"\\n\" + \"\\n\".join(f\"- {i}\" for i in review.issues)\n\n        return EvaluationResult(\n            pr_number=pr_number,\n            repo=repo,\n            test_score=test_score,\n            lint_score=lint_score,\n            review_score=review_score,\n            revision_number=revision_number,\n            test_feedback=test_feedback,\n            lint_feedback=lint_feedback,\n            review_feedback=review_feedback,\n        )\n\n    def evaluate(\n        self,\n        repo: str,\n        pr_number: int,\n        repo_path: Optional[str] = None,\n        revision_number: int = 0,\n    ) -&gt; EvaluationResult:\n        \"\"\"Run full evaluation on a PR.\n\n        Args:\n            repo: Repository in owner/name format.\n            pr_number: Pull request number.\n            repo_path: Local path for running checks (optional).\n            revision_number: Current revision attempt number.\n\n        Returns:\n            EvaluationResult with scores and feedback.\n        \"\"\"\n        logger.info(f\"Evaluating {repo}#{pr_number} (revision {revision_number})\")\n\n        # Run local checks\n        checks = ChecksReport()\n        if repo_path:\n            runner = CheckRunner(repo_path)\n            pr_details = self.pr_reviewer.get_pr_details(repo, pr_number)\n            changed_files = [f.filename for f in pr_details.files]\n            checks = runner.run_all_checks(changed_files)\n\n        # Run LLM review\n        pr_details = self.pr_reviewer.get_pr_details(repo, pr_number)\n        review = self.llm_reviewer.review_pr(pr_details)\n\n        return self._score(\n            checks, review,\n            repo=repo,\n            pr_number=pr_number,\n            revision_number=revision_number,\n        )\n\n    def close(self) -&gt; None:\n        \"\"\"Clean up resources.\"\"\"\n        if self._pr_reviewer:\n            self._pr_reviewer.close()\n</code></pre> <p>Step 4: Run tests</p> <p>Run: <code>venv/bin/python -m pytest tests/test_evaluator.py -q</code> Expected: All pass.</p> <p>Step 5: Commit</p> <pre><code>git add nebulus_swarm/overlord/evaluator.py tests/test_evaluator.py\ngit commit -m \"feat: add Evaluator class with scoring and revision logic\"\n</code></pre>"},{"location":"plans/2026-02-05-v2-phase2-implementation/#task-3-scope-enforcement-data-model","title":"Task 3: Scope Enforcement \u2014 Data Model","text":"<p>Files: - Create: <code>nebulus_swarm/overlord/scope.py</code> - Create: <code>tests/test_scope.py</code></p> <p>Step 1: Write failing tests</p> <pre><code># tests/test_scope.py\n\"\"\"Tests for worker scope enforcement.\"\"\"\n\nfrom nebulus_swarm.overlord.scope import ScopeConfig, ScopeMode\n\n\nclass TestScopeMode:\n    def test_unrestricted_value(self):\n        assert ScopeMode.UNRESTRICTED.value == \"unrestricted\"\n\n    def test_directory_value(self):\n        assert ScopeMode.DIRECTORY.value == \"directory\"\n\n    def test_explicit_value(self):\n        assert ScopeMode.EXPLICIT.value == \"explicit\"\n\n\nclass TestScopeConfig:\n    def test_unrestricted_allows_all(self):\n        scope = ScopeConfig.unrestricted()\n        assert scope.is_write_allowed(\"any/path/file.py\")\n        assert scope.is_write_allowed(\"deeply/nested/dir/file.txt\")\n\n    def test_directory_allows_matching_paths(self):\n        scope = ScopeConfig(\n            mode=ScopeMode.DIRECTORY,\n            allowed_patterns=[\"src/components/**\", \"tests/components/**\"],\n        )\n        assert scope.is_write_allowed(\"src/components/Button.tsx\")\n        assert scope.is_write_allowed(\"src/components/deep/nested/File.tsx\")\n        assert scope.is_write_allowed(\"tests/components/test_button.py\")\n\n    def test_directory_blocks_non_matching_paths(self):\n        scope = ScopeConfig(\n            mode=ScopeMode.DIRECTORY,\n            allowed_patterns=[\"src/components/**\"],\n        )\n        assert not scope.is_write_allowed(\"src/utils/helper.py\")\n        assert not scope.is_write_allowed(\"README.md\")\n        assert not scope.is_write_allowed(\"package.json\")\n\n    def test_explicit_allows_exact_files(self):\n        scope = ScopeConfig(\n            mode=ScopeMode.EXPLICIT,\n            allowed_patterns=[\"src/app.py\", \"tests/test_app.py\"],\n        )\n        assert scope.is_write_allowed(\"src/app.py\")\n        assert scope.is_write_allowed(\"tests/test_app.py\")\n        assert not scope.is_write_allowed(\"src/other.py\")\n\n    def test_from_json_string(self):\n        scope = ScopeConfig.from_json('[\"src/**\", \"tests/**\"]')\n        assert scope.mode == ScopeMode.DIRECTORY\n        assert scope.is_write_allowed(\"src/foo.py\")\n\n    def test_from_json_empty_means_unrestricted(self):\n        scope = ScopeConfig.from_json(\"\")\n        assert scope.mode == ScopeMode.UNRESTRICTED\n\n    def test_from_json_invalid_means_unrestricted(self):\n        scope = ScopeConfig.from_json(\"not valid json\")\n        assert scope.mode == ScopeMode.UNRESTRICTED\n\n    def test_to_json(self):\n        scope = ScopeConfig(\n            mode=ScopeMode.DIRECTORY,\n            allowed_patterns=[\"src/**\"],\n        )\n        json_str = scope.to_json()\n        assert \"src/**\" in json_str\n\n    def test_violation_message(self):\n        scope = ScopeConfig(\n            mode=ScopeMode.DIRECTORY,\n            allowed_patterns=[\"src/**\"],\n        )\n        msg = scope.violation_message(\"README.md\")\n        assert \"README.md\" in msg\n        assert \"src/**\" in msg\n</code></pre> <p>Step 2: Run to verify failure</p> <p>Run: <code>venv/bin/python -m pytest tests/test_scope.py -q</code> Expected: ImportError.</p> <p>Step 3: Implement ScopeConfig</p> <pre><code># nebulus_swarm/overlord/scope.py\n\"\"\"Worker scope enforcement for Minion file access.\"\"\"\n\nimport fnmatch\nimport json\nimport logging\nfrom dataclasses import dataclass, field\nfrom enum import Enum\nfrom typing import List\n\nlogger = logging.getLogger(__name__)\n\n\nclass ScopeMode(Enum):\n    \"\"\"Scope enforcement mode.\"\"\"\n\n    UNRESTRICTED = \"unrestricted\"\n    DIRECTORY = \"directory\"\n    EXPLICIT = \"explicit\"\n\n\n@dataclass\nclass ScopeConfig:\n    \"\"\"Configuration for Minion write scope.\"\"\"\n\n    mode: ScopeMode = ScopeMode.UNRESTRICTED\n    allowed_patterns: List[str] = field(default_factory=list)\n\n    @classmethod\n    def unrestricted(cls) -&gt; \"ScopeConfig\":\n        \"\"\"Create an unrestricted scope.\"\"\"\n        return cls(mode=ScopeMode.UNRESTRICTED)\n\n    @classmethod\n    def from_json(cls, json_str: str) -&gt; \"ScopeConfig\":\n        \"\"\"Parse scope from JSON string (MINION_SCOPE env var).\n\n        Args:\n            json_str: JSON array of allowed path patterns.\n\n        Returns:\n            ScopeConfig. Unrestricted if empty or invalid.\n        \"\"\"\n        if not json_str or not json_str.strip():\n            return cls.unrestricted()\n        try:\n            patterns = json.loads(json_str)\n            if not isinstance(patterns, list) or not patterns:\n                return cls.unrestricted()\n            return cls(mode=ScopeMode.DIRECTORY, allowed_patterns=patterns)\n        except (json.JSONDecodeError, TypeError):\n            logger.warning(f\"Invalid MINION_SCOPE JSON, using unrestricted: {json_str[:100]}\")\n            return cls.unrestricted()\n\n    def to_json(self) -&gt; str:\n        \"\"\"Serialize allowed patterns to JSON.\"\"\"\n        return json.dumps(self.allowed_patterns)\n\n    def is_write_allowed(self, path: str) -&gt; bool:\n        \"\"\"Check if writing to a path is allowed.\n\n        Args:\n            path: Relative file path to check.\n\n        Returns:\n            True if write is allowed.\n        \"\"\"\n        if self.mode == ScopeMode.UNRESTRICTED:\n            return True\n\n        for pattern in self.allowed_patterns:\n            if self.mode == ScopeMode.EXPLICIT:\n                if path == pattern:\n                    return True\n            else:  # DIRECTORY\n                if fnmatch.fnmatch(path, pattern):\n                    return True\n        return False\n\n    def violation_message(self, path: str) -&gt; str:\n        \"\"\"Generate a human-readable violation message.\n\n        Args:\n            path: The path that was blocked.\n\n        Returns:\n            Error message explaining the restriction.\n        \"\"\"\n        allowed = \", \".join(self.allowed_patterns)\n        return (\n            f\"Write to '{path}' is outside your assigned scope. \"\n            f\"Allowed paths: [{allowed}]. \"\n            f\"If you need to modify this file, use task_blocked to request expanded scope.\"\n        )\n</code></pre> <p>Step 4: Run tests</p> <p>Run: <code>venv/bin/python -m pytest tests/test_scope.py -q</code> Expected: All pass.</p> <p>Step 5: Commit</p> <pre><code>git add nebulus_swarm/overlord/scope.py tests/test_scope.py\ngit commit -m \"feat: add ScopeConfig for worker file-level write enforcement\"\n</code></pre>"},{"location":"plans/2026-02-05-v2-phase2-implementation/#task-4-scope-enforcement-toolexecutor-integration","title":"Task 4: Scope Enforcement \u2014 ToolExecutor Integration","text":"<p>Files: - Modify: <code>nebulus_swarm/minion/agent/tool_executor.py:25-43</code> (constructor) - Modify: <code>nebulus_swarm/minion/agent/tool_executor.py:180-207</code> (write_file) - Create: <code>tests/test_scope_executor.py</code></p> <p>Step 1: Write failing tests</p> <pre><code># tests/test_scope_executor.py\n\"\"\"Tests for scope enforcement in ToolExecutor.\"\"\"\n\nfrom pathlib import Path\nfrom unittest.mock import patch\n\nimport pytest\n\npytest.importorskip(\"openai\")\n\nfrom nebulus_swarm.minion.agent.tool_executor import ToolExecutor\nfrom nebulus_swarm.overlord.scope import ScopeConfig, ScopeMode\n\n\nclass TestScopedToolExecutor:\n    def test_unrestricted_allows_write(self, tmp_path):\n        executor = ToolExecutor(workspace=tmp_path)\n        result = executor.execute(\"write_file\", {\"path\": \"foo.py\", \"content\": \"x = 1\"})\n        assert result.success\n\n    def test_scoped_allows_write_in_scope(self, tmp_path):\n        scope = ScopeConfig(mode=ScopeMode.DIRECTORY, allowed_patterns=[\"src/**\"])\n        executor = ToolExecutor(workspace=tmp_path, scope=scope)\n        (tmp_path / \"src\").mkdir()\n        result = executor.execute(\"write_file\", {\"path\": \"src/app.py\", \"content\": \"x = 1\"})\n        assert result.success\n\n    def test_scoped_blocks_write_outside_scope(self, tmp_path):\n        scope = ScopeConfig(mode=ScopeMode.DIRECTORY, allowed_patterns=[\"src/**\"])\n        executor = ToolExecutor(workspace=tmp_path, scope=scope)\n        result = executor.execute(\"write_file\", {\"path\": \"README.md\", \"content\": \"hi\"})\n        assert not result.success\n        assert \"outside your assigned scope\" in result.error\n\n    def test_scoped_blocks_edit_outside_scope(self, tmp_path):\n        scope = ScopeConfig(mode=ScopeMode.DIRECTORY, allowed_patterns=[\"src/**\"])\n        executor = ToolExecutor(workspace=tmp_path, scope=scope)\n        # Create a file outside scope\n        (tmp_path / \"README.md\").write_text(\"original\")\n        result = executor.execute(\"edit_file\", {\n            \"path\": \"README.md\",\n            \"old_text\": \"original\",\n            \"new_text\": \"modified\",\n        })\n        assert not result.success\n        assert \"outside your assigned scope\" in result.error\n\n    def test_scoped_allows_read_outside_scope(self, tmp_path):\n        scope = ScopeConfig(mode=ScopeMode.DIRECTORY, allowed_patterns=[\"src/**\"])\n        executor = ToolExecutor(workspace=tmp_path, scope=scope)\n        (tmp_path / \"README.md\").write_text(\"hello\")\n        result = executor.execute(\"read_file\", {\"path\": \"README.md\"})\n        assert result.success\n        assert \"hello\" in result.output\n\n    def test_default_scope_is_unrestricted(self, tmp_path):\n        executor = ToolExecutor(workspace=tmp_path)\n        result = executor.execute(\"write_file\", {\"path\": \"anywhere.py\", \"content\": \"x\"})\n        assert result.success\n</code></pre> <p>Step 2: Run to verify failure</p> <p>Run: <code>venv/bin/python -m pytest tests/test_scope_executor.py -q</code> Expected: TypeError \u2014 ToolExecutor doesn't accept <code>scope</code> parameter yet.</p> <p>Step 3: Modify ToolExecutor</p> <p>In <code>nebulus_swarm/minion/agent/tool_executor.py</code>:</p> <ol> <li>Add import at top: <code>from nebulus_swarm.overlord.scope import ScopeConfig</code></li> <li>Add <code>scope</code> parameter to <code>__init__</code>:    <pre><code>def __init__(\n    self,\n    workspace: Path,\n    skill_loader=None,\n    skill_getter=None,\n    scope: Optional[ScopeConfig] = None,\n):\n    self.workspace = workspace.resolve()\n    self.scope = scope or ScopeConfig.unrestricted()\n    # ... rest unchanged\n</code></pre></li> <li>Add scope check method:    <pre><code>def _check_write_scope(self, path: str) -&gt; Optional[str]:\n    \"\"\"Check if a write to path is allowed by scope. Returns error or None.\"\"\"\n    if self.scope.is_write_allowed(path):\n        return None\n    return self.scope.violation_message(path)\n</code></pre></li> <li>Add scope check to <code>_write_file</code> (before <code>resolved.write_text</code>):    <pre><code>scope_error = self._check_write_scope(path)\nif scope_error:\n    return ToolResult(tool_call_id=\"\", name=\"write_file\", success=False, output=\"\", error=scope_error)\n</code></pre></li> <li>Add same check to <code>_edit_file</code> (before the edit logic).</li> </ol> <p>Step 4: Run tests</p> <p>Run: <code>venv/bin/python -m pytest tests/test_scope_executor.py -q</code> Expected: All pass.</p> <p>Step 5: Commit</p> <pre><code>git add nebulus_swarm/minion/agent/tool_executor.py tests/test_scope_executor.py\ngit commit -m \"feat: enforce file-level write scope in ToolExecutor\"\n</code></pre>"},{"location":"plans/2026-02-05-v2-phase2-implementation/#task-5-minion-loads-scope-from-environment","title":"Task 5: Minion Loads Scope from Environment","text":"<p>Files: - Modify: <code>nebulus_swarm/minion/main.py:36-70</code> (MinionConfig) - Add test to: <code>tests/test_scope_executor.py</code></p> <p>Step 1: Write failing test</p> <p>Add to <code>tests/test_scope_executor.py</code>:</p> <pre><code>import os\nfrom unittest.mock import patch\n\n\nclass TestMinionScopeLoading:\n    def test_minion_config_loads_scope_from_env(self):\n        from nebulus_swarm.minion.main import MinionConfig\n\n        env = {\n            \"MINION_ID\": \"m-1\",\n            \"GITHUB_REPO\": \"owner/repo\",\n            \"GITHUB_ISSUE\": \"42\",\n            \"GITHUB_TOKEN\": \"ghp_test\",\n            \"MINION_SCOPE\": '[\"src/**\", \"tests/**\"]',\n        }\n        with patch.dict(os.environ, env, clear=True):\n            config = MinionConfig.from_env()\n        assert config.scope is not None\n        assert config.scope.is_write_allowed(\"src/foo.py\")\n        assert not config.scope.is_write_allowed(\"README.md\")\n\n    def test_minion_config_no_scope_means_unrestricted(self):\n        from nebulus_swarm.minion.main import MinionConfig\n\n        env = {\n            \"MINION_ID\": \"m-1\",\n            \"GITHUB_REPO\": \"owner/repo\",\n            \"GITHUB_ISSUE\": \"42\",\n            \"GITHUB_TOKEN\": \"ghp_test\",\n        }\n        with patch.dict(os.environ, env, clear=True):\n            config = MinionConfig.from_env()\n        assert config.scope.is_write_allowed(\"any/file.py\")\n</code></pre> <p>Step 2: Run to verify failure</p> <p>Run: <code>venv/bin/python -m pytest tests/test_scope_executor.py::TestMinionScopeLoading -q</code> Expected: AttributeError \u2014 MinionConfig has no <code>scope</code> field.</p> <p>Step 3: Modify MinionConfig</p> <p>In <code>nebulus_swarm/minion/main.py</code>:</p> <ol> <li>Add import: <code>from nebulus_swarm.overlord.scope import ScopeConfig</code></li> <li>Add field to <code>MinionConfig</code>: <code>scope: ScopeConfig</code></li> <li>In <code>from_env()</code>, add:    <pre><code>scope=ScopeConfig.from_json(os.environ.get(\"MINION_SCOPE\", \"\")),\n</code></pre></li> </ol> <p>Step 4: Run tests</p> <p>Run: <code>venv/bin/python -m pytest tests/test_scope_executor.py -q</code> Expected: All pass.</p> <p>Step 5: Commit</p> <pre><code>git add nebulus_swarm/minion/main.py tests/test_scope_executor.py\ngit commit -m \"feat: Minion loads MINION_SCOPE from environment\"\n</code></pre>"},{"location":"plans/2026-02-05-v2-phase2-implementation/#task-6-proposals-data-model-and-store","title":"Task 6: Proposals \u2014 Data Model and Store","text":"<p>Files: - Create: <code>nebulus_swarm/overlord/proposals.py</code> - Create: <code>tests/test_proposals.py</code></p> <p>Step 1: Write failing tests</p> <pre><code># tests/test_proposals.py\n\"\"\"Tests for the enhancement proposal system.\"\"\"\n\nfrom nebulus_swarm.overlord.proposals import (\n    EnhancementProposal,\n    ProposalStatus,\n    ProposalStore,\n    ProposalType,\n)\n\n\nclass TestProposalTypes:\n    def test_new_skill_type(self):\n        assert ProposalType.NEW_SKILL.value == \"new_skill\"\n\n    def test_statuses(self):\n        assert ProposalStatus.PENDING.value == \"pending\"\n        assert ProposalStatus.APPROVED.value == \"approved\"\n        assert ProposalStatus.REJECTED.value == \"rejected\"\n        assert ProposalStatus.IMPLEMENTED.value == \"implemented\"\n\n\nclass TestEnhancementProposal:\n    def test_create_proposal(self):\n        p = EnhancementProposal(\n            type=ProposalType.NEW_SKILL,\n            title=\"Add React testing skill\",\n            rationale=\"Minion failed React tests 3 times\",\n            proposed_action=\"Create a skill for React component testing\",\n        )\n        assert p.id  # UUID auto-generated\n        assert p.status == ProposalStatus.PENDING\n\n    def test_is_actionable(self):\n        p = EnhancementProposal(\n            type=ProposalType.TOOL_FIX,\n            title=\"Fix linting\",\n            rationale=\"Lint always fails\",\n            proposed_action=\"Update ruff config\",\n        )\n        assert p.is_actionable  # pending\n        p.status = ProposalStatus.APPROVED\n        assert p.is_actionable  # approved but not implemented\n        p.status = ProposalStatus.IMPLEMENTED\n        assert not p.is_actionable\n\n\nclass TestProposalStore:\n    def test_create_and_get(self, tmp_path):\n        store = ProposalStore(db_path=str(tmp_path / \"test.db\"))\n        p = EnhancementProposal(\n            type=ProposalType.NEW_SKILL,\n            title=\"Test proposal\",\n            rationale=\"Testing\",\n            proposed_action=\"Do something\",\n        )\n        store.save(p)\n        loaded = store.get(p.id)\n        assert loaded is not None\n        assert loaded.title == \"Test proposal\"\n\n    def test_list_pending(self, tmp_path):\n        store = ProposalStore(db_path=str(tmp_path / \"test.db\"))\n        p1 = EnhancementProposal(\n            type=ProposalType.NEW_SKILL,\n            title=\"Pending\",\n            rationale=\"r\",\n            proposed_action=\"a\",\n        )\n        p2 = EnhancementProposal(\n            type=ProposalType.TOOL_FIX,\n            title=\"Also pending\",\n            rationale=\"r\",\n            proposed_action=\"a\",\n        )\n        store.save(p1)\n        store.save(p2)\n        pending = store.list_by_status(ProposalStatus.PENDING)\n        assert len(pending) == 2\n\n    def test_approve(self, tmp_path):\n        store = ProposalStore(db_path=str(tmp_path / \"test.db\"))\n        p = EnhancementProposal(\n            type=ProposalType.CONFIG_CHANGE,\n            title=\"Change config\",\n            rationale=\"r\",\n            proposed_action=\"a\",\n        )\n        store.save(p)\n        store.update_status(p.id, ProposalStatus.APPROVED)\n        loaded = store.get(p.id)\n        assert loaded.status == ProposalStatus.APPROVED\n\n    def test_list_empty(self, tmp_path):\n        store = ProposalStore(db_path=str(tmp_path / \"test.db\"))\n        assert store.list_by_status(ProposalStatus.PENDING) == []\n\n    def test_get_nonexistent_returns_none(self, tmp_path):\n        store = ProposalStore(db_path=str(tmp_path / \"test.db\"))\n        assert store.get(\"nonexistent-id\") is None\n</code></pre> <p>Step 2: Run to verify failure</p> <p>Run: <code>venv/bin/python -m pytest tests/test_proposals.py -q</code> Expected: ImportError.</p> <p>Step 3: Implement proposals module</p> <pre><code># nebulus_swarm/overlord/proposals.py\n\"\"\"Enhancement proposal system for supervisor-identified improvements.\"\"\"\n\nimport os\nimport sqlite3\nimport uuid\nfrom contextlib import contextmanager\nfrom dataclasses import dataclass, field\nfrom datetime import datetime\nfrom enum import Enum\nfrom typing import Generator, List, Optional\n\n\nclass ProposalType(Enum):\n    \"\"\"Type of enhancement proposal.\"\"\"\n\n    NEW_SKILL = \"new_skill\"\n    TOOL_FIX = \"tool_fix\"\n    CONFIG_CHANGE = \"config_change\"\n    WORKFLOW_IMPROVEMENT = \"workflow_improvement\"\n\n\nclass ProposalStatus(Enum):\n    \"\"\"Status of an enhancement proposal.\"\"\"\n\n    PENDING = \"pending\"\n    APPROVED = \"approved\"\n    REJECTED = \"rejected\"\n    IMPLEMENTED = \"implemented\"\n\n\n@dataclass\nclass EnhancementProposal:\n    \"\"\"A structured proposal for system improvement.\"\"\"\n\n    type: ProposalType\n    title: str\n    rationale: str\n    proposed_action: str\n    estimated_impact: str = \"Medium\"\n    risk: str = \"Low\"\n    status: ProposalStatus = ProposalStatus.PENDING\n    related_issues: List[int] = field(default_factory=list)\n    id: str = field(default_factory=lambda: str(uuid.uuid4()))\n    created_at: datetime = field(default_factory=datetime.now)\n    resolved_at: Optional[datetime] = None\n\n    @property\n    def is_actionable(self) -&gt; bool:\n        \"\"\"Check if proposal still needs action.\"\"\"\n        return self.status in (ProposalStatus.PENDING, ProposalStatus.APPROVED)\n\n\nclass ProposalStore:\n    \"\"\"SQLite-backed storage for enhancement proposals.\"\"\"\n\n    def __init__(self, db_path: str):\n        self.db_path = db_path\n        self._ensure_dir()\n        self._init_db()\n\n    def _ensure_dir(self) -&gt; None:\n        db_dir = os.path.dirname(self.db_path)\n        if db_dir:\n            os.makedirs(db_dir, exist_ok=True)\n\n    @contextmanager\n    def _conn(self) -&gt; Generator[sqlite3.Connection, None, None]:\n        conn = sqlite3.connect(self.db_path)\n        conn.row_factory = sqlite3.Row\n        try:\n            yield conn\n            conn.commit()\n        finally:\n            conn.close()\n\n    def _init_db(self) -&gt; None:\n        with self._conn() as conn:\n            conn.execute(\"\"\"\n                CREATE TABLE IF NOT EXISTS proposals (\n                    id TEXT PRIMARY KEY,\n                    type TEXT NOT NULL,\n                    title TEXT NOT NULL,\n                    rationale TEXT NOT NULL,\n                    proposed_action TEXT NOT NULL,\n                    estimated_impact TEXT DEFAULT 'Medium',\n                    risk TEXT DEFAULT 'Low',\n                    status TEXT NOT NULL DEFAULT 'pending',\n                    related_issues TEXT DEFAULT '[]',\n                    created_at TEXT NOT NULL,\n                    resolved_at TEXT\n                )\n            \"\"\")\n\n    def save(self, proposal: EnhancementProposal) -&gt; None:\n        \"\"\"Save a proposal to the store.\"\"\"\n        import json\n\n        with self._conn() as conn:\n            conn.execute(\n                \"\"\"INSERT OR REPLACE INTO proposals\n                   (id, type, title, rationale, proposed_action,\n                    estimated_impact, risk, status, related_issues,\n                    created_at, resolved_at)\n                   VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\"\"\",\n                (\n                    proposal.id,\n                    proposal.type.value,\n                    proposal.title,\n                    proposal.rationale,\n                    proposal.proposed_action,\n                    proposal.estimated_impact,\n                    proposal.risk,\n                    proposal.status.value,\n                    json.dumps(proposal.related_issues),\n                    proposal.created_at.isoformat(),\n                    proposal.resolved_at.isoformat() if proposal.resolved_at else None,\n                ),\n            )\n\n    def get(self, proposal_id: str) -&gt; Optional[EnhancementProposal]:\n        \"\"\"Get a proposal by ID.\"\"\"\n        with self._conn() as conn:\n            row = conn.execute(\n                \"SELECT * FROM proposals WHERE id = ?\", (proposal_id,)\n            ).fetchone()\n            if row:\n                return self._row_to_proposal(row)\n            return None\n\n    def list_by_status(self, status: ProposalStatus) -&gt; List[EnhancementProposal]:\n        \"\"\"List proposals with a given status.\"\"\"\n        with self._conn() as conn:\n            rows = conn.execute(\n                \"SELECT * FROM proposals WHERE status = ? ORDER BY created_at DESC\",\n                (status.value,),\n            ).fetchall()\n            return [self._row_to_proposal(r) for r in rows]\n\n    def update_status(\n        self, proposal_id: str, status: ProposalStatus\n    ) -&gt; None:\n        \"\"\"Update a proposal's status.\"\"\"\n        resolved_at = None\n        if status in (ProposalStatus.REJECTED, ProposalStatus.IMPLEMENTED):\n            resolved_at = datetime.now().isoformat()\n\n        with self._conn() as conn:\n            conn.execute(\n                \"UPDATE proposals SET status = ?, resolved_at = ? WHERE id = ?\",\n                (status.value, resolved_at, proposal_id),\n            )\n\n    def _row_to_proposal(self, row: sqlite3.Row) -&gt; EnhancementProposal:\n        import json\n\n        return EnhancementProposal(\n            id=row[\"id\"],\n            type=ProposalType(row[\"type\"]),\n            title=row[\"title\"],\n            rationale=row[\"rationale\"],\n            proposed_action=row[\"proposed_action\"],\n            estimated_impact=row[\"estimated_impact\"],\n            risk=row[\"risk\"],\n            status=ProposalStatus(row[\"status\"]),\n            related_issues=json.loads(row[\"related_issues\"]),\n            created_at=datetime.fromisoformat(row[\"created_at\"]),\n            resolved_at=datetime.fromisoformat(row[\"resolved_at\"])\n            if row[\"resolved_at\"]\n            else None,\n        )\n</code></pre> <p>Step 4: Run tests</p> <p>Run: <code>venv/bin/python -m pytest tests/test_proposals.py -q</code> Expected: All pass.</p> <p>Step 5: Commit</p> <pre><code>git add nebulus_swarm/overlord/proposals.py tests/test_proposals.py\ngit commit -m \"feat: add enhancement proposal system with SQLite store\"\n</code></pre>"},{"location":"plans/2026-02-05-v2-phase2-implementation/#task-7-proposals-cli-commands","title":"Task 7: Proposals CLI Commands","text":"<p>Files: - Create: <code>nebulus_atom/commands/proposals.py</code> - Create: <code>tests/test_proposals_cli.py</code> - Modify: <code>nebulus_atom/main.py</code> (add proposals command)</p> <p>Step 1: Write failing tests</p> <pre><code># tests/test_proposals_cli.py\n\"\"\"Tests for the proposals CLI commands.\"\"\"\n\nfrom nebulus_atom.commands.proposals import format_proposal_list, format_proposal_detail\nfrom nebulus_swarm.overlord.proposals import (\n    EnhancementProposal,\n    ProposalStatus,\n    ProposalType,\n)\n\n\nclass TestFormatProposalList:\n    def test_empty_list(self):\n        output = format_proposal_list([])\n        assert \"No pending proposals\" in output\n\n    def test_formats_proposals(self):\n        proposals = [\n            EnhancementProposal(\n                type=ProposalType.NEW_SKILL,\n                title=\"Add React skill\",\n                rationale=\"Failures\",\n                proposed_action=\"Create skill\",\n            ),\n        ]\n        output = format_proposal_list(proposals)\n        assert \"Add React skill\" in output\n        assert \"new_skill\" in output\n\n\nclass TestFormatProposalDetail:\n    def test_shows_all_fields(self):\n        p = EnhancementProposal(\n            type=ProposalType.TOOL_FIX,\n            title=\"Fix linter\",\n            rationale=\"Ruff config wrong\",\n            proposed_action=\"Update .ruff.toml\",\n            estimated_impact=\"Low\",\n            risk=\"Low\",\n        )\n        output = format_proposal_detail(p)\n        assert \"Fix linter\" in output\n        assert \"Ruff config wrong\" in output\n        assert \"Update .ruff.toml\" in output\n</code></pre> <p>Step 2: Run to verify failure</p> <p>Run: <code>venv/bin/python -m pytest tests/test_proposals_cli.py -q</code> Expected: ImportError.</p> <p>Step 3: Implement CLI module</p> <pre><code># nebulus_atom/commands/proposals.py\n\"\"\"CLI commands for managing enhancement proposals.\"\"\"\n\nfrom typing import List\n\nfrom nebulus_swarm.overlord.proposals import EnhancementProposal\n\n\ndef format_proposal_list(proposals: List[EnhancementProposal]) -&gt; str:\n    \"\"\"Format a list of proposals for terminal display.\"\"\"\n    if not proposals:\n        return \"No pending proposals.\"\n\n    lines = []\n    for p in proposals:\n        lines.append(f\"  [{p.type.value}] {p.title}\")\n        lines.append(f\"    ID: {p.id[:8]}...  Status: {p.status.value}\")\n        lines.append(\"\")\n    return \"\\n\".join(lines)\n\n\ndef format_proposal_detail(proposal: EnhancementProposal) -&gt; str:\n    \"\"\"Format a single proposal with full detail.\"\"\"\n    lines = [\n        f\"Proposal: {proposal.title}\",\n        f\"Type: {proposal.type.value}\",\n        f\"Status: {proposal.status.value}\",\n        f\"Impact: {proposal.estimated_impact}  Risk: {proposal.risk}\",\n        \"\",\n        \"Rationale:\",\n        f\"  {proposal.rationale}\",\n        \"\",\n        \"Proposed Action:\",\n        f\"  {proposal.proposed_action}\",\n    ]\n    if proposal.related_issues:\n        lines.append(\"\")\n        lines.append(f\"Related Issues: {', '.join(f'#{i}' for i in proposal.related_issues)}\")\n    return \"\\n\".join(lines)\n</code></pre> <p>Step 4: Run tests</p> <p>Run: <code>venv/bin/python -m pytest tests/test_proposals_cli.py -q</code> Expected: All pass.</p> <p>Step 5: Add <code>proposals</code> command to <code>main.py</code></p> <p>Add to <code>nebulus_atom/main.py</code> after the <code>review_pr</code> command:</p> <pre><code>@app.command()\ndef proposals(\n    action: str = typer.Argument(..., help=\"Action: 'list', 'approve &lt;id&gt;', 'reject &lt;id&gt;'\"),\n    proposal_id: Optional[str] = typer.Argument(None, help=\"Proposal ID (for approve/reject)\"),\n):\n    \"\"\"Manage enhancement proposals.\"\"\"\n    from nebulus_atom.commands.proposals import format_proposal_list, format_proposal_detail\n    # Implementation wired later when ProposalStore path is configured\n    console = Console()\n    console.print(f\"[yellow]proposals {action} \u2014 not yet wired to store[/yellow]\")\n</code></pre> <p>Step 6: Commit</p> <pre><code>git add nebulus_atom/commands/proposals.py tests/test_proposals_cli.py nebulus_atom/main.py\ngit commit -m \"feat: add proposals CLI commands and formatters\"\n</code></pre>"},{"location":"plans/2026-02-05-v2-phase2-implementation/#task-8-state-db-add-evaluations-and-proposals-tables","title":"Task 8: State DB \u2014 Add Evaluations and Proposals Tables","text":"<p>Files: - Modify: <code>nebulus_swarm/overlord/state.py:42-94</code> (_init_db) - Add tests to: <code>tests/test_evaluator.py</code></p> <p>Step 1: Write failing test</p> <p>Add to <code>tests/test_evaluator.py</code>:</p> <pre><code>class TestEvaluationStorage:\n    def test_store_and_retrieve_evaluation(self, tmp_path):\n        from nebulus_swarm.overlord.state import OverlordState\n\n        state = OverlordState(db_path=str(tmp_path / \"test.db\"))\n        result = EvaluationResult(\n            pr_number=42,\n            repo=\"owner/repo\",\n            test_score=CheckScore.PASS,\n            lint_score=CheckScore.PASS,\n            review_score=CheckScore.NEEDS_REVISION,\n            revision_number=1,\n            review_feedback=\"Needs error handling\",\n        )\n        state.save_evaluation(result)\n        history = state.get_evaluations(repo=\"owner/repo\", pr_number=42)\n        assert len(history) == 1\n        assert history[0][\"review_score\"] == \"needs_revision\"\n</code></pre> <p>Step 2: Run to verify failure</p> <p>Run: <code>venv/bin/python -m pytest tests/test_evaluator.py::TestEvaluationStorage -q</code> Expected: AttributeError \u2014 <code>save_evaluation</code> not defined.</p> <p>Step 3: Add evaluations table to state.py</p> <p>Add to <code>_init_db()</code> in <code>nebulus_swarm/overlord/state.py</code>:</p> <pre><code>cursor.execute(\"\"\"\n    CREATE TABLE IF NOT EXISTS evaluations (\n        id INTEGER PRIMARY KEY AUTOINCREMENT,\n        pr_number INTEGER NOT NULL,\n        repo TEXT NOT NULL,\n        test_score TEXT NOT NULL,\n        lint_score TEXT NOT NULL,\n        review_score TEXT NOT NULL,\n        overall TEXT NOT NULL,\n        revision_number INTEGER DEFAULT 0,\n        feedback TEXT,\n        evaluated_at TEXT NOT NULL\n    )\n\"\"\")\n</code></pre> <p>Add methods:</p> <pre><code>def save_evaluation(self, result: \"EvaluationResult\") -&gt; None:\n    from nebulus_swarm.overlord.evaluator import EvaluationResult\n    with self._get_connection() as conn:\n        conn.execute(\n            \"\"\"INSERT INTO evaluations\n               (pr_number, repo, test_score, lint_score, review_score,\n                overall, revision_number, feedback, evaluated_at)\n               VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)\"\"\",\n            (\n                result.pr_number, result.repo,\n                result.test_score.value, result.lint_score.value,\n                result.review_score.value, result.overall.value,\n                result.revision_number, result.combined_feedback,\n                result.timestamp.isoformat(),\n            ),\n        )\n\ndef get_evaluations(self, repo: str, pr_number: int) -&gt; List[dict]:\n    with self._get_connection() as conn:\n        rows = conn.execute(\n            \"SELECT * FROM evaluations WHERE repo = ? AND pr_number = ? ORDER BY evaluated_at\",\n            (repo, pr_number),\n        ).fetchall()\n        return [dict(r) for r in rows]\n</code></pre> <p>Step 4: Run tests</p> <p>Run: <code>venv/bin/python -m pytest tests/test_evaluator.py -q</code> Expected: All pass.</p> <p>Step 5: Commit</p> <pre><code>git add nebulus_swarm/overlord/state.py tests/test_evaluator.py\ngit commit -m \"feat: add evaluations table to Overlord state DB\"\n</code></pre>"},{"location":"plans/2026-02-05-v2-phase2-implementation/#task-9-run-full-test-suite-and-final-commit","title":"Task 9: Run Full Test Suite and Final Commit","text":"<p>Step 1: Run all tests</p> <p>Run: <code>venv/bin/python -m pytest tests/ -q</code> Expected: 642+ tests pass, 0 failures.</p> <p>Step 2: Verify no broken imports</p> <p>Run: <code>venv/bin/python -c \"from nebulus_swarm.overlord.evaluator import Evaluator; from nebulus_swarm.overlord.scope import ScopeConfig; from nebulus_swarm.overlord.proposals import ProposalStore; print('All imports OK')\"</code></p> <p>Step 3: Final integration commit (if any unstaged changes)</p> <pre><code>git add -A\ngit commit -m \"chore: Phase 2 integration cleanup\"\n</code></pre>"},{"location":"plans/2026-02-05-v2-phase2-supervisor-design/","title":"V2 Phase 2: Supervisor/Worker Formalization","text":"<p>Date: 2026-02-05 Status: Approved Design Prerequisite: V2 Phase 1 (complete)</p>"},{"location":"plans/2026-02-05-v2-phase2-supervisor-design/#overview","title":"Overview","text":"<p>Phase 2 adds a quality gate, enhancement proposals, worker scope enforcement, and skill evolution to the existing Overlord/Minion architecture. All four features are implemented as extensions to the Overlord \u2014 no new services.</p>"},{"location":"plans/2026-02-05-v2-phase2-supervisor-design/#1-evaluation-layer","title":"1. Evaluation Layer","text":"<p>Module: <code>nebulus_swarm/overlord/evaluator.py</code></p> <p>After a Minion completes work and creates a PR, the Overlord invokes the Evaluator instead of immediately posting to Slack.</p>"},{"location":"plans/2026-02-05-v2-phase2-supervisor-design/#evaluation-steps","title":"Evaluation Steps","text":"<ol> <li>Tests \u2014 Run <code>pytest</code> on the PR branch via <code>CheckRunner</code> from the reviewer module.</li> <li>Lint \u2014 Run ruff/flake8 on changed files.</li> <li>LLM Review \u2014 Existing <code>LLMReviewer</code> with pass/fail/revise decision.</li> </ol> <p>Each check produces a score: <code>PASS</code>, <code>FAIL</code>, or <code>NEEDS_REVISION</code>. Overall result: pass if all pass, revise if any need revision, fail if any hard-fail.</p>"},{"location":"plans/2026-02-05-v2-phase2-supervisor-design/#revision-cycle","title":"Revision Cycle","text":"<p>Maximum 2 revisions (3 total attempts):</p> <ol> <li>Evaluator creates a <code>RevisionRequest</code> with specific feedback (test output, review issues).</li> <li>Overlord spawns a new Minion container on the same branch with revision context injected into the system prompt.</li> <li>Revision Minion has access to the original issue + evaluator feedback.</li> <li>After revision, Evaluator runs again.</li> <li>After 2 failed revisions, escalate to user via Slack.</li> </ol>"},{"location":"plans/2026-02-05-v2-phase2-supervisor-design/#data-model","title":"Data Model","text":"<pre><code>@dataclass\nclass EvaluationResult:\n    pr_number: int\n    repo: str\n    test_score: CheckScore        # PASS | FAIL | NEEDS_REVISION\n    lint_score: CheckScore\n    review_score: CheckScore\n    overall: CheckScore\n    feedback: str                  # Combined feedback for revision\n    revision_number: int           # 0 = first attempt\n    timestamp: datetime\n\nclass CheckScore(Enum):\n    PASS = \"pass\"\n    FAIL = \"fail\"\n    NEEDS_REVISION = \"needs_revision\"\n</code></pre> <p>Evaluation history stored in Overlord's SQLite state DB.</p>"},{"location":"plans/2026-02-05-v2-phase2-supervisor-design/#flow","title":"Flow","text":"<pre><code>Minion completes \u2192 creates PR \u2192 reports to Overlord\n    \u2502\n    \u25bc\nEvaluator.evaluate(repo, pr_number)\n    \u2502\n    \u251c\u2500 PASS \u2192 Post success to Slack, finalize PR\n    \u251c\u2500 NEEDS_REVISION (attempt &lt; 3) \u2192 Spawn revision Minion on same branch\n    \u2514\u2500 FAIL or max revisions \u2192 Escalate to user via Slack\n</code></pre>"},{"location":"plans/2026-02-05-v2-phase2-supervisor-design/#2-enhancement-proposal-system","title":"2. Enhancement Proposal System","text":"<p>Module: <code>nebulus_swarm/overlord/proposals.py</code></p> <p>When the Evaluator identifies a capability gap, it creates a structured proposal for user approval. Proposals are never auto-approved.</p>"},{"location":"plans/2026-02-05-v2-phase2-supervisor-design/#triggers","title":"Triggers","text":"<ul> <li>Same failure pattern across 2+ issues (e.g., \"Minion keeps failing on React tests\").</li> <li>Minion explicitly reports a blocker it cannot resolve.</li> <li>LLM review flags a systemic issue (e.g., \"no error handling convention\").</li> </ul>"},{"location":"plans/2026-02-05-v2-phase2-supervisor-design/#proposal-structure","title":"Proposal Structure","text":"<pre><code>@dataclass\nclass EnhancementProposal:\n    id: str                        # UUID\n    type: ProposalType             # new_skill | tool_fix | config_change | workflow_improvement\n    title: str\n    rationale: str                 # What triggered it, evidence\n    proposed_action: str           # What should be done\n    estimated_impact: str          # Low | Medium | High\n    risk: str                      # Low | Medium | High\n    status: ProposalStatus         # pending | approved | rejected | implemented\n    related_issues: list[int]      # Issue numbers that triggered this\n    created_at: datetime\n    resolved_at: Optional[datetime]\n\nclass ProposalType(Enum):\n    NEW_SKILL = \"new_skill\"\n    TOOL_FIX = \"tool_fix\"\n    CONFIG_CHANGE = \"config_change\"\n    WORKFLOW_IMPROVEMENT = \"workflow_improvement\"\n\nclass ProposalStatus(Enum):\n    PENDING = \"pending\"\n    APPROVED = \"approved\"\n    REJECTED = \"rejected\"\n    IMPLEMENTED = \"implemented\"\n</code></pre>"},{"location":"plans/2026-02-05-v2-phase2-supervisor-design/#storage","title":"Storage","text":"<p>SQLite table <code>proposals</code> in the Overlord state DB (same pattern as <code>minion_state</code>).</p>"},{"location":"plans/2026-02-05-v2-phase2-supervisor-design/#user-interaction","title":"User Interaction","text":"<p>Slack: - Proposals posted to channel with Block Kit Approve/Reject buttons. - Approval triggers dispatch (regular issue for most types, skill workflow for <code>new_skill</code>).</p> <p>CLI: - <code>nebulus-atom proposals list</code> \u2014 Show pending proposals. - <code>nebulus-atom proposals approve &lt;id&gt;</code> \u2014 Approve a proposal. - <code>nebulus-atom proposals reject &lt;id&gt;</code> \u2014 Reject with optional reason.</p>"},{"location":"plans/2026-02-05-v2-phase2-supervisor-design/#safety-constraint","title":"Safety Constraint","text":"<p>The Overlord stores proposals and waits for user action. No autonomous self-improvement.</p>"},{"location":"plans/2026-02-05-v2-phase2-supervisor-design/#3-worker-scope-enforcement","title":"3. Worker Scope Enforcement","text":"<p>Module: <code>nebulus_swarm/overlord/scope.py</code></p> <p>The Overlord assigns each Minion a bounded file scope when dispatching work, restricting what the Minion can write to.</p>"},{"location":"plans/2026-02-05-v2-phase2-supervisor-design/#scope-assignment","title":"Scope Assignment","text":"<ol> <li>Overlord analyzes the issue (labels, title, linked files) to determine relevant directories.</li> <li>Overlord passes <code>MINION_SCOPE</code> env var to the container: JSON list of allowed path patterns.    <pre><code>[\"src/components/**\", \"tests/components/**\", \"package.json\"]\n</code></pre></li> <li>Minion's <code>ToolExecutor</code> loads scope on init and checks every write operation.</li> </ol>"},{"location":"plans/2026-02-05-v2-phase2-supervisor-design/#scope-modes","title":"Scope Modes","text":"Mode Behavior When Used <code>unrestricted</code> No write restrictions (default) Overlord can't determine scope <code>directory</code> Writes restricted to directories Inferred from issue labels/file mentions <code>explicit</code> Writes restricted to exact file list Targeted fixes"},{"location":"plans/2026-02-05-v2-phase2-supervisor-design/#enforcement","title":"Enforcement","text":"<ul> <li>Reads: Always allowed. Minion needs full context to understand the codebase.</li> <li>Writes (<code>write_file</code>, <code>edit_file</code>): Checked against scope. Blocked with error message if outside.</li> <li>run_command: Allowed. Working directory must be within workspace (existing check).</li> <li>Violation: Tool returns error explaining the restriction. Minion can ask a question to request expanded scope (routed to user via Slack).</li> </ul>"},{"location":"plans/2026-02-05-v2-phase2-supervisor-design/#implementation","title":"Implementation","text":"<p>Extend <code>nebulus_swarm/minion/agent/tool_executor.py</code>:</p> <pre><code>class ToolExecutor:\n    def __init__(self, workspace: Path, scope: Optional[ScopeConfig] = None):\n        self.workspace = workspace\n        self.scope = scope or ScopeConfig.unrestricted()\n\n    def _check_write_scope(self, filepath: str) -&gt; Optional[str]:\n        \"\"\"Return error message if write is outside scope, None if allowed.\"\"\"\n        if self.scope.mode == ScopeMode.UNRESTRICTED:\n            return None\n        # Check against allowed patterns...\n</code></pre> <p>No changes to the Minion agent itself \u2014 scope is enforced at the tool executor level, transparent to the LLM.</p>"},{"location":"plans/2026-02-05-v2-phase2-supervisor-design/#4-skill-evolution-workflow","title":"4. Skill Evolution Workflow","text":"<p>When a new skill is needed (identified via enhancement proposal or user request), the system follows a structured lifecycle.</p>"},{"location":"plans/2026-02-05-v2-phase2-supervisor-design/#lifecycle","title":"Lifecycle","text":"<ol> <li>Draft \u2014 Overlord generates a skill spec from the proposal context (name, description, triggers, instructions outline). Stored as a <code>SkillDraft</code> in SQLite.</li> <li>Approve \u2014 User reviews via Slack or CLI (<code>nebulus-atom skills drafts</code>). Can modify triggers or instructions before approving.</li> <li>Implement \u2014 Overlord dispatches a Minion to write the <code>.yaml</code> skill file following the existing schema (<code>nebulus_swarm/minion/skills/schema.py</code>). Minion gets the approved spec as issue context.</li> <li>Validate \u2014 Evaluator checks: valid YAML, matches schema, triggers don't conflict with existing skills. LLM review confirms instructions are coherent.</li> <li>Deploy \u2014 Skill file committed to <code>.nebulus/skills/</code> via PR. User merges.</li> </ol>"},{"location":"plans/2026-02-05-v2-phase2-supervisor-design/#no-hot-loading","title":"No Hot-Loading","text":"<p>Skills only become active after the PR is merged and the next Minion picks them up. The user always reviews skill content in a PR.</p>"},{"location":"plans/2026-02-05-v2-phase2-supervisor-design/#skill-registry","title":"Skill Registry","text":"<p>Overlord maintains a <code>skills</code> table tracking: - Known skills and their origin (manual vs evolved) - Usage count per skill - Success rate (issues completed successfully when skill was loaded)</p> <p>Low success rate feeds back into the proposal system \u2014 Evaluator can propose a skill revision.</p>"},{"location":"plans/2026-02-05-v2-phase2-supervisor-design/#integration-summary","title":"Integration Summary","text":"<pre><code>Issue arrives (GitHub label or Slack command)\n    \u2502\n    \u25bc\nOverlord assigns scope (file patterns from issue analysis)\n    \u2502\n    \u25bc\nMinion executes (scoped ToolExecutor, loaded skills)\n    \u2502\n    \u25bc\nEvaluator runs (tests \u2192 lint \u2192 LLM review)\n    \u2502\n    \u251c\u2500 PASS \u2192 Post to Slack, finalize PR\n    \u251c\u2500 NEEDS_REVISION \u2192 Re-dispatch with feedback (max 2x)\n    \u2514\u2500 FAIL (after retries) \u2192 Escalate to user via Slack\n                                    \u2502\n                                    \u25bc\n                        Evaluator detects pattern?\n                        \u251c\u2500 Yes \u2192 Create EnhancementProposal\n                        \u2502           \u2502\n                        \u2502           \u25bc\n                        \u2502       User approves via Slack/CLI\n                        \u2502           \u2502\n                        \u2502           \u25bc\n                        \u2502       type == new_skill?\n                        \u2502       \u251c\u2500 Yes \u2192 Skill evolution workflow\n                        \u2502       \u2514\u2500 No  \u2192 Dispatch as regular issue\n                        \u2514\u2500 No  \u2192 Done\n</code></pre>"},{"location":"plans/2026-02-05-v2-phase2-supervisor-design/#file-changes","title":"File Changes","text":""},{"location":"plans/2026-02-05-v2-phase2-supervisor-design/#new-files","title":"New Files","text":"File Purpose <code>nebulus_swarm/overlord/evaluator.py</code> EvaluationResult, Evaluator class, revision dispatch <code>nebulus_swarm/overlord/proposals.py</code> EnhancementProposal, ProposalStore, Slack formatting <code>nebulus_swarm/overlord/scope.py</code> ScopeConfig, ScopeMode, scope inference from issues <code>nebulus_atom/commands/proposals.py</code> CLI commands for proposal management <code>tests/test_evaluator.py</code> Evaluator tests <code>tests/test_proposals.py</code> Proposal system tests <code>tests/test_scope.py</code> Scope enforcement tests"},{"location":"plans/2026-02-05-v2-phase2-supervisor-design/#modified-files","title":"Modified Files","text":"File Change <code>nebulus_swarm/overlord/main.py</code> Wire evaluator into post-completion, dispatch revisions <code>nebulus_swarm/overlord/state.py</code> Add <code>evaluations</code>, <code>proposals</code>, <code>skills</code> tables <code>nebulus_swarm/minion/agent/tool_executor.py</code> Add write-scope checking <code>nebulus_swarm/minion/main.py</code> Load <code>MINION_SCOPE</code> env var, pass to ToolExecutor <code>nebulus_swarm/overlord/slack_bot.py</code> Proposal buttons, evaluation summaries <code>nebulus_atom/main.py</code> Add <code>proposals</code> CLI command group"},{"location":"plans/2026-02-05-v2-phase2-supervisor-design/#implementation-order","title":"Implementation Order","text":"<ol> <li>Evaluator (item 6) \u2014 Foundation. Tests + lint + LLM review pipeline, revision dispatch.</li> <li>Scope enforcement (item 8) \u2014 Independent. ToolExecutor changes + scope env var.</li> <li>Proposals (item 7) \u2014 Depends on evaluator for pattern detection. SQLite + Slack + CLI.</li> <li>Skill evolution (item 9) \u2014 Depends on proposals. Draft/approve/implement lifecycle.</li> </ol>"},{"location":"plans/2026-02-05-v2-phase2-supervisor-design/#testing-strategy","title":"Testing Strategy","text":"<p>Each module gets its own test file with mocked dependencies: - Evaluator: Mock CheckRunner, LLMReviewer, test scoring logic and revision dispatch - Scope: Test pattern matching, violation detection, unrestricted mode - Proposals: Test CRUD, status transitions, Slack formatting - Skill evolution: Test lifecycle state machine, validation</p>"},{"location":"plans/2026-02-06-overlord-phase-2-implementation/","title":"Overlord Phase 2: Dispatch + Autonomy \u2014 Implementation Plan","text":"<p>Author: West AI Labs Date: 2026-02-06 Status: Implementation Plan Phase: 2 of 5</p>"},{"location":"plans/2026-02-06-overlord-phase-2-implementation/#context","title":"Context","text":"<p>Phase 1 shipped the foundation: project registry, scanner, dependency graph, action scope model, and cross-project memory (110 tests). Phase 2 adds the ability to dispatch actions across multiple projects with configurable autonomy levels.</p> <p>This is the bridge from \"observe and report\" to \"propose and execute.\"</p>"},{"location":"plans/2026-02-06-overlord-phase-2-implementation/#goals","title":"Goals","text":"<ol> <li>Autonomy Engine \u2014 Three levels (cautious/proactive/scheduled) with runtime switching</li> <li>Multi-repo Dispatch \u2014 Coordinate tasks across multiple projects</li> <li>Model Router \u2014 Intelligent task-to-model assignment (local \u2192 cloud-fast \u2192 cloud-heavy)</li> <li>Release Coordination \u2014 Automated multi-repo releases with dependency awareness</li> <li>Pre-approved Actions \u2014 Allow scheduled mode to auto-execute safe operations</li> </ol>"},{"location":"plans/2026-02-06-overlord-phase-2-implementation/#architecture","title":"Architecture","text":""},{"location":"plans/2026-02-06-overlord-phase-2-implementation/#1-autonomy-engine","title":"1. Autonomy Engine","text":"<p>Manages the \"how much freedom does the Overlord have?\" question.</p> <p>Module: <code>nebulus_swarm/overlord/autonomy.py</code></p> <pre><code>@dataclass\nclass AutonomyConfig:\n    \"\"\"Per-project autonomy configuration.\"\"\"\n    project: str\n    level: str  # \"cautious\", \"proactive\", \"scheduled\"\n    pre_approved_actions: list[str] = field(default_factory=list)\n    escalation_rules: dict[str, str] = field(default_factory=dict)\n\nclass AutonomyEngine:\n    \"\"\"Manages autonomy levels and action approval.\"\"\"\n\n    def __init__(self, config: OverlordConfig):\n        self.config = config\n        self._load_autonomy_config()\n\n    def get_level(self, project: str) -&gt; str:\n        \"\"\"Get effective autonomy level for a project.\"\"\"\n        # Check project-specific override, fall back to global\n        return self.config.autonomy_overrides.get(\n            project, self.config.autonomy_global\n        )\n\n    def can_auto_execute(self, action: str, scope: ActionScope) -&gt; bool:\n        \"\"\"Check if action can auto-execute under current autonomy.\"\"\"\n        level = self.get_level(scope.projects[0] if scope.projects else \"\")\n\n        # Cautious: nothing auto-executes\n        if level == \"cautious\":\n            return False\n\n        # Proactive: only read-only actions\n        if level == \"proactive\":\n            return not scope.affects_remote and not scope.destructive\n\n        # Scheduled: check pre-approved list\n        if level == \"scheduled\":\n            return self._is_pre_approved(action, scope)\n\n        return False\n\n    def should_propose(self, action: str, scope: ActionScope) -&gt; bool:\n        \"\"\"Check if Overlord should proactively propose this action.\"\"\"\n        level = self.get_level(scope.projects[0] if scope.projects else \"\")\n\n        # Cautious: never proposes\n        if level == \"cautious\":\n            return False\n\n        # Proactive: proposes low-medium impact actions\n        if level == \"proactive\":\n            return scope.estimated_impact in (\"low\", \"medium\")\n\n        # Scheduled: proposes anything outside pre-approved list\n        if level == \"scheduled\":\n            return not self._is_pre_approved(action, scope)\n\n        return False\n\n    def _is_pre_approved(self, action: str, scope: ActionScope) -&gt; bool:\n        \"\"\"Check if action is in the pre-approved list.\"\"\"\n        # Pre-approved actions are defined per-project in config\n        # Examples: \"merge develop to main\", \"clean stale branches\", \"run tests\"\n        for project in scope.projects:\n            config = self._get_project_autonomy(project)\n            if action in config.pre_approved_actions:\n                return True\n        return False\n</code></pre> <p>Config schema:</p> <pre><code># ~/.atom/overlord.yml\nautonomy:\n  global: proactive\n\n  overrides:\n    nebulus-core: cautious  # Core is critical \u2014 always ask\n    nebulus-gantry: scheduled  # Gantry can run nightly tasks\n\n  pre_approved:\n    # Actions that 'scheduled' mode can auto-execute\n    nebulus-gantry:\n      - \"clean stale branches\"\n      - \"run tests\"\n      - \"update dependency versions\"\n    nebulus-prime:\n      - \"run tests\"\n      - \"clean stale branches\"\n</code></pre>"},{"location":"plans/2026-02-06-overlord-phase-2-implementation/#2-dispatch-engine","title":"2. Dispatch Engine","text":"<p>Coordinates task execution across multiple projects with blast radius awareness.</p> <p>Module: <code>nebulus_swarm/overlord/dispatch.py</code></p> <pre><code>@dataclass\nclass DispatchPlan:\n    \"\"\"A multi-step execution plan.\"\"\"\n    task: str\n    steps: list[DispatchStep]\n    scope: ActionScope\n    estimated_duration: int  # seconds\n    requires_approval: bool\n\n@dataclass\nclass DispatchStep:\n    \"\"\"A single atomic step in a dispatch plan.\"\"\"\n    action: str\n    project: str\n    dependencies: list[str]  # IDs of steps that must complete first\n    model_tier: str  # \"local\", \"cloud-fast\", \"cloud-heavy\", or None\n    timeout: int\n\nclass DispatchEngine:\n    \"\"\"Coordinates task execution across multiple projects.\"\"\"\n\n    def __init__(\n        self,\n        config: OverlordConfig,\n        autonomy: AutonomyEngine,\n        graph: DependencyGraph,\n        model_router: ModelRouter,\n    ):\n        self.config = config\n        self.autonomy = autonomy\n        self.graph = graph\n        self.router = model_router\n\n    def plan(self, task: str) -&gt; DispatchPlan:\n        \"\"\"Convert a high-level task into an execution plan.\"\"\"\n        # Parse task (e.g., \"merge Core develop to main and tag v0.2.0\")\n        # Return plan with steps, dependencies, scope\n        pass\n\n    def execute(self, plan: DispatchPlan) -&gt; DispatchResult:\n        \"\"\"Execute a dispatch plan.\"\"\"\n        # Check autonomy \u2014 can we auto-execute or need approval?\n        if plan.requires_approval:\n            if not self._get_user_approval(plan):\n                return DispatchResult(status=\"cancelled\", reason=\"User denied\")\n\n        # Execute steps in dependency order\n        results = []\n        for step in self._topological_order(plan.steps):\n            result = self._execute_step(step)\n            results.append(result)\n            if not result.success:\n                return DispatchResult(status=\"failed\", steps=results)\n\n        return DispatchResult(status=\"success\", steps=results)\n\n    def _execute_step(self, step: DispatchStep) -&gt; StepResult:\n        \"\"\"Execute a single step.\"\"\"\n        if step.model_tier:\n            # LLM-powered step \u2014 dispatch to worker\n            return self._dispatch_to_worker(step)\n        else:\n            # Direct execution (git command, script, etc.)\n            return self._execute_direct(step)\n</code></pre> <p>Key patterns:</p> <ol> <li>Task decomposition \u2014 Break \"release Core v0.2.0\" into:</li> <li>Check tests pass</li> <li>Merge develop \u2192 main</li> <li>Tag v0.2.0</li> <li>Update dependents (Prime, Edge, Forge)</li> <li>Run tests on dependents</li> <li> <p>Push if all pass</p> </li> <li> <p>Dependency resolution \u2014 Use graph to determine order (Core before Prime)</p> </li> <li> <p>Blast radius awareness \u2014 Evaluate scope before execution, escalate if too broad</p> </li> <li> <p>Rollback on failure \u2014 If step 3 fails, undo steps 1-2</p> </li> </ol>"},{"location":"plans/2026-02-06-overlord-phase-2-implementation/#3-model-router","title":"3. Model Router","text":"<p>Maps tasks to the right model tier based on complexity and cost.</p> <p>Module: <code>nebulus_swarm/overlord/model_router.py</code></p> <pre><code>@dataclass\nclass ModelEndpoint:\n    \"\"\"An available model endpoint.\"\"\"\n    name: str\n    endpoint: str\n    model: str\n    tier: str  # \"local\", \"cloud-fast\", \"cloud-heavy\"\n    concurrent: int\n    health_check_url: str\n\nclass ModelRouter:\n    \"\"\"Routes tasks to appropriate model tiers.\"\"\"\n\n    def __init__(self, config: OverlordConfig):\n        self.endpoints = self._load_endpoints(config)\n        self.tier_preference = [\"local\", \"cloud-fast\", \"cloud-heavy\"]\n\n    def select_model(self, task_type: str, complexity: str = \"medium\") -&gt; ModelEndpoint:\n        \"\"\"Select the best model for a task.\"\"\"\n        # Task type \u2192 tier mapping\n        tier = self._infer_tier(task_type, complexity)\n\n        # Find healthy endpoint in that tier\n        for endpoint in self._get_tier_endpoints(tier):\n            if self._is_healthy(endpoint):\n                return endpoint\n\n        # Fallback to next tier\n        return self._fallback(tier)\n\n    def _infer_tier(self, task_type: str, complexity: str) -&gt; str:\n        \"\"\"Map task type + complexity to tier.\"\"\"\n        # Mechanical tasks \u2192 local\n        if task_type in (\"format\", \"lint\", \"boilerplate\"):\n            return \"local\"\n\n        # Feature implementation \u2192 local (if available)\n        if task_type == \"feature\" and complexity in (\"low\", \"medium\"):\n            return \"local\"\n\n        # Code review \u2192 cloud-fast (Haiku is good at this)\n        if task_type == \"review\":\n            return \"cloud-fast\"\n\n        # Architecture, planning \u2192 cloud-heavy (needs reasoning)\n        if task_type in (\"architecture\", \"planning\", \"debugging\"):\n            if complexity == \"high\":\n                return \"cloud-heavy\"\n            return \"cloud-fast\"\n\n        return \"cloud-fast\"  # Default\n\n    def _is_healthy(self, endpoint: ModelEndpoint) -&gt; bool:\n        \"\"\"Check if endpoint is responding.\"\"\"\n        try:\n            response = requests.get(\n                endpoint.health_check_url, timeout=2\n            )\n            return response.status_code == 200\n        except:\n            return False\n</code></pre> <p>Tier mapping table:</p> Task Type Complexity Tier Model Git chores - None Direct execution Code formatting - local Either 14B or 30B Linting fixes - local Either 14B or 30B Boilerplate - local Prefer 30B Feature (simple) low local Prefer 30B Feature (moderate) medium local Prefer 30B Feature (complex) high cloud-fast Haiku Code review medium cloud-fast Haiku Debugging high cloud-heavy Sonnet Architecture high cloud-heavy Sonnet Planning high cloud-heavy Sonnet <p>Health checking:</p> <ul> <li>Hit <code>/health</code> on each local endpoint at startup</li> <li>Refresh health status every 5 minutes</li> <li>If local backend is down, fall back to cloud tier automatically</li> </ul>"},{"location":"plans/2026-02-06-overlord-phase-2-implementation/#4-release-coordinator","title":"4. Release Coordinator","text":"<p>Automates coordinated releases across dependent projects.</p> <p>Module: <code>nebulus_swarm/overlord/release.py</code></p> <pre><code>@dataclass\nclass ReleaseSpec:\n    \"\"\"Specification for a coordinated release.\"\"\"\n    project: str\n    version: str\n    source_branch: str = \"develop\"\n    target_branch: str = \"main\"\n    update_dependents: bool = True\n    push_to_remote: bool = False\n\nclass ReleaseCoordinator:\n    \"\"\"Coordinates releases across dependent projects.\"\"\"\n\n    def __init__(\n        self,\n        config: OverlordConfig,\n        graph: DependencyGraph,\n        dispatch: DispatchEngine,\n        memory: OverlordMemory,\n    ):\n        self.config = config\n        self.graph = graph\n        self.dispatch = dispatch\n        self.memory = memory\n\n    def plan_release(self, spec: ReleaseSpec) -&gt; DispatchPlan:\n        \"\"\"Plan a coordinated release.\"\"\"\n        steps = []\n\n        # Step 1: Validate source project\n        steps.append(DispatchStep(\n            action=\"validate_tests\",\n            project=spec.project,\n            dependencies=[],\n            model_tier=None,\n            timeout=300,\n        ))\n\n        # Step 2: Merge + tag\n        steps.append(DispatchStep(\n            action=f\"merge {spec.source_branch} to {spec.target_branch}\",\n            project=spec.project,\n            dependencies=[\"validate_tests\"],\n            model_tier=None,\n            timeout=60,\n        ))\n\n        steps.append(DispatchStep(\n            action=f\"tag {spec.version}\",\n            project=spec.project,\n            dependencies=[f\"merge {spec.source_branch} to {spec.target_branch}\"],\n            model_tier=None,\n            timeout=30,\n        ))\n\n        # Step 3: Update dependents (if requested)\n        if spec.update_dependents:\n            downstream = self.graph.get_downstream(spec.project)\n            for dep_project in downstream:\n                steps.append(DispatchStep(\n                    action=f\"update {spec.project} to {spec.version}\",\n                    project=dep_project,\n                    dependencies=[f\"tag {spec.version}\"],\n                    model_tier=None,\n                    timeout=120,\n                ))\n\n                steps.append(DispatchStep(\n                    action=\"validate_tests\",\n                    project=dep_project,\n                    dependencies=[f\"update {spec.project} to {spec.version}\"],\n                    model_tier=None,\n                    timeout=300,\n                ))\n\n        # Step 4: Push (if requested)\n        if spec.push_to_remote:\n            affected = self.graph.get_affected_by(spec.project)\n            steps.append(DispatchStep(\n                action=\"push to remote\",\n                project=spec.project,\n                dependencies=[f\"tag {spec.version}\"],\n                model_tier=None,\n                timeout=60,\n            ))\n\n        scope = ActionScope(\n            projects=self.graph.get_affected_by(spec.project),\n            branches=[spec.source_branch, spec.target_branch],\n            destructive=False,\n            reversible=False if spec.push_to_remote else True,\n            affects_remote=spec.push_to_remote,\n            estimated_impact=\"high\",\n        )\n\n        return DispatchPlan(\n            task=f\"Release {spec.project} {spec.version}\",\n            steps=steps,\n            scope=scope,\n            estimated_duration=sum(s.timeout for s in steps),\n            requires_approval=True,  # Releases always need approval\n        )\n\n    def execute_release(self, spec: ReleaseSpec) -&gt; DispatchResult:\n        \"\"\"Execute a coordinated release.\"\"\"\n        plan = self.plan_release(spec)\n        result = self.dispatch.execute(plan)\n\n        # Log to memory\n        if result.status == \"success\":\n            self.memory.remember(\n                category=\"release\",\n                content=f\"{spec.project} {spec.version} released\",\n                project=spec.project,\n                downstream_updated=self.graph.get_downstream(spec.project),\n            )\n\n        return result\n</code></pre> <p>Example workflow:</p> <pre><code># User command\nnebulus-atom overlord release nebulus-core v0.2.0\n\n# Overlord plans:\n1. Run tests on Core\n2. Merge develop \u2192 main\n3. Tag v0.2.0\n4. Update Core version in Prime, Edge, Forge\n5. Run tests on Prime, Edge, Forge\n6. Ask: \"All tests pass. Push to remote?\"\n\n# User approves\nOverlord pushes Core, Prime, Edge, Forge\nOverlord logs the coordinated release to memory\n</code></pre>"},{"location":"plans/2026-02-06-overlord-phase-2-implementation/#implementation-steps","title":"Implementation Steps","text":""},{"location":"plans/2026-02-06-overlord-phase-2-implementation/#step-1-autonomy-engine-2-modules-cli-tests","title":"Step 1: Autonomy Engine (2 modules + CLI + tests)","text":"<p>Files to create:</p> File Purpose <code>nebulus_swarm/overlord/autonomy.py</code> AutonomyEngine, AutonomyConfig <code>tests/test_overlord_autonomy.py</code> 15-20 tests <p>Files to modify:</p> File Change <code>nebulus_swarm/overlord/registry.py</code> Add <code>pre_approved</code> to autonomy config schema <code>nebulus_atom/commands/overlord_commands.py</code> Add <code>overlord autonomy</code> command group <code>tests/test_overlord_commands.py</code> Add CLI tests for autonomy commands <p>CLI commands:</p> <pre><code>overlord autonomy                           # Show current levels\noverlord autonomy --global proactive        # Set global level\noverlord autonomy --project core cautious   # Override for project\noverlord autonomy --list-approved           # Show pre-approved actions\n</code></pre> <p>Tests:</p> <ul> <li>Can auto-execute under scheduled mode if pre-approved</li> <li>Cannot auto-execute under cautious mode</li> <li>Proactive mode proposes low/medium impact actions</li> <li>Project overrides beat global settings</li> <li>Escalation for high-impact actions</li> </ul> <p>Deliverable: ~500 lines code + 20 tests</p>"},{"location":"plans/2026-02-06-overlord-phase-2-implementation/#step-2-model-router-1-module-tests","title":"Step 2: Model Router (1 module + tests)","text":"<p>Files to create:</p> File Purpose <code>nebulus_swarm/overlord/model_router.py</code> ModelRouter, ModelEndpoint <code>tests/test_overlord_model_router.py</code> 15-20 tests <p>Files to modify:</p> File Change <code>nebulus_swarm/overlord/registry.py</code> Add <code>models</code> section to config schema <p>Config schema:</p> <pre><code>models:\n  local-prime:\n    endpoint: http://localhost:5000/v1\n    model: Qwen2.5-Coder-14B\n    tier: local\n    concurrent: 2\n  local-edge:\n    endpoint: http://nebulus:8080/v1\n    model: qwen3-coder-30b\n    tier: local\n    concurrent: 2\n  cloud-fast:\n    endpoint: https://api.anthropic.com/v1\n    model: claude-haiku-4-5-20251001\n    tier: cloud-fast\n    concurrent: 10\n  cloud-heavy:\n    endpoint: https://api.anthropic.com/v1\n    model: claude-sonnet-4-5-20250929\n    tier: cloud-heavy\n    concurrent: 3\n</code></pre> <p>Tests:</p> <ul> <li>Task type \u2192 tier mapping</li> <li>Health check behavior</li> <li>Fallback to next tier when primary is down</li> <li>Concurrent request limiting</li> <li>User override (force a tier)</li> </ul> <p>Deliverable: ~400 lines code + 20 tests</p>"},{"location":"plans/2026-02-06-overlord-phase-2-implementation/#step-3-dispatch-engine-2-modules-cli-tests","title":"Step 3: Dispatch Engine (2 modules + CLI + tests)","text":"<p>Files to create:</p> File Purpose <code>nebulus_swarm/overlord/dispatch.py</code> DispatchEngine, DispatchPlan, DispatchStep <code>nebulus_swarm/overlord/task_parser.py</code> Natural language task \u2192 DispatchPlan <code>tests/test_overlord_dispatch.py</code> 20-25 tests <code>tests/test_overlord_task_parser.py</code> 10-15 tests <p>Files to modify:</p> File Change <code>nebulus_atom/commands/overlord_commands.py</code> Add <code>overlord dispatch</code> command <code>tests/test_overlord_commands.py</code> Add dispatch CLI tests <p>CLI command:</p> <pre><code>overlord dispatch \"merge Core develop to main\"\noverlord dispatch \"clean stale branches in Prime and Edge\"\noverlord dispatch \"run tests across all projects\"\noverlord dispatch \"release Core v0.2.0\" --push\n</code></pre> <p>Tests:</p> <ul> <li>Task parsing (string \u2192 plan)</li> <li>Dependency ordering of steps</li> <li>Blast radius calculation</li> <li>Autonomy check integration</li> <li>Rollback on failure</li> <li>Direct execution (git commands)</li> <li>Worker dispatch (LLM-powered tasks)</li> </ul> <p>Deliverable: ~800 lines code + 35 tests</p>"},{"location":"plans/2026-02-06-overlord-phase-2-implementation/#step-4-release-coordinator-1-module-cli-tests","title":"Step 4: Release Coordinator (1 module + CLI + tests)","text":"<p>Files to create:</p> File Purpose <code>nebulus_swarm/overlord/release.py</code> ReleaseCoordinator, ReleaseSpec <code>tests/test_overlord_release.py</code> 15-20 tests <p>Files to modify:</p> File Change <code>nebulus_atom/commands/overlord_commands.py</code> Add <code>overlord release</code> command <code>tests/test_overlord_commands.py</code> Add release CLI tests <p>CLI command:</p> <pre><code>overlord release nebulus-core v0.2.0                  # Plan only\noverlord release nebulus-core v0.2.0 --execute        # Plan + execute\noverlord release nebulus-core v0.2.0 --push           # Push after success\noverlord release nebulus-core v0.2.0 --no-dependents  # Skip downstream updates\n</code></pre> <p>Tests:</p> <ul> <li>Release plan generation</li> <li>Downstream update steps</li> <li>Test validation at each stage</li> <li>Push gating</li> <li>Memory logging</li> <li>Rollback on test failure</li> </ul> <p>Deliverable: ~600 lines code + 20 tests</p>"},{"location":"plans/2026-02-06-overlord-phase-2-implementation/#step-5-integration-end-to-end-tests","title":"Step 5: Integration + End-to-End Tests","text":"<p>Files to create:</p> File Purpose <code>tests/test_overlord_e2e.py</code> End-to-end dispatch workflows <p>Test scenarios:</p> <ol> <li>Cautious mode \u2014 propose merge, wait for approval, execute</li> <li>Proactive mode \u2014 auto-propose stale branch cleanup</li> <li>Scheduled mode \u2014 auto-execute pre-approved actions</li> <li>Release flow \u2014 Core release \u2192 update Prime \u2192 run tests \u2192 push</li> <li>Fallback routing \u2014 local backend down \u2192 cloud-fast</li> <li>Failure rollback \u2014 step 3 fails \u2192 undo steps 1-2</li> </ol> <p>Deliverable: ~400 lines code + 10 E2E tests</p>"},{"location":"plans/2026-02-06-overlord-phase-2-implementation/#test-summary","title":"Test Summary","text":"Module Unit Tests E2E Tests Total Autonomy Engine 20 - 20 Model Router 20 - 20 Dispatch Engine 35 - 35 Release Coordinator 20 - 20 Integration - 10 10 Total 95 10 105 <p>Phase 1 + Phase 2 = 215 tests</p>"},{"location":"plans/2026-02-06-overlord-phase-2-implementation/#configuration-reference","title":"Configuration Reference","text":"<p>Full <code>~/.atom/overlord.yml</code> with Phase 2 additions:</p> <pre><code># \u2500\u2500\u2500 Projects (Phase 1) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nprojects:\n  nebulus-core:\n    path: ~/projects/west_ai_labs/nebulus-core\n    remote: jlwestsr/nebulus-core\n    role: shared-library\n    branch_model: develop-main\n    depends_on: []\n\n# \u2500\u2500\u2500 Autonomy (Phase 2) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nautonomy:\n  global: proactive\n\n  overrides:\n    nebulus-core: cautious\n    nebulus-gantry: scheduled\n\n  pre_approved:\n    nebulus-gantry:\n      - \"clean stale branches\"\n      - \"run tests\"\n      - \"update dependency versions\"\n    nebulus-prime:\n      - \"run tests\"\n      - \"clean stale branches\"\n\n# \u2500\u2500\u2500 Models (Phase 2) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nmodels:\n  local-prime:\n    endpoint: http://localhost:5000/v1\n    model: Qwen2.5-Coder-14B\n    tier: local\n    concurrent: 2\n    health_check_url: http://localhost:5000/health\n\n  local-edge:\n    endpoint: http://nebulus:8080/v1\n    model: qwen3-coder-30b\n    tier: local\n    concurrent: 2\n    health_check_url: http://nebulus:8080/health\n\n  cloud-fast:\n    endpoint: https://api.anthropic.com/v1\n    model: claude-haiku-4-5-20251001\n    tier: cloud-fast\n    concurrent: 10\n\n  cloud-heavy:\n    endpoint: https://api.anthropic.com/v1\n    model: claude-sonnet-4-5-20250929\n    tier: cloud-heavy\n    concurrent: 3\n\n# \u2500\u2500\u2500 Dispatch (Phase 2) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\ndispatch:\n  max_concurrent_steps: 5\n  step_timeout_default: 300\n  rollback_on_failure: true\n</code></pre>"},{"location":"plans/2026-02-06-overlord-phase-2-implementation/#success-criteria","title":"Success Criteria","text":"<p>Phase 2 is complete when:</p> <ol> <li>Autonomy levels work \u2014 Cautious blocks everything, proactive proposes, scheduled auto-executes pre-approved</li> <li>Model router works \u2014 Tasks route to correct tier, fallback on health check failure</li> <li>Dispatch executes plans \u2014 Multi-step plans run in dependency order</li> <li>Release coordinator works \u2014 \"release Core v0.2.0\" updates downstream and runs their tests</li> <li>All 105 tests pass \u2014 Unit + E2E coverage</li> <li>CLI is usable \u2014 <code>overlord dispatch</code>, <code>overlord release</code>, <code>overlord autonomy</code> commands work end-to-end</li> </ol>"},{"location":"plans/2026-02-06-overlord-phase-2-implementation/#open-questions","title":"Open Questions","text":"<ol> <li>Task parsing \u2014 Use LLM to parse natural language tasks, or hand-rolled parser with pattern matching?</li> <li>Worker pool \u2014 Reuse existing DockerManager as-is, or extend it for multi-project awareness?</li> <li>Approval UX \u2014 CLI-only approval (blocking), or also support async approval via Slack/Gantry?</li> <li>State persistence \u2014 Should in-flight dispatch plans persist to disk in case Overlord crashes?</li> <li>Telemetry \u2014 Track dispatch outcomes (duration, model used, success/failure) for observability in Phase 5?</li> </ol>"},{"location":"plans/2026-02-06-overlord-phase-2-implementation/#dependencies","title":"Dependencies","text":"<p>Requires from Phase 1: - <code>registry.py</code> \u2014 Project config - <code>graph.py</code> \u2014 Dependency traversal - <code>action_scope.py</code> \u2014 Blast radius evaluation - <code>memory.py</code> \u2014 Logging dispatch outcomes</p> <p>Requires from existing Swarm: - <code>docker_manager.py</code> \u2014 Minion container orchestration - <code>llm_pool.py</code> \u2014 Concurrent LLM request handling (will become ModelRouter)</p> <p>New external dependencies: - None \u2014 uses existing <code>requests</code>, <code>pyyaml</code>, <code>rich</code>, <code>typer</code></p>"},{"location":"plans/2026-02-06-overlord-phase-2-implementation/#timeline-estimate","title":"Timeline Estimate","text":"Task Effort Duration Step 1: Autonomy Engine 500 lines + 20 tests 1 day Step 2: Model Router 400 lines + 20 tests 1 day Step 3: Dispatch Engine 800 lines + 35 tests 2 days Step 4: Release Coordinator 600 lines + 20 tests 1 day Step 5: Integration + E2E 400 lines + 10 tests 1 day Total 2,700 lines + 105 tests 6 days <p>This assumes full-time focused work. With testing, debugging, and iteration, expect 7-10 days.</p>"},{"location":"plans/2026-02-06-overlord-phase-2-implementation/#next-steps","title":"Next Steps","text":"<p>Ready to proceed with Step 1 (Autonomy Engine)?</p>"},{"location":"swarm/","title":"Nebulus Swarm Operations Guide","text":"<p>Nebulus Swarm is an autonomous agent orchestration system with an Overlord + Minions architecture. The Overlord monitors Slack and GitHub for work, spawning ephemeral Minion containers to implement features and fix bugs.</p>"},{"location":"swarm/#architecture-overview","title":"Architecture Overview","text":"<pre><code>                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                    \u2502   GitHub    \u2502\n                    \u2502   Issues    \u2502\n                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                           \u2502 nebulus-ready label\n                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510        \u2502   Overlord  \u2502        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Slack  \u2502\u25c4\u2500\u2500\u2500\u2500\u2500\u2500\u25ba\u2502  (control   \u2502\u25c4\u2500\u2500\u2500\u2500\u2500\u2500\u25ba\u2502  Local LLM  \u2502\n\u2502 Channel \u2502        \u2502   plane)    \u2502        \u2502  (Nebulus)  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2518        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                          \u2502\n           \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n           \u2502              \u2502              \u2502\n    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2510\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2510\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    \u2502   Minion    \u2502\u2502   Minion    \u2502\u2502   Minion    \u2502\n    \u2502 (ephemeral) \u2502\u2502 (ephemeral) \u2502\u2502 (ephemeral) \u2502\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"swarm/#quick-start","title":"Quick Start","text":""},{"location":"swarm/#prerequisites","title":"Prerequisites","text":"<ul> <li>Docker and Docker Compose</li> <li>GitHub Personal Access Token with repo permissions</li> <li>Slack App with Socket Mode enabled</li> <li>Local LLM server (Nebulus/Ollama) running</li> </ul>"},{"location":"swarm/#1-configure-environment","title":"1. Configure Environment","text":"<pre><code># Copy example config\ncp .env.swarm.example .env.swarm\n\n# Edit with your credentials\nvim .env.swarm\n</code></pre> <p>Required environment variables: - <code>SLACK_BOT_TOKEN</code> - Slack Bot OAuth Token (xoxb-...) - <code>SLACK_APP_TOKEN</code> - Slack App-Level Token for Socket Mode (xapp-...) - <code>SLACK_CHANNEL_ID</code> - Channel ID to monitor (C...) - <code>GITHUB_TOKEN</code> - GitHub Personal Access Token (ghp_...) - <code>GITHUB_WATCHED_REPOS</code> - Comma-separated repos (owner/repo1,owner/repo2)</p>"},{"location":"swarm/#2-build-images","title":"2. Build Images","text":"<pre><code># Build Overlord image\ndocker build -t nebulus-overlord:latest -f nebulus_swarm/overlord/Dockerfile .\n\n# Build Minion image\ndocker build -t nebulus-minion:latest -f nebulus_swarm/minion/Dockerfile .\n</code></pre>"},{"location":"swarm/#3-start-overlord","title":"3. Start Overlord","text":"<pre><code>docker-compose -f docker-compose.swarm.yml up -d overlord\n</code></pre>"},{"location":"swarm/#4-verify","title":"4. Verify","text":"<pre><code># Check health\ncurl http://localhost:8080/health\n\n# Check detailed status\ncurl http://localhost:8080/status\n</code></pre>"},{"location":"swarm/#configuration-reference","title":"Configuration Reference","text":""},{"location":"swarm/#environment-variables","title":"Environment Variables","text":"Variable Required Default Description <code>SLACK_BOT_TOKEN</code> Yes - Slack Bot OAuth Token <code>SLACK_APP_TOKEN</code> Yes - Slack App-Level Token <code>SLACK_CHANNEL_ID</code> Yes - Channel to monitor <code>GITHUB_TOKEN</code> Yes - GitHub PAT <code>GITHUB_WATCHED_REPOS</code> Yes - Comma-separated repos <code>GITHUB_DEFAULT_REPO</code> No - Default repo for commands <code>NEBULUS_BASE_URL</code> No http://localhost:5000/v1 LLM server URL <code>NEBULUS_MODEL</code> No qwen3-coder-30b Model name <code>NEBULUS_TIMEOUT</code> No 600 Request timeout (seconds) <code>MAX_CONCURRENT_MINIONS</code> No 3 Max parallel minions <code>CRON_ENABLED</code> No true Enable cron sweeps <code>CRON_SCHEDULE</code> No 0 2 * * * Cron schedule <code>LOG_LEVEL</code> No INFO Log level <code>LOG_FORMAT</code> No console json or console <code>LOG_FILE</code> No - Optional log file path"},{"location":"swarm/#github-labels","title":"GitHub Labels","text":"<p>The Overlord uses these labels to track issue state:</p> Label Description <code>nebulus-ready</code> Issue is ready for a Minion to work on <code>in-progress</code> Minion is actively working on it <code>in-review</code> PR created, awaiting review <code>needs-attention</code> Minion failed, needs human review <code>high-priority</code> Higher priority in queue"},{"location":"swarm/#slack-commands","title":"Slack Commands","text":"<p>Send these commands in the monitored Slack channel:</p> Command Description <code>status</code> Show active minions and queue status <code>queue</code> Show pending issues from GitHub <code>work on owner/repo#123</code> Start work on specific issue <code>work on #123</code> Start work (uses default repo) <code>stop #123</code> Stop minion working on issue <code>stop minion-abc123</code> Stop specific minion <code>pause</code> Pause automatic queue processing <code>resume</code> Resume queue processing <code>history</code> Show recent work history <code>help</code> Show available commands"},{"location":"swarm/#troubleshooting","title":"Troubleshooting","text":""},{"location":"swarm/#overlord-wont-start","title":"Overlord won't start","text":"<ol> <li>Check Docker is running: <code>docker ps</code></li> <li>Verify environment: <code>docker-compose -f docker-compose.swarm.yml config</code></li> <li>Check logs: <code>docker logs overlord</code></li> </ol>"},{"location":"swarm/#minion-containers-stuck","title":"Minion containers stuck","text":"<ol> <li>Check watchdog: Minions without heartbeat for 5 minutes are auto-killed</li> <li>Manual stop: Send <code>stop minion-&lt;id&gt;</code> in Slack</li> <li>Force cleanup: <code>docker rm -f $(docker ps -aq -f label=nebulus.swarm.minion)</code></li> </ol>"},{"location":"swarm/#rate-limit-errors","title":"Rate limit errors","text":"<ol> <li>Check quota: Send <code>queue</code> command to see remaining API calls</li> <li>Wait for reset: The queue sweep skips when rate limited</li> <li>Reduce watched repos to decrease API usage</li> </ol>"},{"location":"swarm/#minion-fails-immediately","title":"Minion fails immediately","text":"<ol> <li>Check LLM server is reachable from Docker network</li> <li>Verify GitHub token has correct permissions</li> <li>Check minion logs: <code>docker logs minion-&lt;id&gt;</code></li> </ol>"},{"location":"swarm/#operational-runbook","title":"Operational Runbook","text":""},{"location":"swarm/#daily-operations","title":"Daily Operations","text":"<ul> <li>Check Slack channel for failed minion notifications</li> <li>Review PRs created by minions</li> <li>Monitor <code>#nebulus-swarm</code> channel for activity</li> </ul>"},{"location":"swarm/#maintenance-tasks","title":"Maintenance Tasks","text":"<p>Update images: <pre><code>docker-compose -f docker-compose.swarm.yml down\ndocker build -t nebulus-overlord:latest -f nebulus_swarm/overlord/Dockerfile .\ndocker build -t nebulus-minion:latest -f nebulus_swarm/minion/Dockerfile .\ndocker-compose -f docker-compose.swarm.yml up -d overlord\n</code></pre></p> <p>View logs: <pre><code># Overlord logs\ndocker logs -f overlord\n\n# Specific minion logs\ndocker logs minion-&lt;id&gt;\n</code></pre></p> <p>Clean up old containers: <pre><code># Remove stopped minion containers\ndocker container prune -f --filter label=nebulus.swarm.minion\n</code></pre></p>"},{"location":"swarm/#incident-response","title":"Incident Response","text":"<p>High failure rate: 1. Pause queue: Send <code>pause</code> in Slack 2. Check LLM server health 3. Review recent failures for patterns 4. Resume when resolved: Send <code>resume</code></p> <p>Overlord crash: 1. Check Docker: <code>docker ps -a | grep overlord</code> 2. View logs: <code>docker logs overlord</code> 3. Restart: <code>docker-compose -f docker-compose.swarm.yml up -d overlord</code> 4. Overlord will resume with persisted state</p>"},{"location":"swarm/#backup-and-recovery","title":"Backup and Recovery","text":"<p>State database: <pre><code># Backup\ndocker cp overlord:/var/lib/overlord/state.db ./state.db.backup\n\n# Restore\ndocker cp ./state.db.backup overlord:/var/lib/overlord/state.db\ndocker restart overlord\n</code></pre></p>"},{"location":"swarm/#monitoring","title":"Monitoring","text":""},{"location":"swarm/#health-endpoint","title":"Health Endpoint","text":"<pre><code>curl http://localhost:8080/health\n</code></pre> <p>Response: <pre><code>{\n  \"status\": \"healthy\",\n  \"active_minions\": 2,\n  \"paused\": false,\n  \"docker_available\": true\n}\n</code></pre></p>"},{"location":"swarm/#status-endpoint","title":"Status Endpoint","text":"<pre><code>curl http://localhost:8080/status\n</code></pre> <p>Returns detailed information about: - Active minions with status - Docker container states - Configuration summary</p>"},{"location":"swarm/#log-analysis","title":"Log Analysis","text":"<p>For JSON logs (<code>LOG_FORMAT=json</code>):</p> <pre><code># Count errors in last hour\ndocker logs overlord --since 1h 2&gt;&amp;1 | grep '\"level\":\"ERROR\"' | wc -l\n\n# Find rate limit events\ndocker logs overlord 2&gt;&amp;1 | grep \"rate_limited\"\n\n# Track specific minion\ndocker logs overlord 2&gt;&amp;1 | grep \"minion-abc123\"\n</code></pre>"},{"location":"testing/ui_regression_prompts/","title":"UI Regression Test Prompts","text":"<p>This document contains a standardized set of prompts for manually verifying the Nebulus Atom CLI agent's capabilities.</p>"},{"location":"testing/ui_regression_prompts/#usage","title":"Usage","text":"<p>Run these prompts interactively (e.g., <code>mn</code>) to ensure the agent correctly identifies and executes the appropriate tools.</p>"},{"location":"testing/ui_regression_prompts/#core-features-legacy","title":"Core Features (Legacy)","text":"Feature Test Prompt Expected Behavior 1. Task Management \"Create a plan to learn about this project. Add a task to read the context file.\" Creates a plan (<code>create_plan</code>), then adds a task. 2. Context Management \"Pin the file 'CONTEXT.md' to the context and then list the current context.\" Calls <code>pin_file</code> on <code>CONTEXT.md</code>, then <code>list_context</code>. 3. File Operations \"List the files in the 'nebulus_atom' directory using a shell command.\" Calls <code>run_shell_command</code> with <code>ls</code> (or equivalent). 4. Skill Execution \"Use the file_info skill to check the size of 'nebulus_atom/main.py'.\" Identifies and executes the <code>file_info</code> skill. 5. RAG / Search \"Search the codebase for 'AgentController' and tell me which file it is in.\" Calls <code>search_code</code> to locate the string. 6. File I/O \"Write a file named 'test_hello.txt' with the content 'Hello World'.\" Calls <code>write_file</code> to create the artifact."},{"location":"testing/ui_regression_prompts/#new-features-v20","title":"New Features (v2.0)","text":""},{"location":"testing/ui_regression_prompts/#1-rich-repl-interface","title":"1. Rich REPL Interface","text":"<p>Prompt:</p> <p>\"What is your current plan?\" Verification: - Prompt Box: Ensure the <code>Nebulus Atom \u27a4</code> prompt remains at the bottom of the screen. - Streaming: Verify the agent's response streams above the input box. - Status Bar: Check the bottom toolbar for <code>CWD | (Branch) | Model</code>. - History: Press <code>Up Arrow</code> to verify you can see the command you just typed.</p>"},{"location":"testing/ui_regression_prompts/#2-session-journal-daily-standup","title":"2. Session Journal (Daily Standup)","text":"<p>Prompt:</p> <p>\"Generate a journal for this session.\" Verification: - Agent calls <code>generate_journal</code>. - Check <code>.nebulus_atom/journals/</code> for a new Markdown file containing your recent interactions.</p>"},{"location":"testing/ui_regression_prompts/#3-codebase-cartographer-ast","title":"3. Codebase Cartographer (AST)","text":"<p>Prompt:</p> <p>\"Map the codebase structure for the 'nebulus_atom' directory.\" Verification: - Agent calls <code>map_codebase(target_dir=\"nebulus_atom\")</code>. - Output should list classes and functions (e.g., <code>AgentController</code>, <code>process_turn</code>).</p>"},{"location":"testing/ui_regression_prompts/#4-shell-macro-generator","title":"4. Shell Macro Generator","text":"<p>Prompt:</p> <p>\"Create a shell macro named 'list_python' that lists all python files in the current directory.\" Verification: - Agent calls <code>create_macro(name=\"list_python\", commands=[\"find . -name \"*.py\"\"])</code>. - Check <code>~/.nebulus_atom/macros/list_python.sh</code> exists and is executable.</p>"},{"location":"testing/ui_regression_prompts/#5-agentic-tdd-loop","title":"5. Agentic TDD Loop","text":"<p>Prompt:</p> <p>\"Start a TDD loop to implement a function 'multiply(a, b)'.\" Verification: - Agent calls <code>start_tdd(goal=\"Implement multiply(a, b)\")</code>. - Observe the agent autonomously:     1.  Create <code>tests/test_multiply.py</code> (failing).     2.  Run the test (fail).     3.  Create/Update <code>multiply.py</code>.     4.  Run the test (pass).</p>"},{"location":"testing/ui_regression_prompts/#6-github-integration-mcp","title":"6. GitHub Integration (MCP)","text":"<p>Prompt:</p> <p>\"Connect to the GitHub MCP server and list my open issues.\" Verification: - Agent calls <code>connect_mcp_server</code>. - Agent calls <code>github_list_issues</code> (or similar discovered tool). - Output should reflect real GitHub data (requires <code>GITHUB_TOKEN</code>).</p>"},{"location":"testing/ui_regression_prompts/#7-multimodal-input","title":"7. Multimodal Input","text":"<p>Prompt:</p> <p>\"Scan the image 'test_image.png'.\" (Ensure you have a dummy image first) Verification: - Agent calls <code>scan_image(path=\"test_image.png\")</code>. - Returns base64 encoded string.</p>"},{"location":"testing/ui_regression_prompts/#8-embedded-documentation","title":"8. Embedded Documentation","text":"<p>Prompt:</p> <p>\"List the available documentation.\" Verification: - Agent calls <code>list_docs</code>. - Output lists files in <code>docs/</code>.</p>"},{"location":"testing/ui_regression_prompts/#9-persistent-tasks","title":"9. Persistent Tasks","text":"<p>Prompt:</p> <p>\"Create a plan to 'Test Persistence'.\" -&gt; Exit -&gt; Restart. Verification: - Upon restart, run <code>get_plan</code> (or \"Show plan\"). - The output should contain \"Test Persistence\".</p>"},{"location":"testing/ui_regression_prompts/#integrated-workflow-the-architect","title":"Integrated Workflow: \"The Architect\"","text":"<p>Prompt:</p> <p>\"I want to understand the 'TaskService'. First, map the codebase to find where it is defined. Second, search the docs for 'Task Management'. Finally, generate a journal of this research.\"</p> <p>Expected Sequence: 1.  <code>map_codebase</code> (finds <code>nebulus_atom/services/task_service.py</code>). 2.  <code>read_doc</code> (reads <code>docs/features/task_management.md</code>). 3.  <code>generate_journal</code> (saves the summary).</p>"}]}