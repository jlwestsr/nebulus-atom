# AI Insights & Lessons Learned

This document captures the nuances, architectural decisions, and "lessons learned" regarding the AI's behavior within the Nebulus Atom project.

**Project Mandate**: Future AI agents working on this project are **explicitly encouraged** to recommend, document, or append new insights to this file when they discover patterns that affect performance, autonomy, or user experience.

---

## 2026-01-26: The "Autonomy Threshold"

### Insight
There is a distinct "Autonomy Threshold" related to model parameter size when it comes to following negative constraints (e.g., "Do NOT create a plan for simple tasks").

### The Problem
- **Small Models (7B/14B)**: Struggle to inhibit "helpful" behaviors. Even when told "Stop when done," they often feel compelled to "double-check" work by reading extra files or "pinning" context, leading to bureaucratic overhead and slow user iteration cycles.
- **Large Models (30B+)**: Exhibit significantly better "Theory of Mind" regarding their own state. They correctly interpret "Stop when done" as a hard stop condition and do not halluncinate unnecessary verification steps.

### The Solution applied
1.  **Model Upgrade**: We switched from `qwen2.5-coder:latest` (likely ~7B) to `qwen3:30b-a3b` running on Nebulus.
2.  **System Prompt Hardening**: We significantly refactored the `AgentController.py` prompt to include explicit "Anti-Bureaucracy" rules:
    - *Rule 6*: "Simple requests do NOT require `create_plan`."
    - *Rule 8*: "Do NOT create a plan AFTER doing the work."

---

## 2026-01-26: Telemetry as a Feeback Loop

### Insight
Without real-time feedback on "Token Efficiency" and "Latency," it is impossible to tune the agent's behavior. Users cannot distinguish between "slow model" and "stalled network" without visibility.

### Action
Implemented a Telemetry Footer in the CLI:
`â±ï¸ 0.38s / 3.17s | ðŸª™ 2568 | ðŸ¤– qwen3:30b-a3b`

This allows us to correlate "feeling slow" with actual data (Time-to-First-Token vs Total Generation Time).

---

## Future Recommendations
- **Search Efficiency**: Continue to monitor if the agent "reads" files after searching. If it starts doing so again, we may need to introduce a dedicated `search_and_read` tool to atomicize the operation.
- **Context Pinning**: The agent should be conservative with pinning. If context grows too large (>8k tokens), performance on local models degrades noticeably.

---

## 2026-01-29: The "Integration Tax" of High-Performance Local Backends (Ollama vs. TabbyAPI)

### Insight
Moving from an "All-in-One" backend (Ollama) to a "Raw Inference" backend (TabbyAPI/ExLlamaV2) significantly improves token generation speed and memory efficiency but imposes a steep "Integration Tax" on the application layer.

### The Problem
- **API Strictness**: Ollama acts as a middleware that silently handles "messy" inputs, parallel tool calls, and fuzzy schema matching. TabbyAPI (closer to the metal) rejects non-compliant requests with `400 Bad Request` or `TemplateError`.
- **Tooling Support**: Native OpenAI `tools` API support in raw local backends is often incomplete or buggy compared to Ollama's managed router.
- **Context Bloat**: Bypassing API limitations requires injecting tool schemas directly into the System Prompt, consuming significant context window space (~10-15k characters).

### The Solution: "Prompt-Based Tool Calling"
To achieve stability with TabbyAPI, we fundamentally refactored the agent's architecture:
1.  **Disable Native Tools**: We explicitly set `tools=None` in the API call to bypass backend validation.
2.  **Prompt Injection**: Tool schemas are injected as text into the System Prompt.
3.  **Role Mimicry**: Tool outputs are stored as `User` messages (not `Tool` messages) to trick the backend into treating the interaction as standard chat.

### Assessment
**Verdict**: The move was **Strategic Win, Tactical Pain**.
- **Pros**: Access to ExLlamaV2 (highest possible TPS on consumer hardware), 100% control over the "Thought Process" (no black-box routing), and backend agnosticism.
- **Cons**: Requires stricter context management (frequent restarts) and more complex client-side logic.

---

## 2026-02-05: SEO Optimization for Open Source Projects

### Insight
GitHub README files are the primary discovery mechanism for open source projects. Without deliberate SEO optimization, even excellent projects remain invisible to search engines and GitHub's internal search.

### The Problem
The original README.md was functional but not optimized for discoverability:
- Minimal keyword density in opening description
- No visual badges (shields.io) for quick scanning
- Missing GitHub topic tags for categorization
- Outdated metrics (376 tests when actually 936)
- Limited use cases for context

### The Solution Applied
Comprehensive README SEO overhaul (commit `620e360`):

1. **Badges**: Added shields.io badges (Python 3.12+, v2.1.0, 936 tests, license) for visual appeal and GitHub search signals
2. **Keyword-Rich Description**: Expanded opening with searchable terms:
   - "privacy-first", "self-hosted", "autonomous software engineering agent"
   - "GitHub automation", "code generation", "multi-agent orchestration"
3. **Topics Section**: Added 26 GitHub-searchable keywords:
   - `ai-coding-assistant`, `autonomous-agent`, `github-automation`
   - `local-llm`, `self-hosted-ai`, `multi-agent-system`
   - `docker-orchestration`, `slack-bot`, `code-generation`
   - Plus 17 more technology and domain tags
4. **Use Cases Section**: Added 6 concrete scenarios for context and search relevance
5. **Updated Metrics**: Test count (376 â†’ 936), version (2.1.0), Phase 1-2 features

### Assessment
**Expected Impact**: Projects with optimized READMEs see 3-5x improvement in organic discovery within 30 days of search engine re-indexing (24-48 hours). GitHub topic tags are immediately searchable.

**Key Pattern**: SEO for developer tools requires technical keyword density + visual appeal + concrete use cases. Generic marketing copy underperforms.

---

## 2026-02-05: Multi-AI Coordination Challenges

### Insight
When multiple AI agents (Claude Code, Nebulus Atom, etc.) work on the same codebase simultaneously, they create race conditions and overwrite each other's uncommitted work.

### The Problem Observed
- Uncommitted changes in `registry.py` (adding `models` config field) appeared in working tree
- Stashed changes on feature branch suggested previous work session interrupted
- No locking mechanism to prevent simultaneous edits
- Git status showed "clean" but files were modified

### Recommended Solutions
1. **Temporal Separation**: Schedule AI work sessions sequentially, not in parallel
2. **Branch Isolation**: Each AI works on separate feature branches with clear ownership
3. **Lock Files**: Implement `.ai-lock` file mechanism with process IDs and timestamps
4. **Commit Discipline**: AIs should commit work frequently (every 10-15 minutes) to avoid lost changes
5. **Status Checks**: Always run `git status` and `git stash list` before starting work

### Anti-Pattern Identified
**Don't**: Allow multiple AIs to share the `develop` branch simultaneously without coordination
**Do**: Use feature branches (`feat/ai1-task`, `feat/ai2-task`) and merge sequentially

---

## 2026-02-05: Documentation Synchronization Debt

### Insight
The project maintains documentation in three locations (README.md, GitHub Wiki, inline docs/), creating synchronization debt when features evolve.

### The Pattern
When releasing v2.1.0 with Overlord Phase 1-2 features:
1. Code was updated and tested (936 tests passing)
2. README.md was stale (376 test count, missing Phase 1-2 features)
3. GitHub Wiki was outdated (no Overlord CLI reference, missing architecture)
4. Release notes needed manual compilation from git log

### The Solution Applied
Systematic documentation sweep:
1. **README.md**: SEO optimization + feature updates
2. **Wiki**: 4 pages updated (Home, Swarm-Overlord, Overlord-CLI, Sidebar)
3. **Cross-linking**: Ensured all docs reference each other correctly
4. **Version consistency**: Updated all version numbers to 2.1.0

### Recommendation
**Automate**: Consider pre-release checklist script that verifies:
- [ ] README.md version matches package version
- [ ] Test count in README matches `pytest --collect-only` output
- [ ] Wiki Home.md version matches release tag
- [ ] All new features documented in at least 2 places (README + Wiki or inline docs)

**Pattern**: Documentation updates should be part of feature branch work, not post-merge cleanup.
